{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPba7fiiVu/sb9vZidULUTR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Differentiable Function in PyTorch\n","In PyTorch, a differentiable function is a mathematical operation or function for which PyTorch can compute the derivative with respect to its inputs. These functions are essential for training neural networks using gradient-based optimaization techniques, such as back propagation."],"metadata":{"id":"_JkT4noBcyU2"}},{"cell_type":"markdown","source":["## * Key Concepts:\n","### 1. Automatic Differentiation (Autograd):\n","* PyTorch uses a system called Autograd to automatically compute the gradients of tensors that require gradients. It does this by tracking operations performed on tensors to build a computational graph. Each node in the graph corresponds to a tensor, and the edges represent the operatioons that were used to compute the tensor.\n","\n","### 2. Differentiable Operations:\n","* For Autograd to compute gradients, the operations applied to the tensors must be differentiable. This means that PyTorch must be able to calculate the derivative of the function with respect to its inputs.\n","* Common differentiablbe operations in PyTorch include:\n","    * Basic arithmetic (addtion, subtraction, multiplication, division)\n","    * Matrix operations (dot product, matrix multiplication)\n","    * Non-linear functios (ReLU, sigmoid, tanh)\n","    * Convolutions ,pooling operations\n","    * Loss functions (e.g., MSELoss, CrossEntropyLoss)\n","    * Activation functions\n","\n","### 3 Backward Functions (grad_fn):\n","* Each differentiable operation in PyTorch has a corresponding backward function that computes the gradient during backpropagation. The **grad_fn** attribute of a tensor points to this function, allowing PyTorch to trace back through the computational graph to calculate gradients.\n","\n","### 4. Requirement for Differentiability:\n","* For a tensor to have gradients calculated, it must have the attribute **requires_grad=True**. This tells PyTorch to track operations on this tensor and to compute gradients during backpropagation.\n","\n","### 5. Non-Differentiable Operations:\n","* Some operations are non-differentiable, meaning that gradients cannot be computed for them. Examples include discreat operations (e.g., rounding, comparison operations) and certain indexing operations.\n","* If a non-differentiable operation is applied, it may cause the computational graph to break, and PyTorch will not be able to compute the gradients. This is why differentiability is crucial in the context of neural networks."],"metadata":{"id":"oG-VGXdudTO7"}},{"cell_type":"markdown","source":["## * Example of a Differentiable Function:"],"metadata":{"id":"aVfxidXjgUBr"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naL5_4q9cmJT","executionInfo":{"status":"ok","timestamp":1724303780335,"user_tz":-540,"elapsed":4763,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"5445b296-1619-4a78-e5c2-8bc654d5a1f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([4., 6.])\n"]}],"source":["import torch\n","\n","# Create a tensor with requires_grad=True\n","x = torch.tensor([2.0, 3.0], requires_grad=True)\n","\n","# Perform a differentiable operation\n","y = x ** 2 # y = x^2\n","\n","# Calculate the gradients by backpropagation\n","y.sum().backward()\n","\n","# x.grad now contains the gradient of y with respect to x\n","print(x.grad) # Output: tensor([4., 6.]) <- x^2の微分は2xなので"]},{"cell_type":"markdown","source":["In this example:\n","* The operation **x \\*\\* 2** is differentiable. The gradient of **y** with respect to **x** is **2x**, so the gradients for the values in **x** will be **[4.0, 6.0]**.\n"],"metadata":{"id":"Db1ujXKVg8Oj"}},{"cell_type":"markdown","source":["## Summary:\n","* A **diferentiable function** in PyTorch is one for which the derivative can be computed by PyTorch's Autograd system. These functions are fundamental for training neural networks since they allow the model to learn by adjusting parameters based on gradients.\n","* Most standard mathematical operations, activation functions, and loss functionss in PyTorch are differentiable.\n","* **Autograd** enable automatic gradient calculation by maintaining a computational graph that tracks operations on tensors.\n","\n","\n","Understanding which operations are differentiable is crucial when constructing models and custom loss functions, as it ensures that backpropagation can be performed correctly."],"metadata":{"id":"xIf8ErDxhjHw"}},{"cell_type":"code","source":[],"metadata":{"id":"AxZNQPtMg5Wu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `grad_fn` attribute in pytorch.tensor\n","\n","In PyTorch, the **grad_fn** parameter is an attribute associated with tensors that tracks the function used to create the tensor. It is part of PyTorch's automatic differentiation engine, known as Autograd, which is responsible for computing gradients during backpropagation in neural networks."],"metadata":{"id":"e3B-j_SWi4D_"}},{"cell_type":"markdown","source":["## Key Points about `grad_fn`\n","### 1. Tracking the Computational Graph\n","* PyTorch builds a computational graph dynamically as you perform operations on tensors. Each tensor in this graph knows how it was created. The **grad_fn** attribute stores this information.\n","\n","### 2. Function that Created the Tensor\n","* The **grad_fn** attribute points to the function that produced the tensor. For example, if a tensor is created as the result of adding two other tensors, the **grad_fn** will point to a **AddBackward0** object.\n","* It's important for tensors that are the result of an operation because it allows PyTorch to track how each tensor was derived from other tensors, which is essential for calculating gradients.\n","\n","### 3. Leaf Tensors and Non-Leaf Tensors:\n","* **Leaf Tensors**: These are tensors that are created by the user, not as the result of an operation, They have **grad_fn** set to **None**.\n","* **Non-Leaf Tensors**: These are tensors that are created as the result of operations on other tensors. The have **grad_fn** pointing to the appropriate backward function.\n","\n","### 4. Example:"],"metadata":{"id":"bFbPRLeg11yl"}},{"cell_type":"code","source":["import torch\n","\n","# Creating a tensor with requires_grad=True so that it tracks operations\n","a = torch.tensor([2.0, 3.0], requires_grad=True)\n","\n","# Perforiming an operation on tensor `a`\n","b = a + 5\n","\n","# Checking the grad_fn attribute\n","print(b.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsaD3iqW1y54","executionInfo":{"status":"ok","timestamp":1724309817435,"user_tz":-540,"elapsed":325,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"0db975b0-7c78-4297-f70b-190d221afb3e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["<AddBackward0 object at 0x7d28ad715150>\n"]}]},{"cell_type":"markdown","source":["In this example:\n","* **a** is a leaf tensor, so **a.grad_fn** is **None**.\n","* **b** is the result of adding 5 to **a**, so **b.grad_fn** points to the **AddBackward0** function, which represents the addition operation.\n","\n","### 5. Use in Backpropagation:\n","* During backpropagation, PyTorch traverses the computational graph starting from the final loss tensor, using the **grad_fn** attributes to follow the chain of operations backward and compute the gradients.\n","\n","### 6. Relevance:\n","* If you're manually constructing or modifying parts of the computational graph or debugging, understading **grad_fn** can be crucial. However, in most cases, when simply building and training models, PyTorch handles this automatically."],"metadata":{"id":"CuvNgSoT3-VP"}},{"cell_type":"markdown","source":["## Summary:\n","* The **grad_fn** attribute in a PyTorch tensor provides a reference to the function that created the tensor. This is essential for PyTorch's automatic differentiation system to work, as it allows the framework to trace back through the operations that produced each tensor and compute the necessary gradients for optimization."],"metadata":{"id":"fIE17EJn474z"}},{"cell_type":"code","source":[],"metadata":{"id":"_GM7avVK38Vl"},"execution_count":null,"outputs":[]}]}