{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOZIHnj776kBF5cDVwCOMD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 06_Batch_Normalization\n","* https://www.youtube.com/watch?v=1U5nOKh9OLQ&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi&index=20\n","* https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_04_4_batch_norm.ipynb"],"metadata":{"id":"-WOKC2Fkj8l7"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"_P5FK3MFQbsw","executionInfo":{"status":"ok","timestamp":1714786780979,"user_tz":-540,"elapsed":10,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qnMgHumJj33b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714784718903,"user_tz":-540,"elapsed":5337,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"61fd1ae0-5847-4543-de08-a95938362eda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Note: using Google CoLab\n","Using device: cpu\n"]}],"source":["import copy\n","import torch\n","\n","try:\n","    import google.colab\n","\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False\n","\n","# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n","import torch\n","has_mps = torch.backends.mps.is_built()\n","device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","source":["# Early Stopping\n","class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.restore_best_weights = restore_best_weights\n","        self.best_model = None\n","        self.best_loss = None\n","        self.counter = 0\n","        self.status = \"\"\n","\n","    def __call__(self, model, val_loss):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","            self.best_model = copy.deepcopy(model.state_dict())\n","        elif self.best_loss - val_loss > self.min_delta:\n","            self.best_loss = val_loss\n","            self.best_model = copy.deepcopy(model.state_dict())\n","            self.counter = 0\n","            self.status = f\"Improvement found, counter reset to {self.counter}\"\n","        else:\n","            self.counter += 1\n","            self.status = f\"No improvement in the last {self.counter} epochs\"\n","            if self.counter >= self.patience:\n","                self.status = f\"Early stopping after {self.counter} epochs\"\n","                if self.restore_best_weights:\n","                    model.load_state_dict(self.best_model)\n","                return True\n","        return False"],"metadata":{"id":"bssYc2MpIjbY","executionInfo":{"status":"ok","timestamp":1714785053204,"user_tz":-540,"elapsed":10,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["The goal of batch normalization is to accelerate learning and stabilize the learning process. The same dataset used previously in this chapter. To implement batch normalization we will modify both preprocessing and a neural network setup. The original preprocessing method used a z-score standardization to make the data more suited for the training process. The model itself was a simple feed-forward neural network built with PyTorch's nn.Sequential API.\n","\n","<br>\n","\n","In the revised code, we've made two major changes. The first change was in data preprocessing. Since batch normalization can reduce the impact of input distribution changes (referred to as **internal covariate(共変量) shift**, we can remove the step of z-score normalization. Batch normalization tends to make the network less sensitive to the scale and distribution of its inputs, thereby minimizing the need for manual, meticulous(几帳面な) data normalization.\n","\n","<br>\n","\n","The second, and most crucial, change was made in the architecture of the neural network itself. We've inserted batch normalization layers into our model by using the **nn.BatchNorm1d()** function. It's important to note that the batch normalization layers are typically <u>added after linear (or convolutional for CovNets) layers but before the activation function</u>. In our case, the sequence is: Linear -> BatchNorm -> ReLU.\n","\n","<br>\n","\n","\n","The batch normalization layers normalize the activations and gradients propagating through a neural network, making the model training more efficient. This can even have a slight regularization efect, somewhat akin(~と同じ) to Dropout.\n","\n","<br>\n","\n","Remember that the use of batch normalization may require some additional computational resources due to additional complexity of the model, but it often results in a significant performance boost that more than compensates for the extra computation time.\n","\n","<br>\n","\n","In summary, the introduction of batch normalization in our neural network model simplifies preprocessing and can potentially improve model training speed, stability, and overall performance.\n","\n","\n"],"metadata":{"id":"jdl8Ro3LJ6tm"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import KFold\n","\n","# Read the data set\n","df = pd.read_csv(\n","    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n","    na_values=['NA','?'])\n","\n","# Generate dummies for job\n","df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\",dtype=int)],axis=1)\n","df.drop('job', axis=1, inplace=True)\n","\n","# Generate dummies for area\n","df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\",dtype=int)],axis=1)\n","df.drop('area', axis=1, inplace=True)\n","\n","# Generate dummies for product\n","df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\",dtype=int)],axis=1)\n","df.drop('product', axis=1, inplace=True)\n","\n","# Missing values for income\n","med = df['income'].median()\n","df['income'] = df['income'].fillna(med)\n","\n","# Convert to PyTorch Tensors\n","x_columns = df.columns.drop(['age', 'id'])\n","x = torch.tensor(df[x_columns].values, dtype=torch.float32, device=device)\n","y = torch.tensor(df['age'].values, dtype=torch.float32, device=device).view(-1, 1)\n","\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"mWm3eGrGJ2W7","executionInfo":{"status":"ok","timestamp":1714786280303,"user_tz":-540,"elapsed":1890,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"d98a79e0-c6ef-485d-97c0-47d04b277941"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id   income     aspect  subscriptions  dist_healthy  save_rate  \\\n","0   1  50876.0  13.100000              1      9.017895         35   \n","1   2  60369.0  18.625000              2      7.766643         59   \n","2   3  55126.0  34.766667              1      3.632069          6   \n","3   4  51690.0  15.808333              1      5.372942         16   \n","4   5  28347.0  40.941667              3      3.822477         20   \n","\n","   dist_unhealthy  age  pop_dense  retail_dense  ...  area_b  area_c  area_d  \\\n","0       11.738935   49   0.885827      0.492126  ...       0       1       0   \n","1        6.805396   51   0.874016      0.342520  ...       0       1       0   \n","2       13.671772   44   0.944882      0.724409  ...       0       1       0   \n","3        4.333286   50   0.889764      0.444882  ...       0       1       0   \n","4        5.967121   38   0.744094      0.661417  ...       0       0       1   \n","\n","   product_a  product_b  product_c  product_d  product_e  product_f  product_g  \n","0          0          1          0          0          0          0          0  \n","1          0          0          1          0          0          0          0  \n","2          0          1          0          0          0          0          0  \n","3          0          1          0          0          0          0          0  \n","4          1          0          0          0          0          0          0  \n","\n","[5 rows x 55 columns]"],"text/html":["\n","  <div id=\"df-2d2ed137-a7df-4929-ab7e-87f0813da97b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>income</th>\n","      <th>aspect</th>\n","      <th>subscriptions</th>\n","      <th>dist_healthy</th>\n","      <th>save_rate</th>\n","      <th>dist_unhealthy</th>\n","      <th>age</th>\n","      <th>pop_dense</th>\n","      <th>retail_dense</th>\n","      <th>...</th>\n","      <th>area_b</th>\n","      <th>area_c</th>\n","      <th>area_d</th>\n","      <th>product_a</th>\n","      <th>product_b</th>\n","      <th>product_c</th>\n","      <th>product_d</th>\n","      <th>product_e</th>\n","      <th>product_f</th>\n","      <th>product_g</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>50876.0</td>\n","      <td>13.100000</td>\n","      <td>1</td>\n","      <td>9.017895</td>\n","      <td>35</td>\n","      <td>11.738935</td>\n","      <td>49</td>\n","      <td>0.885827</td>\n","      <td>0.492126</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>60369.0</td>\n","      <td>18.625000</td>\n","      <td>2</td>\n","      <td>7.766643</td>\n","      <td>59</td>\n","      <td>6.805396</td>\n","      <td>51</td>\n","      <td>0.874016</td>\n","      <td>0.342520</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>55126.0</td>\n","      <td>34.766667</td>\n","      <td>1</td>\n","      <td>3.632069</td>\n","      <td>6</td>\n","      <td>13.671772</td>\n","      <td>44</td>\n","      <td>0.944882</td>\n","      <td>0.724409</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>51690.0</td>\n","      <td>15.808333</td>\n","      <td>1</td>\n","      <td>5.372942</td>\n","      <td>16</td>\n","      <td>4.333286</td>\n","      <td>50</td>\n","      <td>0.889764</td>\n","      <td>0.444882</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>28347.0</td>\n","      <td>40.941667</td>\n","      <td>3</td>\n","      <td>3.822477</td>\n","      <td>20</td>\n","      <td>5.967121</td>\n","      <td>38</td>\n","      <td>0.744094</td>\n","      <td>0.661417</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 55 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d2ed137-a7df-4929-ab7e-87f0813da97b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2d2ed137-a7df-4929-ab7e-87f0813da97b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2d2ed137-a7df-4929-ab7e-87f0813da97b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b1ca2df4-e681-41ac-802d-367d66298c65\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b1ca2df4-e681-41ac-802d-367d66298c65')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b1ca2df4-e681-41ac-802d-367d66298c65 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Set random seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Cross-Validate\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Early stopping parameters\n","patience = 10\n","\n","fold = 0\n","for train_idx, test_idx in kf.split(x):\n","    fold += 1\n","    print(f\"Fold {fold}\")\n","\n","    x_train, x_test = x[train_idx], x[test_idx]\n","    y_train, y_test = y[train_idx], y[test_idx]\n","\n","    # PyTorch DataLoader\n","    train_dataset = TensorDataset(x_train, y_train)\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","    # Create the model and optimizer\n","    model = nn.Sequential(\n","        nn.Linear(x.shape[1], 20),\n","        nn.BatchNorm1d(20),\n","        nn.ReLU(),\n","        nn.Linear(20, 10),\n","        nn.BatchNorm1d(10),\n","        nn.ReLU(),\n","        nn.Linear(10, 1)\n","    )\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    loss_fn = nn.MSELoss()\n","\n","    # Early Stopping variables\n","    best_loss = float('inf')\n","    early_stopping_counter = 0\n","\n","    # Training loop\n","    EPOCHS = 500\n","    epoch = 0\n","    done = False\n","    es = EarlyStopping(patience=patience)\n","    while not done and epoch < EPOCHS:\n","        epoch += 1\n","        model.train()\n","        for x_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            output = model(x_batch)\n","            loss = loss_fn(output, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            val_output = model(x_test)\n","            val_loss = loss_fn(val_output, y_test)\n","\n","        # Early stopping\n","        if es(model, val_loss):\n","            done = True\n","\n","    print(f\"Epoch {epoch}/{EPOCHS}, Validation Loss: \"\n","      f\"{val_loss.item()}, {es.status}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pqOrGFXiOfdp","executionInfo":{"status":"ok","timestamp":1714786802997,"user_tz":-540,"elapsed":13325,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"1dbb36bc-39a6-45d7-dbce-5f05050f623d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1\n","Epoch 14/500, Validation Loss: 164.2581024169922, Early stopping after 10 epochs\n","Fold 2\n","Epoch 18/500, Validation Loss: 38.37013244628906, Early stopping after 10 epochs\n","Fold 3\n","Epoch 16/500, Validation Loss: 62.91709518432617, Early stopping after 10 epochs\n","Fold 4\n","Epoch 19/500, Validation Loss: 29.32093048095703, Early stopping after 10 epochs\n","Fold 5\n","Epoch 21/500, Validation Loss: 342.0139465332031, Early stopping after 10 epochs\n"]}]},{"cell_type":"code","source":["# Final evaluation\n","model.eval()\n","with torch.no_grad():\n","    oos_pred = model(x_test)\n","score = torch.sqrt(loss_fn(oos_pred, y_test)).item()\n","print(f\"Fold score (RMSE): {score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XueNmPbQrdh","executionInfo":{"status":"ok","timestamp":1714786918628,"user_tz":-540,"elapsed":7,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"951b0978-db95-480e-9a0c-e396771f69f3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold score (RMSE): 3.636986494064331\n"]}]},{"cell_type":"markdown","source":["Batch normalization can often improve the performance of a model, but it's not a guarantee. It can depend on many factors such as the specific problem being addressed, the data, the architecture of the model, and the specific training regime.\n","\n","* Dataset: Batch normalization is particularly useful when dealing with high dimensional data and tends to be more effective with larger datasets. If your dataset is small or simple, batchnormalization may not make a significant difference and might even cause the performance to decrease slightly.\n","* Model Complexity: In simpler models, batch normalization may not lead to a significant performance boost, and might even degrade the performance slightly, as it introduces extra complexity and computation.\n","* Training Prameters: The improvement due to batch normalization also depends on the other hyperparameters you're using, like the learning rate and the batch size. If the original model was already well-optimized with these parameters, the addition of batch normalization may not help much, and could potentially even harm performance.\n","\n","\n","In other words, while batch normalization is generally a useful technique for improving model performance and stability, it's not a silber bullet and it won't always lead to an improvement. It's always a good idea to experiment with different architectures and techniques to see what works best for your specific problem."],"metadata":{"id":"pPX6d5VqRQ5F"}},{"cell_type":"markdown","source":["# Internal covariate shif\n","\n","**Internal covariate shift** is a concept in deep learning that refers to the problem when the distribution of inputs to each layer of a neural network changes during training. This issue can make the training process slower and less stable, because each layer needs to continuously adapt to new distributions of input data.\n","\n","<br>\n","\n","Here's how it happens: as you train a network, the weights and biases of earlier layers change. Because the output of one layer is the input to the next, these changes can lead to significant variations in the input distibutions received by deeper layers. This forces the layers to constantly readjust to new distributions, which can slow down the learning process and make it harder to converge to an optimal set of weights.\n","\n","<br>\n","\n","To mitigate this problem, a technique called Batch Normaliaation was introduced. It normalizes the inputs of each layer ot have zero mean and unit variance, effectively stabilizing the distributions of inputs that each layer receives. This helps to accelerate the training process adn improve the performance of deep neural networks."],"metadata":{"id":"RNrXoTRbb1Ij"}},{"cell_type":"markdown","source":["# Explanation by GPT4\n","Batch normalization is a technique used to improve the training of deep neural networks by stabilizing the distributions of layer inputs. Here's a breakdown of how it works and how it can be implemented:"],"metadata":{"id":"ZuZRALFddSPd"}},{"cell_type":"markdown","source":["## How Batch Normalizaion Wroks\n","1. **Normalization**: For each mini-batch during training, batch normalization normalizes the activations of the previous layer for each neuron. This is achieved by subtracting the mini-batch mean and dividing by the mini-batch standard devidation. Mathematically, if $x$ is an input to a neuron, its normalized value $\\hat{x}$ would be:\n","\n","    $\n","    \\hat{x} = \\frac{x - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}}\n","    \\$\n","\n","   where $\\mu_{\\text{B}}$ and $ \\sigma_{\\text{B}}^2 $ are the mean and variance of the mini-batch, and $\\epsilon$ is a small constant added for numerical stability.\n","   \n","2. **Scalling and Shifting**: After normalization, batch normalization introduces two learnable parameters, gammna(γ) and beta(β), which allow the model to scale and shift the normalzied values. This helps the model to retain the representational power of the network, even after normalization. The output of batch normalization is:\n","\n","    $\n","    y = \\gamma \\hat{x} + \\beta\n","    $\n","\n","    where $y$ is the output that goes into the next layer.\n","\n","3. **During Inference**: When the model is used for inference (i.e., making predictions on new data), the mean and variance used for normalization are not computed based on the individual batch. Instead, they are estimated from the entire training dataset, typically using an exponential moving average accumulated during the training phase.\n"],"metadata":{"id":"FeaJyxsPdl8U"}},{"cell_type":"markdown","source":["## Benefits of Batch Normalization\n","* **Improves Gradient Flow**: Helps mitigate the problem of vanishing or exploding gradients.\n","* **Allows Higher Learning Rates**: Stabilizing the distribution of inputs allows for using higher learning rates without the risk of divergence.\n","* **Reduces Overfitting**: Acts as a form of regularization, slightly reducing the need for other regularization techniques like dropout."],"metadata":{"id":"SF_K9ljDhMmW"}},{"cell_type":"markdown","source":["## Implementation in PyTorch\n","In PyTorch, batch normalization is straighforward to implement using the **torch.nn.BatchNorm1d** (for 1D data) or **torch.nn.BatchNorm2d** (for images) modules."],"metadata":{"id":"sE4s2qmUhvsy"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class SimpleNet(nn.Module):\n","    def __init__(self):\n","        super(SimpleNet, self).__init__()\n","        self.layer1 = nn.Linear(10, 20)\n","        self.bn1 = nn.BatchNorm1d(20)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        return x\n","\n","# Example usage\n","model = SimpleNet()\n","input_tensor = torch.randn(5, 10)  # batch size of 5, input features 10\n","output = model(input_tensor)\n"],"metadata":{"id":"2abyPW-LQ5bx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this example, the **BatchNorm1d** layer normalizes the output of the first linear layer <u>before applying the ReLU activation function</u>. This pattern is typical when using batch normalization in deep learning architectures."],"metadata":{"id":"iLKrLgZ0iMnQ"}},{"cell_type":"markdown","source":["## Effect of batch normalization\n","To see how batch normalization affects training in practice, let's go through a simple experiment using PyTorch. We will two small neural networks on a dataset: one with batch normalization and one without. This will us to observe the differences in terms of training speed, stability, and final performance.\n","\n","<br>\n","\n","We can use a standard dataset like MNIST, which is common for demonstrating basic neural network functionalitides. MNIST is a dataset of handwritten digits, each image being 28*28 pixels. We'll create two models:\n","1. **Model without Batch Normalization**\n","2. **Model with Batch Normalization**"],"metadata":{"id":"rE6-K8K6iUDP"}},{"cell_type":"markdown","source":["### Step1: Prepare the Dataset"],"metadata":{"id":"fivcko8mjKmP"}},{"cell_type":"code","source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Transformation applied on each image\n","transform = transforms.Compose([\n","    transforms.ToTensor(), # Convert images to Pytorch tensors\n","    transforms.Normalize((0.5,), (0.5,)) # Normalize the dataset\n","])\n","\n","# Load MNIST dataset\n","train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMeezeTyiSeu","executionInfo":{"status":"ok","timestamp":1714791916824,"user_tz":-540,"elapsed":1877,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"f12c0805-b2d9-4bad-c7c9-76008f5480dc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 76013455.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 32589640.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 55956077.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 2435506.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Step2: Define the Networks"],"metadata":{"id":"nM_bl7XXkDRX"}},{"cell_type":"code","source":["class NetWithoutBN(nn.Module):\n","    def __init__(self):\n","        super(NetWithoutBN, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 100)\n","        self.fc2 = nn.Linear(100, 50)\n","        self.fc3 = nn.Linear(50, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","class NetWithBN(nn.Module):\n","    def __init__(self):\n","        super(NetWithBN, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 100)\n","        self.bn1 = nn.BatchNorm1d(100)\n","        self.fc2 = nn.Linear(100, 50)\n","        self.bn2 = nn.BatchNorm1d(50)\n","        self.fc3 = nn.Linear(50, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        x = self.relu(self.bn1(self.fc1(x)))\n","        x = self.relu(self.bn2(self.fc2(x)))\n","        x = self.fc3(x)\n","        return x\n"],"metadata":{"id":"9kUAYHTmj8Ie","executionInfo":{"status":"ok","timestamp":1714791967179,"user_tz":-540,"elapsed":410,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Step3: Train the Models\n","We'll train both models using the same settings for a fair comparison."],"metadata":{"id":"3kQdT6LUkP-2"}},{"cell_type":"code","source":["def train_model(model, train_loader):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    model.train()\n","\n","    for epoch in range(10):\n","        running_loss = 0.0\n","        for i, (inputs, labels) in enumerate(train_loader):\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n","\n","# Create model instance\n","model_without_bn = NetWithoutBN()\n","model_with_bn = NetWithBN()\n","\n","# Train models\n","print(\"Training model without Batch Normalization:\")\n","train_model(model_without_bn, train_loader)\n","\n","print(\"\\nTraining model with Batch Normalization:\")\n","train_model(model_with_bn, train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJW7WLINkOZB","executionInfo":{"status":"ok","timestamp":1714792534082,"user_tz":-540,"elapsed":352787,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"af439f66-eadd-44a9-dfdc-54d35255d1d3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training model without Batch Normalization:\n","Epoch 1, Loss: 0.4218110325875313\n","Epoch 2, Loss: 0.20883379394470503\n","Epoch 3, Loss: 0.15682862677982748\n","Epoch 4, Loss: 0.1269275237014815\n","Epoch 5, Loss: 0.10774067386206407\n","Epoch 6, Loss: 0.09516789344921787\n","Epoch 7, Loss: 0.0838137591527755\n","Epoch 8, Loss: 0.07666492778490157\n","Epoch 9, Loss: 0.06920000928159216\n","Epoch 10, Loss: 0.06457089920896096\n","\n","Training model with Batch Normalization:\n","Epoch 1, Loss: 0.28356621894778916\n","Epoch 2, Loss: 0.1024788323480056\n","Epoch 3, Loss: 0.0748192254465812\n","Epoch 4, Loss: 0.05807305201898013\n","Epoch 5, Loss: 0.04942067677322934\n","Epoch 6, Loss: 0.041073954941432816\n","Epoch 7, Loss: 0.03470213371075825\n","Epoch 8, Loss: 0.032978087577673204\n","Epoch 9, Loss: 0.029559690331077592\n","Epoch 10, Loss: 0.027585424574850095\n"]}]}]}