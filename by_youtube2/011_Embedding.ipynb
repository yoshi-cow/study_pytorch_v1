{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMIalB1TjHwKKr0YiKLHaV3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What are Embedding Layers in PyTorch\n","* https://www.youtube.com/watch?v=e6kcs9Uj_ps&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi&index=30\n","* https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_06_4_embedding.ipynb\n","\n","Embedding Layers are a handy feature of PyTorch that allows the program to automatically insert additional information into the data flow of your neural network. An embedding layer would automatically allow you to insert vectors in the place of word indexes.\n","\n","\n","Programmers often use embedding layers with Natural Language Processing (NLP); however, you can use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can think of an <u>embedding layer as dimension expansion</u>. However, the hope is that theses additional dimensions provide more information to the model and provide a better score."],"metadata":{"id":"yehuuQrx1n7m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWhx_zbk1fWc"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False"]},{"cell_type":"markdown","source":["# Simple Embedding Layer Example\n","* **num_embeddings** = How large is the vocabulary? How many categories are you encoding? This parameter is the number of items in your \"lookup table\".\n","* **embedding_dim** = How many numbers in the vector you wish to return.\n","\n","\n","Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to 4 number vectors. This neural network does nothing more than passing the embedding on to the output. But it does let us see what the embedding is doing. Each feature vector coming in will have two such features."],"metadata":{"id":"d_6V9jA31vUl"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n","optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=0.001)\n","loss_function = nn.MSELoss()"],"metadata":{"id":"jZhTdemJ3kxE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a look at the structure of this neural network to see what is happening inside it."],"metadata":{"id":"oJhOuMzc3ye1"}},{"cell_type":"code","source":["print(embedding_layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99ENkfy_3wty","executionInfo":{"status":"ok","timestamp":1715669551871,"user_tz":-540,"elapsed":280,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"ef7b4245-84a9-455f-a8fc-a3f44b2b1b34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding(10, 4)\n"]}]},{"cell_type":"markdown","source":["For this neural network, which is just an embedding layer, the input is a vector size 2. These two inputs are interger numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (imput_dim) possible interger values for the two inputs. The output is 2 (input_length) lenght 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n","\n","\n","Now, let us query the neural network with two rows. Thi input is two integer values, as was specified when we created the neural network."],"metadata":{"id":"T_5s7x5y39ng"}},{"cell_type":"code","source":["input_tensor = torch.tensor([[1, 2]], dtype=torch.long)\n","print(input_tensor)\n","print(\"input_tensor.shape: \", input_tensor.shape, '\\n')\n","\n","pred = embedding_layer(input_tensor)\n","print(pred)\n","print(\"pred.shape: \", pred.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bodJV1oa38c-","executionInfo":{"status":"ok","timestamp":1715669955491,"user_tz":-540,"elapsed":261,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"31bbc7e8-915f-4300-8709-2f9b63a46459"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2]])\n","input_tensor.shape:  torch.Size([1, 2]) \n","\n","tensor([[[-1.0776, -0.0567, -1.0366, -0.3445],\n","         [-0.6044, -1.7945, -0.5762, -0.7177]]], grad_fn=<EmbeddingBackward0>)\n","pred.shape:  torch.Size([1, 2, 4])\n"]}]},{"cell_type":"markdown","source":["Here we see two length-4 vectors that PyTorch looked up for each input interger. Recall that Python arrays are zero-based. PyTorch replaced the value of 1 with the second row of the 10 * 4 lookup matrix. Similarly, PyTorch returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathmatical operations other than inserting the correct row from the lookup table."],"metadata":{"id":"mUlmvFAY6BdE"}},{"cell_type":"code","source":["embedding_layer.weight.data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOuys1Ua7Mhr","executionInfo":{"status":"ok","timestamp":1715670409169,"user_tz":-540,"elapsed":16,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"17a5c332-3a8f-4f3b-caec-352b9e000e4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.9302, -0.6667,  0.3645,  1.5885],\n","        [-1.0776, -0.0567, -1.0366, -0.3445],\n","        [-0.6044, -1.7945, -0.5762, -0.7177],\n","        [ 0.7623, -0.4987,  0.1511,  0.1636],\n","        [-0.5855, -0.8876, -1.8424,  1.3046],\n","        [ 0.0557,  0.6229,  0.7430, -2.4860],\n","        [ 0.8600, -1.7739,  0.5993, -0.5418],\n","        [-0.0722,  0.8435,  0.3237, -0.9304],\n","        [-0.6080, -0.5653, -1.3088, -0.2174],\n","        [-1.1741,  1.8047,  1.1985,  0.3427]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["The values above are random parameters that PyTorch generated as starting points. Generally, we will transfer an embedding or train these random values into something useful. The following section demonstrates how to embed a hand-coded embedding."],"metadata":{"id":"O7PbDdYo7JTS"}},{"cell_type":"markdown","source":["# Transferring An Embedding\n","Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding. One-hot encoding would transform the input interger values of **0**, **1**, and **2** to the vectors **[1, 0, 0]**, **[0, 1 ,0]**, and **[0, 0, 1]** respectively.  The following code replaced the random lookup values in the embedding layer with this one-hot coding-inspired lookup table."],"metadata":{"id":"_n1aJxP-7kjj"}},{"cell_type":"code","source":["# Define the embedding lookup matrix\n","embedding_lookup = torch.tensor([\n","    [1, 0, 0],\n","    [0, 1, 0],\n","    [0, 0, 1]\n","], dtype=torch.float32) # Make sure to use float32 for weight matrices\n","\n","# Create the embedding layer\n","embedding_layer = nn.Embedding(num_embeddings=3, embedding_dim=3)\n","\n","# Set the weights of the embedding layer\n","embedding_layer.weight.data = embedding_lookup"],"metadata":{"id":"lgrS-jrp5cW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have the following parameters for the Embedding layer:\n","* input_dim=3 - There are three different integer categorical values allowed.\n","* output_dim=3 - Three columns represent a categorical value with three possible values per one-hot encoding.\n","* input_length=2 - The input vector has two of these categorical values.\n","\n","\n","We query the neural network with two categorical values to see the lookup performed"],"metadata":{"id":"Qao1dBcO8mMb"}},{"cell_type":"code","source":["# Create the input tensor directly in PyTorch\n","input_tensor = torch.tensor([[0, 1]], dtype=torch.long)\n","print(input_tensor)\n","print(\"input_tensor.shape: \", input_tensor.shape, '\\n')\n","\n","# Forward pass to get the predictions\n","pred = embedding_layer(input_tensor)\n","print(pred)\n","print(\"pred.shape: \", pred.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcKnttEq8dHW","executionInfo":{"status":"ok","timestamp":1715670947481,"user_tz":-540,"elapsed":312,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"389af9b0-88c7-4401-b194-493b6cd87817"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1]])\n","input_tensor.shape:  torch.Size([1, 2]) \n","\n","tensor([[[1., 0., 0.],\n","         [0., 1., 0.]]], grad_fn=<EmbeddingBackward0>)\n","pred.shape:  torch.Size([1, 2, 3])\n"]}]},{"cell_type":"markdown","source":["The give output show that we provided the program with two rows from the one-hot encoding table. This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values possible."],"metadata":{"id":"Oru9bxx_9X53"}},{"cell_type":"markdown","source":["# Training an Embedding"],"metadata":{"id":"axllc5f19ok2"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import OneHotEncoder\n","from torch.nn.utils.rnn import pad_sequence"],"metadata":{"id":"uY-Sv9at9mj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create a neural network that classifies restaurant reviews according to positibe or negative. This neural network can accept strings as input, such as given here. This code also includes positive or negative labels for each review."],"metadata":{"id":"QmyU2wKF95lT"}},{"cell_type":"code","source":["# Define 10 resturant reviews.\n","reviews = [\n","    'Never coming back!',\n","    'Horrible service',\n","    'Rude waitress',\n","    'Cold food.',\n","    'Horrible food!',\n","    'Awesome',\n","    'Awesome service!',\n","    'Rocks!',\n","    'poor work',\n","    'Couldn\\'t have done better']\n","\n","# Define labels (1=negative, 0=positive)\n","labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"],"metadata":{"id":"o6xJxVbR94Y1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that the second to the last label is incorrect. Errors such as this are not too out of the oridinary, as most training data could have some noise.\n","\n","\n","We difine a vocaburaly size of 50 words. Though we do not have 50 words, it is okay to use a value larger than needed. If there are more than 50 words, the least frequently used  words in the training set are automatically dropped by the embedding layer during training. For input, we one-hot encode the strings. We use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-Learn would expand these strings to the 0's and 1's as we would typically see for dummy variables. TensorFlow translates all words to index values and replaces each word with that index."],"metadata":{"id":"OkPjexmE-O7x"}},{"cell_type":"code","source":["hash('Never') % 50"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCJ8udexEuOe","executionInfo":{"status":"ok","timestamp":1715672966822,"user_tz":-540,"elapsed":8,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"770fa9ab-3f3f-41af-b694-0ee73f4e38aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# One-hot encode reviews\n","VOCAB_SIZE = 50\n","# `%` - 割り算の余りを算出\n","encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n","\n","print(f\"Encoded reviews: {encoded_reviews}\")\n","# reviewsリストの各要素の単語を数値に変換した結果が以下"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XF7iYMNS-OPw","executionInfo":{"status":"ok","timestamp":1715674621782,"user_tz":-540,"elapsed":268,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"65a05eab-5a55-4e9c-fbd6-8def4b287317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded reviews: [tensor([10,  5, 27]), tensor([33, 29]), tensor([28,  0]), tensor([39,  5]), tensor([33, 21]), tensor([36]), tensor([36, 20]), tensor([19]), tensor([28,  3]), tensor([ 3, 15, 46, 18])]\n"]}]},{"cell_type":"markdown","source":["The program one-hot encodes these reviews to word indexes; however, their lengths are different. We pad these reviews to 4 words and truncate any words beyond the fourth word."],"metadata":{"id":"DDpJ-8roAg9O"}},{"cell_type":"code","source":["MAX_LENGTH = 4\n","padded_reviews = pad_sequence(encoded_reviews, batch_first=True, padding_value=0).narrow(1, 0, MAX_LENGTH)\n","print(padded_reviews)\n","# pad_sequence()で各行ごとに、足りない要素をゼロで埋める"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIUk7q5EAZdt","executionInfo":{"status":"ok","timestamp":1715674624225,"user_tz":-540,"elapsed":274,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"001c6f10-c115-4c40-c632-1d7bbeafc0ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[10,  5, 27,  0],\n","        [33, 29,  0,  0],\n","        [28,  0,  0,  0],\n","        [39,  5,  0,  0],\n","        [33, 21,  0,  0],\n","        [36,  0,  0,  0],\n","        [36, 20,  0,  0],\n","        [19,  0,  0,  0],\n","        [28,  3,  0,  0],\n","        [ 3, 15, 46, 18]])\n"]}]},{"cell_type":"markdown","source":["As specified by the **padding=post** setting, each review is padded by appending zeros at the end, as specified the **padding=post** setting.\n","\n","Next we create a neural network to learn to classify reviews."],"metadata":{"id":"HJ0QXtZyBep7"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Embedding(VOCAB_SIZE, 8),\n","    nn.Flatten(),\n","    nn.Linear(8 * MAX_LENGTH, 1),\n","    nn.Sigmoid()\n",")"],"metadata":{"id":"zJe_cc0RBQj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This network accepts four integer inputs that specify the indexes of a padded movie review. The first embedding layer converts these four indexes into four length vectors 8. These vectors come from the lookup table the contains 50 (VOCAB_SIZE) rows of vectors of length 8. This encoding is evident by the 400 (8 times 50) parameters in the embedding layer. The output size from the embedding layer is 32 (4 words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class classification network, we use the sigmoid activation function and binary_crossentropy.\n","\n","\n","The program now trains the neural network. The embedding lookup and dense 33 weights are updated to produce a better score."],"metadata":{"id":"Sua0Pb3TCJDA"}},{"cell_type":"code","source":["criterion = nn.BCELoss() # Binary Cross Entropy\n","optimizer = optim.Adam(model.parameters())\n","\n","# Training the model\n","epochs = 100\n","for epoch in range(epochs):\n","    optimizer.zero_grad()\n","    outputs = model(padded_reviews.long())\n","    loss = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float))\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"id":"H6VDXeb6CEul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see the learned embeddings. Think of each word's vector as a location in the 8 dimension space where words associated with positive reviews are close to other words. Similarly, training places negative reviews close to each other. In addition to the training setting these embeddings, the 33 weights between the embedding layer and output neuron similarly learn to transform these embeddings into an actual prediction. You can see these embeddings here."],"metadata":{"id":"JnyQOZ_hKBRu"}},{"cell_type":"code","source":["embedding_weights = list(model[0].parameters())[0]\n","print(embedding_weights.shape)\n","print(embedding_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWrEJ2s9Juap","executionInfo":{"status":"ok","timestamp":1715674662593,"user_tz":-540,"elapsed":311,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"0029f716-de57-4205-8f7a-2f480ee2fc8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([50, 8])\n","Parameter containing:\n","tensor([[-1.6768, -1.9031,  0.0252, -0.5319,  0.7442,  1.2344,  1.2802,  0.7962],\n","        [ 0.6082,  1.5734,  0.8852,  1.5405, -2.1552, -0.1733, -2.5363, -1.8048],\n","        [-0.1492,  1.6089,  0.1645, -1.0403,  0.2712, -1.6028,  0.1053, -0.7191],\n","        [ 0.6954, -1.6610,  0.2845,  1.5048, -0.6243, -0.6798, -0.3955, -1.2092],\n","        [-1.4907,  0.9310, -1.1549,  1.8388, -0.4695,  0.3294, -1.2787, -1.2318],\n","        [-0.6396, -1.8355,  1.7043,  0.1649, -0.2356,  2.3743,  0.1703,  0.0206],\n","        [ 1.3712,  1.5142, -1.3969,  0.1835,  1.2148,  0.2743, -0.4599, -0.8549],\n","        [-0.4438,  2.9385,  0.5593, -0.5218,  0.4831,  0.2573,  0.8810,  0.8127],\n","        [ 1.2870,  0.4554,  0.8146, -1.7634,  0.6124, -0.5731,  0.3127, -0.4764],\n","        [-0.4755,  0.0245,  0.8558, -1.7638, -1.3385, -0.0087,  2.2258, -0.0103],\n","        [ 0.6813,  0.8824, -0.3097,  1.1874,  0.0993, -0.5140, -0.5533, -0.8991],\n","        [-0.0908, -1.2281,  1.0500,  0.6610, -1.4649,  0.2212,  1.7743,  0.6779],\n","        [ 1.2071,  0.2756, -0.2140,  0.1028, -0.5048, -0.0397, -1.3507, -0.8505],\n","        [ 1.0651,  0.5209,  2.0131, -0.1079,  1.5516, -1.1672,  1.1168, -1.0345],\n","        [-1.4373, -0.6045, -0.5518,  0.0914,  0.7340, -0.5626,  0.8511,  1.0198],\n","        [ 0.8860, -0.8141, -2.0018, -1.1789,  0.3858,  0.0087,  0.1968,  0.9700],\n","        [-1.0607, -0.9928,  0.3572, -1.1780, -0.9809, -0.7212,  0.3429,  2.6140],\n","        [-0.9631, -0.3540,  0.4112,  0.1381,  0.1556, -0.2168, -1.3339, -0.2911],\n","        [ 0.8132, -1.1159,  0.7731,  1.1610,  0.1942, -0.0579,  0.0351, -0.9738],\n","        [ 0.8570,  0.6611, -0.1332,  3.1691, -0.2940,  0.1413, -1.6355,  0.6197],\n","        [ 0.5026, -0.3630,  1.5019, -1.2236, -0.7388, -2.1049, -2.9198, -0.6077],\n","        [-1.7121,  1.1900,  0.1451,  0.1295,  1.6857,  0.3442, -0.1823,  1.5164],\n","        [-0.0968, -0.2384,  0.6267, -0.6336, -1.0708, -0.9010,  0.1539, -1.0828],\n","        [ 1.3981, -1.3790, -1.5512,  0.5412, -1.2060, -0.8258, -0.9301,  0.1111],\n","        [-0.3535, -0.8407, -0.4108, -0.2852, -0.4100,  0.3485, -0.3155,  0.8710],\n","        [ 1.2353, -1.0198, -0.2795,  0.7739,  0.0062, -2.5315,  0.8168,  0.4283],\n","        [-0.1208,  0.2596,  0.7500,  0.9017,  1.5175,  0.3532, -0.9871,  0.3400],\n","        [-0.4101,  0.2876,  0.1694,  0.6072, -0.9165,  1.4394, -1.4988, -1.0762],\n","        [ 0.7774,  2.1285, -0.2690,  0.2541,  0.3612,  1.1878,  1.7531,  1.6090],\n","        [ 0.2917,  1.5863, -0.3273,  1.1030,  1.3771, -0.2699, -0.4868,  1.4601],\n","        [-0.1036, -1.1122,  0.0860,  1.2805, -0.2736,  0.8332, -0.4771, -0.0778],\n","        [ 0.6066, -0.9421, -2.1699,  1.0159,  1.6427, -2.1860,  1.3736, -0.0905],\n","        [-0.1821,  0.8496, -0.1632,  1.5865,  0.1377,  0.8606, -0.6385,  0.4083],\n","        [ 0.1897, -1.5022,  1.9282,  1.1888,  0.1758,  1.2454,  0.4755, -0.6290],\n","        [ 1.1034, -0.2738, -0.5887,  2.0678,  1.4990,  0.0864, -0.2210, -0.6345],\n","        [ 0.3417,  0.4183, -1.1494, -0.9344, -2.2479,  0.8518, -1.2213,  1.3206],\n","        [ 0.1941, -0.5253, -0.1182,  1.0176, -0.9688, -1.5336, -1.2798,  1.3068],\n","        [ 0.1599, -0.2427, -1.1694, -1.2168, -1.1640,  0.4367, -0.8990, -1.3766],\n","        [ 1.0014, -0.4049,  0.0094,  0.0482,  0.0169, -0.9616,  1.0647,  1.1170],\n","        [-0.5794,  0.1048, -1.9221, -0.3010, -1.7307,  0.1912,  2.5871, -0.9536],\n","        [ 0.3040,  0.5629,  1.0868, -0.3143,  0.6906, -2.1352,  0.2795, -0.1663],\n","        [-0.1158,  1.1874, -0.4437,  0.5990, -1.7543,  0.3951, -0.7228, -1.4140],\n","        [-1.4568,  1.2722,  0.7505, -0.4333, -2.0336,  0.0457, -1.5770, -0.7332],\n","        [ 1.8772,  0.9929,  0.7768, -1.6032,  0.5319,  0.2605, -1.0865,  0.7663],\n","        [ 1.3679, -0.4274, -0.2146, -0.5448, -1.3007,  1.5698,  0.2629, -0.5016],\n","        [-0.3285, -0.0849, -1.4859, -0.6186, -1.9616, -0.6700, -1.1091, -2.4013],\n","        [-0.2434, -0.7885, -0.1361, -1.6380,  0.3604, -1.7997, -1.1582,  1.5624],\n","        [ 0.0288, -0.7773, -0.9003, -0.4276, -0.4841, -0.0738,  0.0871, -2.4891],\n","        [-1.2998, -0.0152, -0.1042, -0.2775, -0.5310,  0.8606, -0.3071,  0.5265],\n","        [-1.7185,  0.5021, -1.5409,  0.1173,  0.6891, -0.2446, -1.6810, -0.0159]],\n","       requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["We can now evaluate this neural network's accuracy, including the embeddings and the learned dense layer."],"metadata":{"id":"LTf1uY0ZKrTq"}},{"cell_type":"code","source":["# Evaluation\n","with torch.no_grad():\n","    outputs = model(padded_reviews.long())\n","    predicted_labels = (outputs > 0.5).float().squeeze()\n","    accuracy = (predicted_labels == torch.tensor(labels)).float().mean().item()\n","    loss_value = criterion(outputs.squeeze(), torch.tensor(labels, dtype=torch.float)).item()\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Loss: {loss_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4eTEM6V2KobV","executionInfo":{"status":"ok","timestamp":1715674671599,"user_tz":-540,"elapsed":346,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7609643c-ce69-4569-8ed2-50cc7ead1540"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0\n","Loss: 0.3505394160747528\n"]}]},{"cell_type":"markdown","source":["The accuracy is greate, but there could be overfitting. It would be good to use early stopping to not overfit for a more complex data set. However, the loss is not perfect. Even though the predicted probabilities indicated a correct prediction in every case, the program did not achieve absolute confidence in each correct answer. The lack of confidence was likely due to the small amount of noise in the data set. Some words that appeared in both positive and negative reviews contributed to this lack of absolute certainty."],"metadata":{"id":"C9Pnq1zLLhnF"}},{"cell_type":"markdown","source":["## About One-hot Encode by `[torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]`\n","The goal is to convert the texual data(reviews) into a numerical format that can be used as input for the neural network.\n","Here is the detailed explanation of the code:\n","```\n","VOCAB_SIZE = 50\n","encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n","```"],"metadata":{"id":"P9cn0z9HOn0l"}},{"cell_type":"markdown","source":["### Components:\n","1. **VOCAB_SIZE**:\n","```\n","VOCAB_SIZE = 50\n","```\n","    * This sets the vocablary size to 50. It means that we will represent each word by an index ranging from 0 to 49.\n","\n","2. **Encoding Reviews**:\n","```\n","encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n","```\n","    * Here, we are converting each review (a string) into a tensor of integers (indices).\n","\n","### Detailed Breakdown:\n","1. **Splitting Reviews into Words**:\n","```\n","review.split()\n","```\n","    * For each review, **`split()`** method is called. This splits the review string into a list of words. For example, **\"Never coming back!\" becomes **[\"Never\", \"coming\", \"back!\"]**.\n","\n","2. **Hashing and Modulo Operation**:\n","```\n","[hash(word) % VOCAB_SIZE for word in review.split()]\n","```\n","    * For each word in the split review, we apply the **`has`** function.\n","    * The **`has`** function generates a unique interger for each word.\n","    * We then use the modulo operation **`% VOCAB_SIZE`** to map the hash value to an index within the range of **0** to **49** (since **VOCAB_SIZE** is 50). This ensures that each word is represented by an index that fits within our defined vocabulary size.\n","    * For example, if **hash(\"Never\") returns **135798642**, then **`135798642 % 50`** might return **42**, so **\"Never\"** is represented by the index **42**.\n","\n","3. **Creating Tensors**:\n","```\n","torch.tensor([...])\n","```\n","    * After computing the indices for all words in a review, we wrap the list of indices in **torch.tensor** to create a tensor.\n","    * Each review is now represented as a tensor of integers.\n","\n","### Example:\n","Let’s take a concrete example to see how a review is encoded.\n","\n","**Review**: `\"Never coming back!\"`\n","\n","**Split into Words**: `[\"Never\", \"coming\", \"back!\"]`\n","\n","**Hash and Modulo Operation**:\n","- `hash(\"Never\") % 50 = 42`\n","- `hash(\"coming\") % 50 = 17`\n","- `hash(\"back!\") % 50 = 9`\n","\n","**Encoded Review**:\n","- Tensor: `torch.tensor([42, 17, 9])`\n","\n","**Applying this to all reviews**:\n","The process is repeated for each review in the list, resulting in `encoded_reviews`, a list of tensors.\n","\n","### Summary\n","This step converts each word in the reviews into a numerical index based on its hash value and maps it into a fixed vocabulary size. This process ensures that the textual data is transformed into a numerical format that can be fed into the neural network."],"metadata":{"id":"MhU1AFi8OxvT"}},{"cell_type":"markdown","source":["## Why does `hash(word) % VOCAB_SIZE for word`\n","\n","The calculation **`hash(word) % VOCAB_SIZE`** is used to map words to integer indices within a fixed range. Here are the reasons for using this approach:\n","\n","( **x % y**: x/yの剰余)"],"metadata":{"id":"Erjfir8BVAq_"}},{"cell_type":"markdown","source":["### Reason for Hashing and Modulo Operation\n","1. **Fixed Vocablary Size**:\n","    * **Purpose**: Neural networks typically require <u>fixed-size input dimensions</u>.\n","    * **Reason**: By using the the modulo operation with a fixed **VOCAB_SIZE**, we ensure that all words are mapped to indices within the range **[0, VOCAB_SIZE - 1]**. This keeps the input dimension consistent.\n","2. **Handling Unknown Words**:\n","    * **Purpose**: Real-world data often contains a vast number of unique words, including typos, slang, and rare words.\n","    * **Reason**: the hashing approach allows the model to handle any word, even those not seed during training, by mapping them to one of the fixed indices. This is a way to deal with out-of-vocabulary words without requiring explicit handling.\n","3. **Simplicity and Efficiency**:\n","    * **Purpose**: Directly converting words to indices should be computationally efficient.\n","    * **Reason**: Hash functions provide a simple and quick way to convert strings(words) into integers. The modulo operation ensures the result stays within the desired range. This combination is both fast and easy to implement.\n","4. **Avoiding Large Memory Usage**:\n","    * **Purpose**: Managing large vocabulary size can lead to significant memory usage.\n","    * **Reason**: By limiting the vocabulary size to a manageable number(**VOCAB_SIZE**), we prevent the model from needing to handle an excessively large number of unique indices, which can be memory-intensive.\n","\n","### Example for Clarity\n","Imagine we have a vocabulary size of 50(VOCAB_SIZE=50). Here's a step-by-step example of how a word is mapped to an index:\n","1. **Word**: \"`delicious`\"\n","2. **Hash Calculation**:\n","    * `hash(\"delicious\")` might return a large integer, e.g., `103728491`.\n","3. **Modulo Operation**:\n","    * `103728491 % 50` results in an index within `[0, 49]`, e.g., `41`.\n","\n","\n","This process ensures that no matter how large the hash value, the final index will always ift within the range defined by **VOCAB_SIZE**.\n","\n","\n","### Why Not Using Direct Indices or One-Hot Encoding?\n","1. **Direct Indices**:\n","    * If we were to assign a unique index to every word directly, the vocabulary could become very large, especially in applications with diverse and extensive text data. This would require a correspondingly large embedding matrix and could lead to significant computational and memory challenges.\n","2. **One-Hot Encoding**:\n","    * One-hot encoding would result in <u>very sparse vectors(mostly zeros)</u> with a size equal to the vocabulary. For large vocabularies, this approach is highly inefficient in terms of memory and computation. Instead, using embeddings allows us to represent words in as dense, lower-dimensional space.\n","\n","\n","### Summary\n","The use of `hash(word) % VOCAB_SIZE` allows for a fixed-size, efficient, and straightforward way to convert words into numerical indices. This ensures consistency, handles out-of-vocabulary words gracefully, and maintains computational efficiency. This approach is particularly useful in scenarios with large or dynamic vocabularies.\n"],"metadata":{"id":"OCqwHQWzVHRe"}},{"cell_type":"code","source":[],"metadata":{"id":"w9Q2OdAJLDPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## What is Modulo Operation\n","\n","The \"modulo operation\" is a mathematical operation that finds the remainder of the division of one number by another. In other words, for two numbers (a) and (b), the expression (a % b) (read as \"a modulo b\") returns the remainder when (a) is divided by (b).\n","\n","### How the Modulo Operation Works\n","1. **Basic Definition**:\n","    * If (a) and (b) are intergers, then (a % b) gives the remainder when (a) is divided by (b).\n","    * The formula can be written as:\n","$$\n","a \\% b = a - \\left\\lfloor \\frac{a}{b} \\right\\rfloor \\times b\n","$$\n","    * Here, $\\left\\lfloor \\frac{a}{b} \\right\\rfloor$ represents the floor function, which gives the largest integer less than or equal to $\\frac{a}{b}$.\n","\n","2. **Example Calculation**:\n","    * 10 % 3:\n","        * $10 \\div 3$ is $3$ with a remainder of $1$.\n","        * So, 10 % 3 = 1\n","    * 20 % 5:\n","        * $20 \\div 5$ is $4$ with no remainder.\n","        * So, 20 % 5 = 0\n","    * 7 % 4:\n","        * $ 7 \\div 4$ is $1$ with a remainder of $3$.\n","        * So, 7 % 4 = 3\n","\n","### Why Use the Modulo Operation?\n","In the context of the provided code, the modulo operation is used to ensure that the result of hashing a word 8which can produce a very large integer) fits within a predefined range, namely, the size of the vocabulary (**VOCAB_SIZE**). This is crucial for maintaining consistency and preventing excessively large indices.\n","\n","### Applying Modulo in the Code\n","Here's a step-by-step breakdown of how the modulo operation is used in your code:\n","\n","```python\n","VOCAB_SIZE = 50\n","encoded_reviews = [torch.tensor([hash(word) % VOCAB_SIZE for word in review.split()]) for review in reviews]\n","```\n","\n","1. **Hash Function**:\n","   - `hash(word)` generates a large integer value based on the input word.\n","   - Example: `hash(\"delicious\")` might return `103728491`.\n","\n","2. **Modulo Operation**:\n","   - `hash(word) % VOCAB_SIZE` maps the large integer to a smaller range.\n","   - If `VOCAB_SIZE` is 50, the result will be between 0 and 49.\n","   - Example: `103728491 % 50` results in `41`.\n","\n","By using the modulo operation, we ensure that every word is represented by an index within the fixed range `[0, VOCAB_SIZE - 1]`. This keeps the vocabulary size manageable and the input dimensions consistent for the neural network."],"metadata":{"id":"G8LiKHr4vDXf"}},{"cell_type":"code","source":[],"metadata":{"id":"TcHzlKdLWaCL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"obL8huc0WZ5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"EWdogm-TWNIJ"}}]}