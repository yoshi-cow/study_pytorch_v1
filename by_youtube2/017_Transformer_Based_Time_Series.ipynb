{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPt8I6rDifE8A6insCIW6cr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformers for TimeSeries\n","* https://www.youtube.com/watch?v=NGzQpphf_Vc&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi&index=49\n","* https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_10_3_transformer_timeseries.ipynb"],"metadata":{"id":"YJXfmNv6YeNj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBuktX2_Ms2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717215601662,"user_tz":-540,"elapsed":721,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"d9cd1bf8-e103-42dd-ba62-4a64401db5aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Note: using Google CoLab\n","Using device: cuda\n"]}],"source":["try:\n","    import google.colab\n","    COLAB = True\n","    print(\"Note: using Google CoLab\")\n","except:\n","    print(\"Note: not using Google CoLab\")\n","    COLAB = False\n","\n","# Make use of a GPU or MPS (Apple) if one is available.  (see module 3.2)\n","import torch\n","has_mps = torch.backends.mps.is_built()\n","device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","source":["The transformative landscape of deep learning has witnessed monumental strides(進展) in the recent past, particulary in the domain of Natural Langeage Processing (NLP). Central to this revolution has been the advent(出現) of transformer architectures, which, with their attention mechanisms, have pushed teh boundaries of what's achievable in tasks like machine translation, sentiment analysis, and language modeling. However, while transformers initially rose to prominence primarily within the realm of NLP, their models for time-series predictions--a challenge that, though numerically distinct, bears conceptual resemblance to understanding sequences in language.\n","\n","In time-series prediciton, the objective often centers around forecasting future values based on historical data. This could involve predicting stock prices, weather patterns, or even the consumption of electricity in a region. At its core, this is a sequence-to-sequence task, where the past values form an input sequence and the future values to be predicted form an output sequence. Now, consider the silmilarities with machine translation in NLP, where an input sequence (sentence) in one language is translated into an output sequence in another language. Both scenarios require the model to recognize patterns, interdependencies, and context across sequences.\n","\n","This chapter delves deep into the nuances of using PyTorch transformers for time-series prediction. We will embark(乗船する) on this journey by first establishing a foundational understanding of how transformers operate within NLP space, before segueing into their adaptation for numeric sequences. By juxtaposing these two applications, readers will gain a comprehensive appreciation of the transformer's versatility(多用途性) and the subtle considerations required when transitioning from text to time.\n","\n"],"metadata":{"id":"urUBWYcaY5EN"}},{"cell_type":"markdown","source":["## Load Sun Spot Data for a Transformer Time Series"],"metadata":{"id":"EhT7GkM-slRm"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.preprocessing import StandardScaler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","names = ['year', 'month', 'day', 'dec_year', 'sn_value',\n","         'sn_error', 'obs_num', 'unused1']\n","df = pd.read_csv(\n","    \"https://data.heatonresearch.com/data/t81-558/SN_d_tot_V2.0.csv\",\n","    sep=';', header=None, names=names,\n","    na_values=['-1'], index_col=False)"],"metadata":{"id":"NGjW1oTYsqxk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data preprocessing is the same as was introduced in the previous section. We will use data before the year 2000 as training, the rest is used for validation."],"metadata":{"id":"rpLA-asbsuRd"}},{"cell_type":"code","source":["# Data Preprocessing\n","start_id = max(df[df['obs_num'] == 0].index.tolist()) + 1\n","df = df[start_id:].copy()\n","df['sn_value'] = df['sn_value'].astype(float)\n","df_train = df[df['year'] < 2000]\n","df_test = df[df['year'] >= 2000]\n","\n","spots_train = df_train['sn_value'].to_numpy().reshape(-1, 1)\n","spots_test = df_test['sn_value'].to_numpy().reshape(-1, 1)\n","\n","scaler = StandardScaler()\n","spots_train = scaler.fit_transform(spots_train).flatten().tolist()\n","spots_test = scaler.transform(spots_test).flatten().tolist()"],"metadata":{"id":"x920I2qosy8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just like we did for LSTM in the previous section, we again break the data into sequences."],"metadata":{"id":"m0dOxggVs0Fu"}},{"cell_type":"code","source":["# Sequence Data Preparation\n","SEQUENCE_SIZE = 10\n","\n","def to_sequences(seq_size, obs):\n","    x = []\n","    y = []\n","    for i in range(len(obs) - seq_size):\n","        window = obs[i:(i + seq_size)]\n","        after_window = obs[i + seq_size]\n","        x.append(window)\n","        y.append(after_window)\n","\n","    return torch.tensor(x, dtype=torch.float32).view(-1, seq_size, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1)\n","\n","x_train, y_train = to_sequences(SEQUENCE_SIZE, spots_train)\n","x_test, y_test = to_sequences(SEQUENCE_SIZE, spots_test)\n","\n","# Setup data loaders for batch\n","train_dataset = TensorDataset(x_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","test_dataset = TensorDataset(x_test, y_test)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"K-fQIO8xs12P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x, y in train_loader:\n","    print(f\"Shape of x [batch, time, features]: {x.shape}\")\n","    print(f\"Shape of y [batch, features]: {y.shape}\\n\")\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDsVw4QMtqhH","executionInfo":{"status":"ok","timestamp":1717212461331,"user_tz":-540,"elapsed":265,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"cff10610-3516-4851-e71d-9ccd81304969"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x [batch, time, features]: torch.Size([32, 10, 1])\n","Shape of y [batch, features]: torch.Size([32, 1])\n","\n"]}]},{"cell_type":"markdown","source":["## Positional Encoding for Transformers\n","In the realm of the transformer architecture, a pivotal component that ensures the model's success is its ability to consider the sequence's order. Unlike traditional RNNs or LSTMs, which process sequences step-by-step and inherently respect their order, transformers process all tokens in a sequence simultaneously. While this parallel processing significantly boosts computational efficiency and allows for long-range dependencies to be captured more effectively, it also means that transformers, in their native form, are oblivious(気づかない) to the position or ordder of tokens in a sequence. This is where the concept of positional encoding comes into play.\n","\n","Positional encoding is a mechanism to provide the transformer with information about the position of tokens within a sequence. Essentially, it infuses(注ぐ) order information into the otherwise position-agnostic embeddings. By adding positional encodings to the token embeddings before feeding them into the transformer, each token's position in the sequence becomes discernible(識別可能な) to the model.\n","\n","Positional encodings are vectors that get added to the embeddings of tokens. The intuition is to design these vectors in such a way their values or patterns are unique for each position, allowing the model to differentiate between different positions in the sequence.\n","\n","A popular method to generate positional encodings is using sinusoidal functions. For a given position $p$  in the sequence and dimension $d$ of the embedding, the positional encoding is computed as:\n","$$ PE(2,i) = \\sin(\\frac{p}{10000^{2i/d}}) $$\n","$$ PE(2,i+1) = \\cos(\\frac{p}{10000^{2i/d}}) $$\n","\n","Where $i$ is the dimension index. These sinusoidal function generate values between -1 and 1 and ensure a unique and repeatable pattern for each position.\n","\n","The choice of sinusoidal functions isn't arbitary(任意の). They have two compelling properties:\n","1. They produce values between -1 and 1, making them compatible with most embedding value ranges.\n","2. Their patterns allow the model to extrapolate positions beyond the sequence lengths seen during training.\n","\n","One might wonder, why not just append or add the token's position as an integer to the embedding? The challenge with this approach is scale. Embedding values, especially after being trained, can exist within a specific range, and directly adding large integers (for tokens further down in long sequences) might disrupt(混乱させる) the information in the original embeddings.\n","\n","Furthermore, using raw intergers wouldn't provide a consistent way for the model to generalize or extrapolate to sequence lengths not seen during training. Sinusoidal functions, in contrast, offer a predictable pattern that aids in such extrapolation.\n","\n","The following code describes a simple implementation of a transformer-based model using PyTorch's built-in functionnalities. The **TransformerModel** class encapsulates a transformer-based neural network designed for sequence processing. Upon initialization, the model sets up several components: an encoder to adjust the input data to a desired dimensions, a **pos_encoder** to bestow(授ける) the sequence with positional information, a core **transformer_encoder** comprising several layers to process the sequence, and a **decoder** to produce the final output. As data flows through the model during the forward pass, it undergoes a series of transformations: it's first projected to a higher dimension, then augmented(増加させる) with positional encodings, processed by the transformer layers, and finally, the last token's representation is harnessed to produce the output. An instance of this model is readily created and can be assigned to a comuputaion device for further training or inference."],"metadata":{"id":"xH-FiqGYtunc"}},{"cell_type":"code","source":["# Positional Encoding for Transformer\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"metadata":{"id":"ZwsNLmMttrzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Constructing the Trnsformer Model\n","The following code constructs the actual transformer-based model for time series prediction. The model is constructed to accept the following parameters.\n","* **input_dim**: The dimension of the input data, in this case we use only one input, the number of sunspots.\n","* **d_model**: The number of features in the transformer model's internal representations (also the size of embeddings). This controls how much a model can remember and process.\n","* **nhead**: The number of attention heads in the multi-head self-attention mechanism.\n","* **num_layers**: The number of transformer encoder layers.\n","* **dropout**: The dropout probability."],"metadata":{"id":"rPYkp7Vc19-m"}},{"cell_type":"code","source":["# Model definition using Trnsformer\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n","        super(TransformerModel, self).__init__()\n","\n","        self.encoder = nn.Linear(input_dim, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, dropout)\n","        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n","        self.decoder = nn.Linear(d_model, 1)\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.pos_encoder(x)\n","        x = self.transformer_encoder(x)\n","        x = self.decoder(x[:, -1, :])\n","        return x\n","\n","model = TransformerModel()\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrYmDUTJ29QH","executionInfo":{"status":"ok","timestamp":1717215608332,"user_tz":-540,"elapsed":562,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"e8ad8504-a5a2-4b5b-a624-7973b30d8265"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TransformerModel(\n","  (encoder): Linear(in_features=1, out_features=64, bias=True)\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-1): 2 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n","        )\n","        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n","        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["The transformer architecture in PyTorch is governed by crucial configuration choices, among which **d_model**, **n_head**, and **num_layers** hold significant weight.\n","\n","The **d_model** denotes the dimensionality of the input embeddings and affects the model's capacity to learn intricate representations.\n","While a more substantial **d_model** can bolster(強化する) the richness of the model's understanding, it also amplifies(増幅する) the computational demand and can pose overfitting risks if not carefully chosen. Parallely, the model's gradient flow and initialization are impacted by this choice, though the Transformer's normalization layers often moderate potential issues.\n","\n","\n","On the other hand, **n_head** reflects the count of heads in the multi-head attention mechanism. A higher number of heads grants the model the prowess to simultaneously focus on diverse segments of the input, enabling the capture of varied contextual nuances. However, there's a trade-off. Beyond a specific threshold, the computational overhead might outweigh the marginal(ギリギリの) performance gains. This parallel processing, provided by multiple attention heads, tends to offer more stable and varied gradient information, positively influencing the training dynamics.\n","\n","\n","Lastly, the **num_layers** parameter dictates the depth of the Transformer, determining the number of stacked encoder or decoder layers. A deeper model, as a result of increased layers, can discern(識別する) more complex and hierarchical relationships in data. Still, there's a caveat（警告): after a certain depth, potential performance enhancements may plateau(停滞する) and the risk of overfitting might escalate. Training deeper models also comes with its set of challenges. Although residual connections and normalization in Transformers alleviate some concerns, a high layer count might necessitate techniques like gradient clipping or learning rate adjustments for stble training.\n","\n","\n","In essence, these parameters intricately(複雑に) balance model capacity, computational efficiency, and generalization capability. Their optimal settings often emerge from task-specific experimentation, the nature of the data, and available computational prowess(力量).\n","\n"],"metadata":{"id":"lTAhJE0J5GHD"}},{"cell_type":"markdown","source":["## Training the Model\n","Training a transformer-based model adheres(を遵守する、くっつく) to many of the familiar paradigms and best practices that apply to other neural network architectures. Much like the models we've encountered before, a transformer-based model benefits from training in batches, which helps in both computational efficiency and generalization. Batched training ensures that the model updates its weights based on teh average gradient over several data points, rather than being excessively influenced by any single instance. Additionally, the use of early stopping acts as a safeguard against overfitting. By monitoring the model's performance on a validation set and halting training when no significant improvement is observed over a set number of epochs, we ensure that the model generalizes well and doesn't just memorize the training data. The validation set, it remains an essential component in the training regimen, providing a proxy measure of the model's performance on unseen data and guiding hyperparameter tuning. Thus, while transformer architectures introduce novel mechanisms and complexities, the foundational principles of training deep learning models in PyTorch remain consistent."],"metadata":{"id":"c5EQo8_D90-w"}},{"cell_type":"code","source":["# Train the model\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n","\n","epochs = 1000\n","early_stop_count = 0\n","min_val_loss = float('inf')\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in train_loader:\n","        x_batch, y_batch = batch\n","        x_batch = x_batch.to(device)\n","        y_batch = y_batch.to(device)\n","\n","        optimizer.zero_grad()\n","        y_pred = model(x_batch)\n","        loss = criterion(y_pred, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            x_batch, y_batch = batch\n","            x_batch = x_batch.to(device)\n","            y_batch = y_batch.to(device)\n","            y_pred = model(x_batch)\n","            loss = criterion(y_pred, y_batch)\n","            val_losses.append(loss.item())\n","\n","    val_loss = np.mean(val_losses)\n","    scheduler.step(val_loss)\n","\n","    if val_loss < min_val_loss:\n","        min_val_loss = val_loss\n","        early_stop_count = 0\n","    else:\n","        early_stop_count += 1\n","\n","    if early_stop_count >= 5:\n","        print(\"Early stopping!\")\n","        break\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMwTv6aM3ZGw","executionInfo":{"status":"ok","timestamp":1717215826575,"user_tz":-540,"elapsed":207655,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"9e7831eb-e545-49fa-8c2f-a3693d910134"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1000, Train Loss: 0.3037, Val Loss: 0.3037\n","Epoch 2/1000, Train Loss: 0.0413, Val Loss: 0.0413\n","Epoch 3/1000, Train Loss: 0.1478, Val Loss: 0.1478\n","Epoch 4/1000, Train Loss: 0.0707, Val Loss: 0.0707\n","Epoch 5/1000, Train Loss: 0.0511, Val Loss: 0.0511\n","Epoch 6/1000, Train Loss: 0.0227, Val Loss: 0.0227\n","Epoch 7/1000, Train Loss: 0.0334, Val Loss: 0.0334\n","Epoch 8/1000, Train Loss: 0.1481, Val Loss: 0.1481\n","Epoch 9/1000, Train Loss: 0.0855, Val Loss: 0.0855\n","Epoch 10/1000, Train Loss: 0.0348, Val Loss: 0.0348\n","Epoch 11/1000, Train Loss: 0.0141, Val Loss: 0.0141\n","Epoch 12/1000, Train Loss: 0.0850, Val Loss: 0.0850\n","Epoch 13/1000, Train Loss: 0.0859, Val Loss: 0.0859\n","Epoch 14/1000, Train Loss: 0.0530, Val Loss: 0.0530\n","Epoch 15/1000, Train Loss: 0.1756, Val Loss: 0.1756\n","Early stopping!\n"]}]},{"cell_type":"markdown","source":["We can now evaluate the performance of this model."],"metadata":{"id":"qhupAQ-kB3lP"}},{"cell_type":"code","source":["# Evaluation\n","model.eval()\n","predictions = []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        x_batch, y_batch = batch\n","        x_batch = x_batch.to(device)\n","        # y_batch = y_batch.to(device)\n","        y_pred = model(x_batch)\n","        predictions.extend(y_pred.squeeze().tolist())\n","\n","rmse = np.sqrt(np.mean((scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))\n","print(f\"Score (RMSE): {rmse:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VU7wSw28B2Od","executionInfo":{"status":"ok","timestamp":1717215855096,"user_tz":-540,"elapsed":516,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"d0777247-f609-441d-cc8c-07d4fc17418f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Score (RMSE): 14.8968\n"]}]},{"cell_type":"markdown","source":["# The detail of Positional Encoding\n","```python\n","# Positional Encoding for Transformer\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","```"],"metadata":{"id":"_nkvd5KCHgaT"}},{"cell_type":"markdown","source":["## 1. Class Initialization\n","```python\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","```\n","* P**ositionalEncoding(nn.Module)**: This defines a new class PositionalEncoding that inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n","* **__init__(self, d_model, dropout=0.1, max_len=5000)**: The constructor method initializes the class with three parameters: d_model (the dimension of the model), dropout (dropout rate), and max_len (the maximum length of the sequences).\n","* **super(PositionalEncoding, self).__init__()**: This line calls the constructor of the parent class nn.Module.\n","* **self.dropout = nn.Dropout(p=dropout)**: This line initializes a dropout layer with the given dropout rate.\n","\n","## 2. Creating the Positional Encoding Matrix\n","```python\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","```\n","* **pe = torch.zeros(max_len, d_model)**: This creates a tensor **pe** of shape **(max_len, d_model)** filled with zeros. This tensor will hold the positional encoding\n","* **postion = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)**: This creates a tensor of shape **(max_len, 1)** constaining position indices from 0 to max_len-1.\n","* **div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0 / d_model))**: This creates a tensor of shape **(d_model/2)** containing the exponential terms used to scale the positions.\n","\n","```python\n","torch.arange(0, 5, dtype=torch.float)\n","# -> tensor([0., 1., 2., 3., 4.])\n","\n","torch.arange(0, 5, dtype=torch.float).unsqueeze(1)\n","# -> tensor([[0.],\n","#        [1.],\n","#        [2.],\n","#        [3.],\n","#        [4.]])\n","\n","torch.arange(0, 5, 2).float()\n","# -> tensor([0., 2., 4.])\n","```\n","\n","## 3. Applying Sine and Cosine Functions\n","```python\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","```\n","* **pe[:, 0::2] = torch.sin(position * div_term)**: This applies the sine function to even-indexed dimensions of **pe**.\n","* **pe[:, 1::2] = torch.cos(position * div_term)**: This pplies the cosine function to odd-indexed dimensions of **pe**.\n","\n","## 4. Reshaping and Registering the Buffer\n","```python\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","```\n","* **pe = pe.unsqueeze(0).transpose(0, 1)**: This adds a new dimension to **pe** and then transpose it. The shape of **pe**  becomes **(max_len, 1, d_model)**.\n","* **self.register_buffer('pe', pe)**: This registers **pe** as a buffer in the module. Buffers are tensors that are not considered model parameters but are part of the module's state.\n","\n","## 5. Forward Method\n","```python\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","```\n","* **def forward(self, x)**: The **forward** method defines the computation performed at every call of the module.\n","* **x = x + self.pe[:x.size(0), :]**: This lines adds the positional encoding to the input tensor **x**.\n","* **return self.dropout(x)**: This applied dropout to the output tensor and returns it."],"metadata":{"id":"oXBsJmCOHq-Z"}},{"cell_type":"markdown","source":["## Creating Synthetic Data and Running the Class"],"metadata":{"id":"sYbGHnbNfWFF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# Positional Encoding for Transformer\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","# Synthetic data\n","batch_size = 2\n","seq_len = 10\n","d_model = 16\n","\n","# Create a random tensor with the shape (seq_len, batch_size, d_model)\n","x = torch.randn(seq_len, batch_size, d_model)"],"metadata":{"id":"UWHk2rM3HjWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IjbkWJdlIhzZ","executionInfo":{"status":"ok","timestamp":1717223580367,"user_tz":-540,"elapsed":283,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"db53e67f-46aa-4aed-8dba-8f97d566df8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 2, 16])\n"]}]},{"cell_type":"code","source":["x[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptkTrAZSggYH","executionInfo":{"status":"ok","timestamp":1717223694572,"user_tz":-540,"elapsed":311,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"64c34e96-2ae2-445e-ea0d-7ad2766e6209"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0671, -2.2780, -1.2814,  2.0654,  0.1273, -0.0603, -2.0497, -1.4655,\n","         -0.5372,  1.3063,  0.5705,  1.1857,  0.5660, -0.2755, -0.2692,  0.9791],\n","        [-1.5609, -0.3353,  0.5599, -1.2867, -0.6484, -2.7458,  0.5772, -1.5535,\n","         -0.4650, -0.3856, -0.1863, -0.3983,  0.9908, -2.0183, -1.0985, -0.9316]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Initialize the PositionalEncoding module\n","pos_encoder = PositionalEncoding(d_model)\n","\n","# Pass the input tensor through the PositionalEncoding module\n","x_encoded = pos_encoder(x)\n","\n","print(x_encoded.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Frp2KYq9Inf1","executionInfo":{"status":"ok","timestamp":1717223650122,"user_tz":-540,"elapsed":280,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"2a51e4ae-7a24-4893-8bc7-b557548c4e6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 2, 16])\n"]}]},{"cell_type":"code","source":["x_encoded[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3QZIUOPI1P8","executionInfo":{"status":"ok","timestamp":1717223675520,"user_tz":-540,"elapsed":321,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"2443fa9e-742d-4d3a-b0a2-d620c9db2639"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.3834, -3.5435, -1.1001,  1.2319,  1.0118,  0.6236, -1.9655, -0.5619,\n","         -0.4970,  0.0000,  0.6655,  2.4281,  0.0000,  0.0000, -0.2959,  2.1990],\n","        [-1.2765, -1.3849,  0.9457, -2.4926,  0.1499, -2.3602,  0.9534, -0.6597,\n","         -0.0000,  0.6781, -0.1754,  0.6681,  1.1109, -1.1315, -1.2174,  0.0759]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## About `0::2` in `pe[:, 0::2]`\n","The notation **`0::2`** in PyTorch is a slicing operation, and it means \"start at index 0, go to the end, and take every 2ned element\".\n","\n","### Understanding the Slice Notation `0::2`\n","* **0**: This is the start index. It tells the slice to start from the first element (index 0).\n","* **`:`**: This colon means \"go up to the end\" (it can be omitted in this context, but it's there fo clarity).\n","* **2**: This is the step value. It means \"take every 2nd element\".\n","\n","### Example\n","```python\n","list_example = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","sliced_list = list_example[0::2]\n","print(sliced_list)\n","```\n","The output will be:\n","```python\n","[0, 2, 4, 6, 8]\n","```\n","This means it starts at index 0 and picks every 2nd element from the list.\n","\n","### Applying to `pe[:, 0::2]`\n","Now, applying this to **`pe[:, 0::2]`**:\n","* **`pe[:, 0::2]`**:\n","    * **`:`** before the comma means all rows.\n","    * **`0::2`** after the comma means start at column 0 and take every 2nd column."],"metadata":{"id":"-FjD0sKhgoMG"}},{"cell_type":"markdown","source":["### Example in 2D Tensor\n","Consider a 2D tensor example to visualize this:"],"metadata":{"id":"csAxU8txiO68"}},{"cell_type":"code","source":["import torch\n","\n","# Create a tensor with shape (5, 10)\n","tensor_example = torch.arange(50).reshape(5, 10)\n","print(\"Original Tensor:\\n\",tensor_example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5tVAw9SgaIS","executionInfo":{"status":"ok","timestamp":1717224266600,"user_tz":-540,"elapsed":13,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"ae52c0cb-f73c-4ebc-b78c-6929f17d7067"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Tensor:\n"," tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n","        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n","        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n"]}]},{"cell_type":"code","source":["# Apply the slice\n","sliced_tensor = tensor_example[:, 0::2]\n","print(\"Sliced Tensor:\\n\", sliced_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaKzF0EailCC","executionInfo":{"status":"ok","timestamp":1717224290170,"user_tz":-540,"elapsed":272,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"c8c7e892-18cb-4eee-e430-ad111c77d1a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sliced Tensor:\n"," tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])\n"]}]},{"cell_type":"markdown","source":["As you can see, the sliced tensor contains every 2nd column from the original tensor."],"metadata":{"id":"xqLUFLLJi2jz"}},{"cell_type":"markdown","source":["### Applying to Positional Encoding\n","In the positional encoding context:\n","* **`pe[:, 0::2] = torch.sin(position * div_term)`**: This applies the sine function to the odd-indexed columns of **pe**.\n","* **`pe[:, 1::2] = torch.cos(position * div_term)`**: This applies the cosine function to the even-indexed columns of **pe**.\n","\n","### Summary\n","* **`0::2`** means \"start at index 0 and take every 2nd element until end\".\n","* In the context of **`[:, 0::2]`**, it applies the sine function to odd-indexed dimensions (columns) and the cosine function to even-indexed dimenstions (columns)."],"metadata":{"id":"Ub3U3yxBi7yb"}},{"cell_type":"code","source":[],"metadata":{"id":"PYl0H75oizOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## About `register_buffer`\n","**`register_buffer`** is a method in PyTorch used within **`nn.Module`** to register a tensor as a buffer. A buffer is a tensor that is not a model parameter but is still part of the module's state. These buffers are typiccally not updated during training (i.e., they do not require gradients), but they are useful for things like positional encodings, running statistics in batch noramalization, or other fixed data that needs to be stored in the module.\n","\n","### Key Points About `regisuter_buffer`\n","1. **State Management**:\n","    * Buffers are part of the module's state. They are saved and loaded when you save or load the model's state dictionary. This makes them useful for storing data that is part of the model but should not be treated as a parameter.\n","2. **Fixed Data**:\n","    * Buffers are usually used for fixed data that does not require gradient updates, such as constants or other intermediate computations that should be preserved.\n","3. **Non-learnable Parameters**:\n","    * Unlike parameters registered using **register_parameter**, buffers do not require gradients and do not appear in the list of parameters returned by **parameters()**.\n","    \n","\n"],"metadata":{"id":"xHMmQbGRjwJz"}},{"cell_type":"markdown","source":["### Example Usage in Posigional Encoding\n","In the **PositionalEncoding** class, **register_buffer* is used to store the positional encoding teosor **pe**:"],"metadata":{"id":"GcvnUgdXllob"}},{"cell_type":"code","source":["d_model = 10\n","max_len = 5000\n","\n","pe = torch.zeros(max_len, d_model)\n","position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","pe[:, 0::2] = torch.sin(position * div_term)\n","pe[:, 1::2] = torch.cos(position * div_term)\n","print(pe.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SEVvfIWAmvLp","executionInfo":{"status":"ok","timestamp":1717225386235,"user_tz":-540,"elapsed":13,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"3d6294b3-3c59-4216-9eb1-a6a8e4dfc09c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5000, 10])\n"]}]},{"cell_type":"code","source":["pe = pe.unsqueeze(0).transpose(0, 1)\n","pe.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dStOD0odmur0","executionInfo":{"status":"ok","timestamp":1717225416946,"user_tz":-540,"elapsed":399,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"58bbf3d7-4698-425e-b3b6-f9add0549174"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5000, 1, 10])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"metadata":{"id":"yxgx750klysA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here's what happens with **registerbuffer** in this context:\n","* **Creating the Buffer**: **pe** is a tensor that contains the precomputed positional encodings. It is created and populated with values based on the sine and cosine function.\n","* **Registering the Buffer**: **self.resigter_buffer('pe', pe)** registers **pe** as a buffer in the module. This means **pe** will be part of the module's state, saved, and loaded with the model, but it will not be treated as a learnable parameter.\n","* **Accessing the Buffer**: In the **forward** method, the buffer **pe** can be accessed using **self.pe**, and it is added to the input tensor **x**."],"metadata":{"id":"G5CSq6HenZd7"}},{"cell_type":"markdown","source":["### Why Use `register_buffer`?\n","Using **register_buffer** ensures that:\n","* The tensor is included in the module's state dictionary, which is important for saving and loading the model.\n","* The tensor is not treated as a parameter that requires gradient updates.\n","* The tensor can be moved to the appropriate device(CPU/GPU) along with the rest of the model using **.to()** or **.cuda()** methods."],"metadata":{"id":"tM7B9zCUoMpT"}},{"cell_type":"markdown","source":["### Practical Example"],"metadata":{"id":"z7EIuHaRowLZ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class ExampleModule(nn.Module):\n","    def __init__(self):\n","        super(ExampleModule, self).__init__()\n","        self.param = nn.Parameter(torch.randn(3, 3))\n","        buffer = torch.ones(3, 3)\n","        self.register_buffer('buffer', buffer)\n","\n","    def forward(self, x):\n","        return self.param + self.buffer\n","\n","\n","# Create an instance of the module\n","module = ExampleModule()\n","\n","# Print the state dictionary\n","print(\"State Dict:\", module.state_dict())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQUcJtGLozRy","executionInfo":{"status":"ok","timestamp":1717226007389,"user_tz":-540,"elapsed":406,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"2caaa4cc-c820-4aa4-a3ea-43ca93678ece"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State Dict: OrderedDict([('param', tensor([[ 0.9268, -0.3773, -0.9999],\n","        [ 1.3651, -0.8092,  1.0600],\n","        [-0.1706,  0.2358, -1.5510]])), ('buffer', tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]]))])\n"]}]},{"cell_type":"code","source":["# Check if buffer is included in the state dict\n","assert 'buffer' in module.state_dict()"],"metadata":{"id":"N4k880rmpTqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the parameters of the module\n","print(\"Parameters:\", list(module.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHcLXPOCpf1A","executionInfo":{"status":"ok","timestamp":1717226080607,"user_tz":-540,"elapsed":9,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"35b621db-ba10-436f-96ee-43709b67cf88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameters: [Parameter containing:\n","tensor([[ 0.9268, -0.3773, -0.9999],\n","        [ 1.3651, -0.8092,  1.0600],\n","        [-0.1706,  0.2358, -1.5510]], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["# Print the buffer of the module\n","print(\"Buffer:\", list(module.buffers()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UKX0JwEpoUB","executionInfo":{"status":"ok","timestamp":1717226118292,"user_tz":-540,"elapsed":19,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"43ba05d7-2443-4a0a-ede9-3b81638dbfa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Buffer: [tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])]\n"]}]},{"cell_type":"markdown","source":["In this example:\n","* **self.param** is a learnable parameter registerd using **nn.Parameter**.\n","* **buffer** is a fixed tensor registered using **register_buffer**.\n","\n","When you print the state dictionary of the module, both **param** and **buffer** will be included. However, **buffer** will not be listed as a parameter that requires gradients."],"metadata":{"id":"n1sGi9uIp-fn"}},{"cell_type":"code","source":[],"metadata":{"id":"eV0X1Tjzpxkz"},"execution_count":null,"outputs":[]}]}