{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnIJ+JPg2DLfRJz8mMegzT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Architecting Network: Hyperparameters\n","* https://www.youtube.com/watch?v=YTL2BR4U2Ng&list=PLjy4p-07OYzuy_lHcRW8lPTLPTTOmUpmi&index=40\n","* https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_08_3_pytorch_hyperparameters.ipynb\n","\n","You have probably noticed several hyperparameters introduced previously in this course that you need to choose for your neural network. The number of layers, neuron counts per layer, layer types, and activation functions are all choices you must make to optimize your neural network. Some of the categories of hyperparameters for you to choose from coming from the following list:\n","* **Number of Hidden Layers and Neuron Counts**\n","* **Activation Functions**\n","* **Advanced Activateion Functions**\n","* **Regularization: L1, L2, Dropout**\n","* **Batch Normalization**\n","* **Training Parameters**\n","\n","\n","The following sections will introduce each of these categories for PyTorch. While I will provide some general guidelines for hyperparamter selection, no two tasks are the same. You will benefit from experimentation with these values to determine what works best for your neural network. In the next part, we will see how machine learning can select some of these values independently.\n"],"metadata":{"id":"s2Q0SLxvR8au"}},{"cell_type":"markdown","source":["## Number of Hidden Layers and Neuron Counts\n","The structure of PyTorch layers is perhaps the hyperparameters that most become aware of first. How many layers should you have? How many neurons are on each layer? What activation function and layer type should you use? These are all questions that come up when designing a neural network. There are many different types of lyers in PyTorch, listed here:\n","\n","* **Activation**: PyTorch allows you to add activation function using torch.nn modules. Instead of an activation layer, you typically specify the activation function directly after a Linear (or other) layer type.\n","* **Regularization**: For L1/L2 regularization in PyTorch, you generally don't use a separate layer. Instead, you can add weight decay when setting up an optimizer like SGD or Adam. This works as an L2 regularization. For L1, you might need to implement it manually.\n","* **Linear**: The original neural network layer type. In this layer type, every neuron connects to the next layer. The input vector is one-dimensional, and placing specific inputs next does not affect each other.\n","* **Dropout**: It operates by randomly setting a fraction of input units to 0 at each forward pass, which helps in preventing overfitting. In PyTorch, Dropout is applied during training only by default.\n","* **Flatten**: Flattens the input to 1D and does not affect the batch size.\n","* **Permute(変換)**: PyTorch tensors have a permute method that can be used to rearrange the dimensions of a tensor, which is useful when working with different types of layers that expect certain input shapes and for tasks such as connecting RNNs and convolutional network.\n","* **RepeatVector**: Repeats the input n times.\n","\n","There is always trial and error for choosing a good number of neurons and hidden layers. Generally, the number of neurons on each layer will be larger closer to the hidden layer and smaller towards the output layer. This configuratin gives the neural network a somewhat triangular or trapezoid(台形) appearance."],"metadata":{"id":"FJfOIHY1Y0pv"}},{"cell_type":"markdown","source":["## Activation Functions\n","Activation functions are a choice that you must make for each layer. Generally, you can follow this guideline:\n","* Hidden Layers = RELU\n","* Output Layer - Softmax for classification, linear for regression.\n","\n","Some of the common activation functions in PyTorch are listed here:\n","* **Softmax**: Used for multi-class classification. Ensures all output neurons behave as probabilities and sum to 1.0.\n","* **elu**: Exponential linear unit. Exponential Linear Unit or its widely known name ELU is a function that tends to converge cost to zero faster and produce more accurate results. Can produce negative outputs.\n","* **selu**: Scaled Exponential Liner Unit (SELU), essentially **elu** multiplied by a scaling costant.\n","* **softplus**: Softplus activation function. $log(exp(x) + 1)$ Introduced in 2001.\n","* **softsign**: Softsign activation function. $x / (abs(x) + 1)$ Simlar to tanh, but not widely used.\n","* **relu**: Very popular neural network activation function. Used for hidden layers, cannot output negative values. No trainable parameters.\n","* **tanh**: Classic neural network activation function, though often replaced by relu family on modern networks.\n","* **sigmoid**: Classic neural network activation. Often used on output layer of a binary classifier.\n","* **hard sigmoid**: Less computationally expensive variant sigmoid.\n","* **exp**: Exponential (base e) activation function.\n","\n","Documents\n","* https://pytorch.org/docs/stable/nn.html\n","\n","\n","\n","\n","\n"],"metadata":{"id":"fM3cGPRtbqne"}},{"cell_type":"markdown","source":["## Batch Normalization and Dropout\n","* https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n","* https://pytorch.org/docs/stable/generated/torch.nn.functional.batch_norm.html"],"metadata":{"id":"xBkvt19LeRLJ"}},{"cell_type":"markdown","source":["## Training Parameters\n","* https://pytorch.org/docs/stable/optim.html\n","* **Batch Size**: Usually small, such as 32 or so.\n","* **Learning Rate**: Usually small, 1e-3 or so."],"metadata":{"id":"8cQlX66xeYoB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwzC9YaERpfH"},"outputs":[],"source":[]}]}