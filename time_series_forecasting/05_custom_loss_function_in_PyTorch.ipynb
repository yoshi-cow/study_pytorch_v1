{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"132COfZUMSKe3vapUXwHEQTIoiVlAl-zE","authorship_tag":"ABX9TyOFcFET86x9p5DVElAzep1m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. check dimension of loss function when using batch"],"metadata":{"id":"C7p-rc3UgA-D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfcDF-DjerZM"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","source":["# Load data and normalize\n","\n","data_path = '/content/drive/MyDrive/study_DeepLearning/data/AMZN.csv'\n","data = pd.read_csv(data_path)\n","data = data[['Date', 'Close']]\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","data['Date'] = pd.to_datetime(data['Date'])\n","\n","from copy import deepcopy as dc\n","\n","def prepare_dataframe_for_lstm(df, n_steps):\n","  df = dc(df)\n","  df['Date'] = pd.to_datetime(df['Date'])\n","\n","  df.set_index('Date', inplace=True)\n","\n","  for i in range(1, n_steps + 1):\n","    df[f'Close(t-{i})'] = df['Close'].shift(i)\n","\n","  df.dropna(inplace=True)\n","\n","  return df\n","\n","lookback = 7\n","shifted_df = prepare_dataframe_for_lstm(data, lookback)\n","shifted_df_as_np = shifted_df.to_numpy()\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n","\n","# データをx, yに分割\n","X = shifted_df_as_np[:, 1:]\n","y = shifted_df_as_np[:, 0]\n","\n","X = dc(np.flip(X, axis=1))\n","\n","# 学習とテストを分割するindex位置算出\n","split_index = int(len(X) * 0.95)\n","\n","X_train = X[:split_index]\n","X_test = X[split_index:]\n","\n","y_train = y[:split_index]\n","y_test = y[split_index:]\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Qaesrooj8mV","executionInfo":{"status":"ok","timestamp":1712912959605,"user_tz":-540,"elapsed":647,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"14b74650-d39c-4ab6-f1ee-ed129ad2a70c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((6183, 7), (326, 7), (6183,), (326,))"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# pytorchにあてはめられるように各データに次元を追加する\n","# この作業の意味がよくわからない？？？？？\n","X_train = X_train.reshape((-1, lookback, 1))\n","X_test = X_test.reshape((-1, lookback, 1))\n","\n","y_train = y_train.reshape((-1, 1))\n","y_test = y_test.reshape((-1, 1))\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeYeOk3ZkYKf","executionInfo":{"status":"ok","timestamp":1712912987224,"user_tz":-540,"elapsed":237,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7c718ba5-4c7f-4e66-daae-c34d89a3c877"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((6183, 7, 1), (326, 7, 1), (6183, 1), (326, 1))"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# tensorに変換\n","X_train = torch.tensor(X_train).float()\n","y_train = torch.tensor(y_train).float()\n","X_test = torch.tensor(X_test).float()\n","y_test = torch.tensor(y_test).float()\n","\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztKiwnT_kfrE","executionInfo":{"status":"ok","timestamp":1712912994210,"user_tz":-540,"elapsed":260,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"d69d668e-12cd-428c-f35e-551395c9bce2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([6183, 7, 1]),\n"," torch.Size([326, 7, 1]),\n"," torch.Size([6183, 1]),\n"," torch.Size([326, 1]))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Datasetクラスの定義\n","from torch.utils.data import Dataset\n","\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, i):\n","        return self.X[i], self.y[i]\n","\n","train_dataset = TimeSeriesDataset(X_train, y_train)\n","test_dataset = TimeSeriesDataset(X_test, y_test)\n","\n","# DataLoaderの作成（バッチ処理用）\n","from torch.utils.data import DataLoader\n","\n","batch_size = 16\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"TJViRbk5ki1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1バッチの中身確認\n","for _, batch in enumerate(train_loader):\n","    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n","    print(x_batch.shape, y_batch.shape)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPxi3O46kr-2","executionInfo":{"status":"ok","timestamp":1712913044764,"user_tz":-540,"elapsed":15,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"909f2901-53eb-4c39-b032-e02a4fcb54be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 7, 1]) torch.Size([16, 1])\n"]}]},{"cell_type":"code","source":["lstm = nn.LSTM(1, 4, 1,batch_first=True)\n","fc = nn.Linear(4, 1)"],"metadata":{"id":"tVXe0Q8Wksvd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out, _ = lstm(x_batch)\n","out = fc(out[:, -1, :])\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z27tshJTlKrs","executionInfo":{"status":"ok","timestamp":1712913213543,"user_tz":-540,"elapsed":406,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bcb8c8b0-53b9-497a-e888-e8b322eaa443"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 1])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4N_3mbVmlWP9","executionInfo":{"status":"ok","timestamp":1712913228681,"user_tz":-540,"elapsed":404,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"ebcc41d7-ddd9-43c5-b9c8-396bb09841f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.5053],\n","        [-0.5058],\n","        [-0.5091],\n","        [-0.5054],\n","        [-0.5319],\n","        [-0.5105],\n","        [-0.5076],\n","        [-0.5066],\n","        [-0.5068],\n","        [-0.5104],\n","        [-0.5053],\n","        [-0.5052],\n","        [-0.5057],\n","        [-0.5058],\n","        [-0.5063],\n","        [-0.5107]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["print(y_batch.shape)\n","y_batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iyyNVVXZlZ9d","executionInfo":{"status":"ok","timestamp":1712913245845,"user_tz":-540,"elapsed":5,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bc2644f9-6a3b-46d3-c0a0-221545932dd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 1])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.9921],\n","        [-0.9783],\n","        [-0.8759],\n","        [-0.9907],\n","        [ 0.0189],\n","        [-0.8468],\n","        [-0.9244],\n","        [-0.9522],\n","        [-0.9452],\n","        [-0.8384],\n","        [-0.9919],\n","        [-0.9949],\n","        [-0.9815],\n","        [-0.9773],\n","        [-0.9633],\n","        [-0.8270]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 損失関数設定\n","loss_function = nn.MSELoss()\n","\n","# 損失求める\n","loss = loss_function(out, y_batch)"],"metadata":{"id":"x_PDwZeWleLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUEtSDdvltsm","executionInfo":{"status":"ok","timestamp":1712913311848,"user_tz":-540,"elapsed":11,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"cf0fc557-c9b0-42c3-9aba-27afadf08188"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.1971, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["loss.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Se23VgYBluRV","executionInfo":{"status":"ok","timestamp":1712913334184,"user_tz":-540,"elapsed":6,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"361c7c15-eb14-4b16-c233-29a89d116f8d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["type(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubbHeZY-lyON","executionInfo":{"status":"ok","timestamp":1712915423194,"user_tz":-540,"elapsed":14,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"72595bd3-22b3-4ec8-ea69-99053875681e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["loss.item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQWuyUNVtxxa","executionInfo":{"status":"ok","timestamp":1712915443066,"user_tz":-540,"elapsed":9,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"b34fdbbc-e7aa-48ce-e7c0-76b98b5bf823"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1970682442188263"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["y_batch.item()"],"metadata":{"id":"-N-uNDiXt1x6","executionInfo":{"status":"error","timestamp":1712915467766,"user_tz":-540,"elapsed":22,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"0ada9a30-579c-4cdf-f24f-c8660fadbb25","colab":{"base_uri":"https://localhost:8080/","height":141}},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"a Tensor with 16 elements cannot be converted to Scalar","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-af49c383b8d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 16 elements cannot be converted to Scalar"]}]},{"cell_type":"markdown","source":["* 損失関数の戻り値は、ベクトルではなく、スカラーっぽい。なので、batch内それぞれの値を返すわけではなく、batch内それぞれの値の平均値を返している。"],"metadata":{"id":"cCEJUzxMxklT"}},{"cell_type":"markdown","source":["# Examole of Custom Loss function\n","Q: I have a question custom loss function in PyTorch.\n","I want to create loss function to calculate sharpe ratio (= return.mean() / return.std()). And Because of batch process return has size of (batch_size, predicted numbers)=(16, 5).\n","Could you make loss function for PyTorch?\n","\n","\n","A: In this context, since we're dealing with a loss function, we typically want to minimize loss. Thus, we use the negative Sharpe ratio, as maximizing the Sharpe ratio is equivalent to minimizing its negative.\n","\n","For a tensor **returns** with a shape of **(batch_size, predicted_numbers)**, where each row represents the returns for a different predictin or asset, the steps are as follows:\n","\n","1. Calculate the mean return across the second dimension (predicted numbers).\n","2. Calculate the standard deviation of returns across the same dimension.\n","3. Compute the Sharpe ratio as the ratio of these two quantities.\n","4. Negate the Sharpe ratio to form a loss function (since we want to maximize the Sharpe ratio, minimizing its negative is equivalent).\n","\n","\n","Here's how you can define this custom loss function using PyTorch:"],"metadata":{"id":"YZ73_WY7x6pN"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class NegativeSharpeRatioLoss(nn.Module):\n","    def __init__(self):\n","        super(NegativeSharpeRatioLoss, self).__init__()\n","\n","    def forward(self, returns):\n","        # Calculate mean of returns along the second dimension\n","        mean_returns = returns.mean(dim=1)\n","\n","        # Calculate standard deviation of returns along the same dimension\n","        std_dev_returns = returns.std(dim=1, unbiased=False)  # using unbiased=False for sample standard deviation\n","\n","        # Compute the Sharpe ratio\n","        sharpe_ratio = mean_returns / std_dev_returns\n","\n","        # Return the negative Sharpe ratio as loss\n","        return -sharpe_ratio.mean()  # Average over the batch to get a single loss value\n","\n","# Example usage:\n","# Create a sample returns tensor of shape (16, 5)\n","sample_returns = torch.randn(16, 5)  # Random data simulating returns\n","\n","# Instantiate the loss function\n","loss_func = NegativeSharpeRatioLoss()\n","\n","# Compute the loss\n","loss = loss_func(sample_returns)\n","\n","print(\"Loss (Negative Sharpe Ratio):\", loss.item())\n"],"metadata":{"id":"tQMonNiBt7-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this implementation:\n","* We inherit from **nn.Module** to creat a custom loss class.\n","* We override the **frward** method to define how the loss is computed.\n","* **unbiased=False** in **std()** means we use the sample standard deviation, which is typical in training scenarios.\n","\n","\n","This functin will cumpute the average negative Sarpe ratio for a batch of return predictions, making it suitable for batch training processes in neural networks."],"metadata":{"id":"zgJwj_tG36v5"}},{"cell_type":"markdown","source":["## Why do we need to do mean to \"sharpe_ratio\" when we return?\n","\n","Returning the mean of the Sharpe ratio across the batch when computing the loss in a neural network training scenario has several purpose:\n","1. **Uniform LOss Representation**: by averaging the Sharpe ratio across the batch, <u>the loss function provides a single scalar value</u> representing the performance of the entire batch. This is standard practice for training neural networks because optimization algorithms (like SGD, Adam, etc.) <u>require a single loss value</u> to perform parameter updates. If the loss function returned a vector (one Sharpe ratio per item in the batch), it would not directly fit into these optimization algorithms, which expect a scalar value to minimize.\n","2. **Stability and Generalization**: Averaging reduces the variance in the loss estimates between different batches. This can lead to more stable and reliable gradient estimates during training, as the optimizer is less likely to be misled by the moise inherent in any single example or small set of examples. Essentially, it helps in smoothing out the learning process.\n","3. **Consistency with Other Loss Functions**: Most built-in PyTroch loss functions, such as MSE(Mean Squared Error) or Cross-Entropy Loss, <u>compute the average over the bath</u> to ensure that the scale of the loss doesn't change, with the batch size. This makes the learning rate and other hyperparameters more generally applicable, rather than having to adjust them based on how many examples you have in a batch.\n","4. **Scalability**: Averaging the Sharpe ratios makes the loss computation scalable across different batch sizes. Without averaging, the scale of the loss could vary depending on the batch size, which could complicate the optimization process, especially when changing the batch size during experiments or when moving from a development environment (where you might use smaller batches for speed) to a production or training environment (where larger batches might be used for efficiency).\n","\n","\n","\n","Overall, averaging the Sharpe ratios to compute the loss ensures that training process is efficient, consistent, and scalable across different training configurations and batch sizes."],"metadata":{"id":"PIwoQwLl62jE"}},{"cell_type":"markdown","source":["# Careful points to make custom loss function\n","\n","1. **Batch Compatiblility**: The loss function should be compatible(互換性がある) with batch processing. This means it should be able to handle multiple examples at once, typically represented as the first dimension of the input tensor. Each operation within the loss function should be applied independently across each element of the batch, but <u>final output should usually be a single scaler value</u> that represents the average loss across the batch.\n","2. **Scalar Output**: Most optimization algorithms <u>expect a single scalar value as the output of the loss function</u>. This is because the gradient descent steps are computed based on this scalar to update all model parameters. Therefore:\n","    * **Aggregate Outputs**: You typically aggregate individual losses (one per batch item) into a single scalar using operations like **mean()** or **sum()**. The chice between summing and averaging can depend on the specific situation but average (**mean()**) is more common as it normalizes the loss relative to the batch size.\n","3. **Stability and Numerical Issues**:\n","    * **Avoid Division by Zero**: Ensure that your loss function is numerically stable. For example, when computing ratios or logarithms, make sure the denominator or the argument of the logarithm cannot be zero or close to zero.\n","    * **Normalization**: Sometimes it's necessary to add small constants (e.g. **epsilon** in the denominator) to prevent undefined behaviors.\n","4. **Consistency Across Batch Size**:\n","    * **Independent of Batch Size**: The loss should ideally not bve dependent on the batch size. This is why averaging the loss over the batch sizze is common, as it makes the loss comparable regardless of the number of examples in each batch.\n","5. **Gradient Flow**:\n","    * **Ensure Non-zero Gradients**: Ensure that your custom loss function does not inadvertently(うっかり) lead to situations where gradients could vanish or explode, especially in deep networks. This can involve careful structuring of operations and, occasionally, regularization terms.\n","6. **Bachward Pass**:\n","    * **Automatic Differentiation**: PyTorch's automatic differentiation engine, Aurograd, should be able to complute derivatives of your custom loss function. Use PyTorch operations as they are generally designed to be automatically differentiable.\n","7. **Testing and Validation*:\n","    * **Empirical Testing**: Before full-scale training, it’s crucial to empirically test the custom loss function on small datasets or with simplified models to ensure it behaves as expected.\n","    * **Debuggability**: Make the function as simple and interpretable as possible, which aids in debugging. If a complex function is giving unexpected results, consider breaking it down into smaller, testable parts.\n","\n","\n","**Example Scenario**\n","\n","If your loss function, for instance, computes a financial metric (like the Sharpe ratio) that involves division, ensure the standard deviation isn’t zero by adding a small epsilon (**1e-8** or similar) to the denominator.\n","\n","Here's how you could modify the Sharpe ratio example for better stability:"],"metadata":{"id":"rFLY4eFG-r3l"}},{"cell_type":"code","source":["class NegativeSharpeRatioLoss(nn.Module):\n","    def __init__(self, epsilon=1e-8):\n","        super(NegativeSharpeRatioLoss, self).__init__()\n","        self.epsilon = epsilon\n","\n","    def forward(self, returns):\n","        mean_returns = returns.mean(dim=1)\n","        std_dev_returns = returns.std(dim=1, unbiased=False) + self.epsilon  # Add epsilon to avoid division by zero\n","        sharpe_ratio = mean_returns / std_dev_returns\n","        return -sharpe_ratio.mean()  # Average over the batch\n"],"metadata":{"id":"QGjeAKXQI6tu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this modified version, **epsilon** is added to prevent division by zero and ensure the loss function remains numerically stable. This kind of careful handling is especially important in financial applications where returns can sometimes be very uniform, leading to near-zero standard deviations."],"metadata":{"id":"zwjr_D5dI-lS"}},{"cell_type":"markdown","source":["# Custom Loss Function by Class or Function\n","Creating a custom class for a loss function in PyTorch by extending **nn.Module** is common and beneficial for several reason, including the ability to easily integrate with other PyTorch modules, to maintain consistency with PyTorch's object-oriented approach, and to potentially encapsulate additional parameters or methods that the loss function might need. However, it's also completely valid to define a custom loss function using a simple Python function if your use case is straightforward.\n","\n","## Defininig a Custom Loss Function as a Python Function\n","Here's how you could modify the **NegativeSharpeRatioLoss** using a plain Python function:"],"metadata":{"id":"vAK9tf47JHQO"}},{"cell_type":"code","source":["import torch\n","\n","def negative_sharpe_ratio_loss(returns, epsilon=1e-8):\n","    mean_returns = returns.mean(dim=1)\n","    std_dev_returns = returns.std(dim=1, unbiased=False) + epsilon  # Adding epsilon to avoid division by zero\n","    sharpe_ratio = mean_returns / std_dev_returns\n","    return -sharpe_ratio.mean()  # Return the negative Sharpe ratio as loss\n","\n","# Example usage:\n","# Create a sample returns tensor of shape (16, 5)\n","sample_returns = torch.randn(16, 5)  # Random data simulating returns\n","\n","# Compute the loss\n","loss = negative_sharpe_ratio_loss(sample_returns)\n","\n","print(\"Loss (Negative Sharpe Ratio):\", loss.item())\n"],"metadata":{"id":"Gagu_nHGJL4X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Key Differences and When to Use Each Approach\n","1. **Simplicity vs. Flexibility**:\n","    * **Function**: Use a functio when your loss computation is straightforward and doesn't need to maintain state (i.e., no internal parameters other than inputs and hyperparameters).\n","    * **Class**: Use a class when your loss function might need to maintain state, hold parameters, or provide multiple utility functions.\n","2. **Integration**:\n","    * **Function**: Easy to write and use, especially for quick experiments or custom scripts.\n","    * **Class**: Better ingegrate with PyTorch’s way of defining models and other modules, especially useful when building complex models or when the loss function needs to behave similarly to other modules in terms of parameter handling, GPU allocation, etc.\n","3. **Extensibility**:\n","    * **Function**: Less extensible, as it can't easily encapsulate related functionality without global functions.\n","    * **Class**: Can easily be extended with additional methods and properties, and can override or extend existing functionalities in a structured way.\n","\n","\n","\n","For many practical applications, especially in research and development, starting with a simple function-based approach is often sufficient. If the loss function needs to evolve or if integration with more complex systems is required, refactoring it into a class-based approach might be appropriate."],"metadata":{"id":"0E2kdOCmKFjm"}},{"cell_type":"code","source":[],"metadata":{"id":"yi-iLaBtLRZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# \"Auto Grad\" in custom loss function\n","Q: In above custom loss function by \"def\", Does \"Automatic Differentiation\" work correctly? And What's condition does cause defect about \"Automatic Differentiation\"?\n","\n","\n","A: Yes, automatic differenciation in PyTorch will work correctly with the custom loss function defined usig a PyTorch function, as long as you use PyTorch operations that support gradient calculations. PyTorch's automatic differentiation engine, **Augograd**, tracks operations on tensors for which the **requires_grad** attribute is set to **True**. When you perform operations on these tensors using PyTorch functions, **Autograd** constructs a computation graph in the backgroud, which is used to compute gradients when you call **.backward() or the final output tensor.\n","\n","## When Automatic Differentiation Works:\n","1. **Tensor Operations**: As long as the operations within your custom loss function are standard PyTorch tensor operations, **Autograd** will correctly track these operations and calculate gradients.\n","2. **Continuity of Gradient Tracking**: The gradient tracking must not be interrupted. If all tensor operations are linked and none explicitly stops the gradient computation (e.g., by detaching tensors froom the computation graph), gradients will flow correctly through the computation grahp.\n","\n","\n","## Conditions That Could Cause Issue with Automatic Differentiation:\n","1. **Detaching Tensors**: If any tensor involved in the computation of the loss is detached from the computation graph (**tensor.detach()**), gradients will not flow back past this point, potentially breaking the gradient comutation.\n","2. **In-place Operation**: Performing in-place operations on tensors that require gradient calculations can sometimes lead to incorrect gradient computations because it modifies the data directly, thus potentially destroying information required for computing derivatives.\n","3. **Using Non-PyTorch Code**: If part of the loss computation uses non-PyTorch operations (e.g., converting tensors to NumPy arrays, performing operations, and converting them back), gradients cannot be computed for these operations because **Autograd** can only track PyTorch tensor operations.\n","4. **Loss Returns Non-Scalar Outputs Without Proper Aggregtaion**: If the custom function does not aggregates the output to a scalar (usig **mean()**, **sum()**, etc.) and instead returns a vector or matrix, then calling **.backward** without additional parameters will result in an error. **Autograd** expects scalar outputs to compute gradients directly, or if the output is a tensor, it requires explicit gradient agrguments.\n","5. **Discontinuous Operations**: Operations that are mathematically discontinuous (e.g., integer-based rounding operations) may either not have gradients or have gradients that are not meaningful.\n","\n","## Example of Proper Usage\n","In the provided loss function example, as long as the operations are standard (mean, std, division) and involve tensors that are part of the PyTorc ecosystem, gradients will be calculated correctly:"],"metadata":{"id":"ZTAnm7ZQLR9z"}},{"cell_type":"code","source":["import torch\n","\n","def negative_sharpe_ratio_loss(returns, epsilon=1e-8):\n","    # Ensure the input has `requires_grad=True` if this is the tensor you want to optimize\n","    mean_returns = returns.mean(dim=1)\n","    std_dev_returns = returns.std(dim=1, unbiased=False) + epsilon\n","    sharpe_ratio = mean_returns / std_dev_returns\n","    return -sharpe_ratio.mean()\n","\n","# Example usage\n","sample_returns = torch.randn(16, 5, requires_grad=True)  # Enable gradient tracking\n","loss = negative_sharpe_ratio_loss(sample_returns)\n","loss.backward()  # Compute gradients\n"],"metadata":{"id":"CKt9Q1DyPEft"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this example, if **sample_returns** is a tensor with **requires_grad=True**, then calling **backward()** on **loss** will correctly compute gradients for **sample_returns**, assuming a intermediate steps are differetiable and do not break the gradient tracking."],"metadata":{"id":"c_B5oF4pPIrr"}},{"cell_type":"markdown","source":["# **`requires_grad=True` of torch.tensor\n","Q: In torch.tensor, Is `requires_grad=True` default setting?\n","\n","A: No, in PyTorch, the default setting for **requires_grad** on a new tensor is **False**. This setting means that by defalut, PyTorch will not track operations on these tensors for the purpose of gradient computation. This is efficient for tensors that do not need gradients, such as fixed model parameters or input data that do not change during training.\n","\n","If you want PyTorch to compute gradients with respect to a tensor during backpropagation, you must explicitly set **requires_grad=True**. This is commonly done for tensor representing model parameters and for any tensors for which you need gradients to optimize during training.\n","\n","\n","Here’s how you can set **requires_grad=True** for a tensor:"],"metadata":{"id":"cQVgNS4aPy3Q"}},{"cell_type":"code","source":["import torch\n","\n","# Creating a tensor without requiring gradient computation\n","x = torch.randn(5, 5)\n","\n","# By default, requires_grad is False\n","print(\"Default requires_grad:\", x.requires_grad)  # Output: False\n","\n","# Setting requires_grad to True to enable gradient computation\n","x.requires_grad_(True)\n","print(\"After setting requires_grad:\", x.requires_grad)  # Output: True\n"],"metadata":{"id":"xpzCW5IkPyC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The setting is essential for training neural networks, where you typically need gradients for weights and biases to perform parameter updates based on gradient descent. However, for tensors used in forward computations that do not require gradient updates (like fixed parameters or temporary variables in computations), you should leave **reauires_grad** as **False** to improve performance and reduce memory usage."],"metadata":{"id":"AC3YuCbKR_QD"}},{"cell_type":"markdown","source":["## Pass tensor data coming from train data to custom loss function\n","Q: When I pass return tensor (60, 4) to loss function, How do we set `requires_grad=True` in return tensor. This return tensor come from pytorch dataset class.\n","\n","\n","A: When dealing with tensors that come from a PyTorch dataset or are manipulated during the data processign stages prior to being used in training (such as in your loss function), it's <u>crucial to ensure they have **requires_grad=True**</u> if you intend to compute gradients with respect to these tensors. This is often relevant in scenarios where the tensors are not just data, but parameters or intermediates that you want to optimize directly.\n","\n","\n","If your **returns** tensor (with dimensions [60, 4]) originates from a dataset and is intended to influence the model parameters or is part of an optimization, you will need to explicitly set **requires_grad=True**. Here are a few scenarios and how to handle them:\n","\n","1. **Tensor Comes From Data That Doesn't Require Gradient**: If the tensor is purely data (like input features or tareget labels), you typically don't set **requires_grad=True** because these are not parameters you optimize.\n","2. **Tensor Is a Model Output or Intermediate Calculation**: If the tensor is a model output or an intermediate value that affects the computation of loss and you need gradients to flow back for optimization, then ensureing **requires_grad=True** is crucial. This is usually handled automatically by PyTorch if the tensor is a result of operations involving model parameters with **requires_grad=True**.\n","\n","### Setting **`requires_grad=True**`\n","Here's how to ensure that your tensor has **requires_grad=True** when it is necessary:\n","\n","* Case1: Directly Createing a Tensor: If you directly creating a tensor, you can specify **requires_grad** at creation time:\n","\n","```\n","returns = torch.randn(60, 4, requires_grad=True)\n","```\n","\n","* Case2: The Tensor Comes from Dataset or Other Sources: If the tensor is coming from a dataset or generated by data transformations and needs gradients, you need to set **requres_grad** after its creation:\n","\n","```\n","# Assume `returns` is loaded or computed somewhere without requiring gradients\n","returns = torch.randn(60, 4)  # `requires_grad` is False by default\n","\n","# Enable gradient computation\n","returns.requires_grad_(True)\n","```\n","\n","### Using the Tensor in Custom Loss Functions\n","When you pass this tensor to your loss function, if it involves trainalbe parameters or you need gradients with respect to the tensor itself for some reason (like optimizaing input features), you must ensure it is set requre gradients. Here's how you would use it in the loss computations:\n","\n","```\n","loss = negative_sharpe_ratio_loss(returns)\n","loss.backward()\n","```\n","\n","This setup will compute gradients with respect to **returns** if your loss computation logic and model structure require optimizing or adjusting **return** directly. This is somewhat uncommon for typical input data but can be relevant in certain advanced machine learning techniques such as input feature optimization, adversarial training, or when **return** itself is a parameterized output of another part of your model.\n","\n"],"metadata":{"id":"9VJBjjjdSmIb"}},{"cell_type":"markdown","source":["# Whether my sharpe_ration loss function need **`requres_grad=True`** or not\n","Q: So far, I learn `requires_grad=True` about custom loss function thanks to you. So, I create following code including my custom loss function named \"def sharpe_loss(y, Fred_w)\". Does variable y in \"sharpe_loss\" function need  `requres_grad=True`? Please teach me if my following code need the code about `requres_grad=True` for loss function or not and explain the reason.\n","\n","```\n","def sharpe_loss(y, pred_w):\n","    '''\n","    y: [batch_size, sequence_length, features]=[64, 50, 4]\n","        - contents of features are each asset's return\n","    pred_w: [batch_size, features]=[64, 4]\n","        - contents of features are each predicted weight\n","    Defines the Sharpe Ratio as the loss function.\n","    '''\n","    # add dimensioin of batch to pred_w\n","    add_d_pred_w = pred_w.unsqueeze(1) # [64, 4] -> [64, 1, 4]\n","\n","    # multiply each return with each weight\n","    y_o = y * add_d_pred_w # [64, 50, 4] * [64, 1, 4] = [64, 50, 4]\n","\n","    # sum up each record\n","    summed = y_o.sum(dim=2) # [64, 50, 4] -> [64, 50]\n","    # portfolio_returns = summed.flatten() # [64, 50] -> [3200] # dimensionの確認必要！！！\n","\n","    # cals sharpe ratio\n","    sharpe_ratio = summed.mean() / summed.std()\n","    # sharpe_ratio = portfolio_returns.mean() / portfolio_returns.std()\n","    return -sharpe_ratio # don't forget `* -1` to increase sharpe ratio\n","```\n","\n","A: In your code, the variable **y** in the sharpe_loss function represents the returns data for each asset over  a series of time, structured within a batch from your dataset. Whether **y** needs **requres_grad=True** depends on how you intend to use the gradients calculated from the loss function:\n","\n","## When **`requires_grad=True`** is Necessary\n","1. **Parameter Optimization**: If **y** (or a similar tensor) is a model parameter or if it's being directly optimized (like weights in a typical neural network), you'd want gradients to be computed. In typical supervides learing models, this is not the case for input data like your returns **y**, instead, gradients are required for model weights.\n","2. **Model Inputs**: If you are adjusting model inputs based on gradiet information (common in some advanced techniques like adversarial training or input feature optimization), **y** would require gradients.\n","\n","## Given Cotext: `y` and `requires_grad`\n","In your scenario, **y** is derived from the dataset as return values and is not part of the parameters being optimized directly. It is used for loss calculation against <u>**pred_w**, which presumably would be the model outputs and should have **requires_grad=True** because they are directly influenced by the model parameters</u>.\n","\n","## Checking `requires_grad`\n","if **y** is just input data (as it appears in your setup), it does not need **requires_grad=True**. You are calculating the loss to update the model parameters that influence **pred_w**, not **y**. Here's what generally happens.\n","* **inputs(y)**: Typically do not require gradients because they are not being optimized.\n","* **Model Output(pred_w)**: Should have **requires_grad=True** because they are directly affected by model parameters, and you need gradients to perform backpropagation to optimize these parameters.\n","\n","## Adjusting to the Code\n","If you need to ensure **requires_grad=True** for model outputs(**pred_w** here, if it represents the weights predicted by the model), this is automatically handled when you define these as outputs of your model layers (like your **PortfolioLSTM** model). Ensure that the tensors feeding into your model do not have **requres_grad=True** unintentionally, as it could affect performance and memory usage. HereYs how you might handle this typically:\n","\n","```\n","outputs = model(sequences)  # outputs will have `requires_grad=True` if model parameters have it\n","loss = sharpe_loss(targets, outputs)  # Loss calculation\n","loss.backward()  # Backpropagate to compute gradients\n","```\n","in this step:\n","* **sequences**: Should be your model input and typically don't requre gradients.\n","* **targets**: Are your training targets and don't need gradients.\n","* **outputs**: Are the model's predictions; gradients are required here to update model parameters.\n","\n","\n","## Conclusion\n","Your current setup does not requurei **y** to have **requres_grad=True**, as it serves as input data for calcuating the Sharpe Ratio against the model's predictions(**pred_w**). Only ensure that your model output(**pred_w**) are properly linked to them model's parameters (which they naturally should be if they are computed as shown in your model structur). This setup is typical for training models where only parameters (not inputs or fixed data) need gradient computation.\n","\n"],"metadata":{"id":"uMds17WVdprO"}},{"cell_type":"markdown","source":["# Bad process to break predicted output and linkage to model's parameters\n","In PyTorch, maintaining the proper linkage between model outputs and the model's parameters is crucial for correct gradient computation and effective training. There are a few scenarios or operations that can potentially disrupt this linkage, resulting in the outputs not being correctly associated with the model parameters, which prevents gradients from being properly computed during backpropagation. Here are some common causes:\n","\n","## 1. Detaching the Tensor\n","Using **.detach()** or **.detach_()** on a tensor creates a new tensor that no longer tracks operations for gradient computation. This is often used when you want to perform operations on model outputs without affecting the gradients:\n","```\n","# Example of breaking the gradient link\n","output = model(input)\n","detached_output = output.detach()\n","# Any operation on `detached_output` won't backpropagate to model parameters.\n","```\n","\n","## 2. Using Numpy Conversions\n","Converting a tensor to a NumPy array breaks the connection with the computation graph because NumPy does not support automatic differentiation:\n","```\n","output = model(input)\n","numpy_output = output.numpy()  # This conversion breaks the gradient linkage.\n","# Converting back to a tensor won't restore the original computation graph\n","tensor_output = torch.from_numpy(numpy_output)\n","```\n","\n","## 3. In-place Operations\n","Certain in-place operations can potentially interfere with the gradient computation if they overwrite values that are requred to compute gradients:\n","```\n","output = model(input)\n","output += some_other_tensor  # Safe if done correctly but can be problematic in some contexts\n","```\n","It's particularly risky if **some_oterh_tensor** involves operations that modify gradients or if the operation disrupts the versioning of **output** needed for correct gradient computation.\n","\n","\n","## 4. Incorrect Use of .data or .tensor.data\n","Directly manipulating **.data** of a tensor should be avoided as it bypasses the graph and can lead to incorrect gradient computations:\n","```\n","output = model(input)\n","output.data = some_value  # This is not tracked by Autograd\n","```\n","\n","\n","## 5. Improper Layer Connection\n","Sometimes model outputs may not be correctly linked to model parameters if the layers are not properly connected within the model's architecture. This can happen if certain layers are not included in the forward pass or are excluded from the model's parameter list:\n","```\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.layer1 = nn.Linear(10, 20)\n","        # This layer is created but not used or improperly connected\n","        self.unused_layer = nn.Linear(20, 5)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        # Assume unused_layer is not applied\n","        return x\n","```\n","In this example, **unused_layer** is not contributing to the model output, and its parameters are not being updated during training.\n","\n","\n","## Best Practice\n","To ensure the proper linkage of model outputs to model parameters, follow these best practice:\n","* **Use tensor operations supported by PyTorch's Autograd for all operations that need gradient computation.**\n","* **Avoid converting tensors to and from NumPy or other formats within the computation or training loop unless absolutely necessary, and ensure any such conversion is done outside the gradient computation requirements.**\n","* **Be mindful of tensor operations and manipulation, particularly with in-place operations and direct data manipulations.**\n","\n","\n","By adhering to these practices, you ensure that your model's output remains properly connected to its parameters, allowing effective training and optimization.\n"],"metadata":{"id":"fzsYnyovjP6S"}},{"cell_type":"markdown","source":["## Don't manipulate output!!!\n","When working with model outputs in PyTorch, it's important to be cautious with how you manipulate them to ensure that you don't inadvertently break the connection to the computation graph that tracks gradients. Here are some key points to remember:\n","\n","1. **Avoid Detaching Unless Necessary**: If you use `.detach()` or work with the `.data` attribute, you're effectively stopping gradient computation from that point onward. This should only be used when you intentionally want to prevent gradients from backpropagating or when using the output for non-gradient-based calculations, such as metrics computation or data visualization.\n","\n","2. **Be Careful with In-place Operations**: In-place operations can sometimes interfere with the autograd mechanism, particularly if they modify values that autograd needs to track for gradient calculations. Use in-place operations judiciously and ensure they do not disrupt the training process.\n","\n","3. **Do Not Convert to and from NumPy Inside Training Loops**: Conversions between PyTorch tensors and NumPy arrays break the gradient tracking because NumPy does not support PyTorch’s autograd. Always perform such conversions outside of the computational paths that require gradient updates.\n","\n","4. **Preserve Computational Graph**: Ensure that any operation that needs to retain gradient tracking is part of the main computational graph. Avoid operations that could result in losing the connection to this graph unless that's your explicit intention.\n","\n","By following these guidelines, you can help ensure that your model learns effectively during training because the gradient descent process will have the necessary information to optimize model parameters. Keeping these considerations in mind will help you avoid common pitfalls in implementing training loops and custom model behaviors in PyTorch."],"metadata":{"id":"4ziTG6rlnhWm"}},{"cell_type":"markdown","source":["# Custom Loss Function and Backpropagation Process\n","1. **Prediction Output**: In a neural network, the model gerates predictins during the forward pass. These predictions are typically outputs of the final layer of the model and are a function of the input data and the model's parameters (weights and biases).\n","2. **Loss Calculation**: The custom loss function takes these predictions and, usually, the ture target valeus (labels) as inputs. However, it can also incorporate other features or additional data, as you've mentioned. The loss function computes a scalaer value that quantifies the difference or error between the predictions and the targets.\n","3. **Using the Loss for Backpropagation**:\n","    * **Gradient Computation**: PyTorch uses automatic differentiation to calculate gradients of the loss with respect to each parameter of the model. This process is initiated when you call **.backward()** on the loss tensor. During this step, PyTorch traverses(横断旅行する、通過する) the computation graph backwards from the loss tensor to each parameter tensor that contributed to the loss.\n","    * **Parameter Updates**: After gradients are computed, an optimizer (like SGD, Adam, etc.) adjust the model parmeters based on these grdients. This adjustment aims to minimize the loss, thus improving the model's predictions over training iterations.\n","\n","\n","## Key Aspects of the Backpropagation:\n","* **Gradient Flow**: The gradients flow backward through the same path used to compute the forward predictions. This path, or computation graph, includes all operations and transformations applied to input data and parameters to obtain the prediction.\n","* **Computation Graph Maintenance**: It's crucial that the entire sequence of operations from inputs through predictions to loss computation maintains intact connections. Any operation within this sequence that breaks these connections (like tensor detaching or direct manipulations that bypass PyTorch's autograd system) can prevent gradients from properly propagating back to the parameters, hindering effective training.\n","\n","## Example of a Simple Training Step:\n","```\n","# Assume model, loss_function, optimizer, data_loader are defined\n","for inputs, targets in data_loader:\n","    optimizer.zero_grad()   # Reset gradients accumulation\n","    outputs = model(inputs)  # Forward pass: compute predictions\n","    loss = loss_function(outputs, targets)  # Compute loss\n","    loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n","    optimizer.step()  # Update model parameters\n","```\n","\n","In summary, your understanding of using the loss to guide backpropagation through the model’s parameter linkages is spot on. This process is foundational to training neural networks effectively, ensuring that the model learns to predict more accurately by iteratively reducing the loss over training epochs."],"metadata":{"id":"TFIKRvt3q1xr"}},{"cell_type":"markdown","source":["# computation graph\n","In PyTorch, the computation graph is a fundamental concept that is essential for enabling automatic differentiation, which is the mechanism by which gradients are computed for neural network parameters. The computation graph is essentially a directed graph where the nodes represent tensor operations, and the edges represent the tensor themselves.\n","\n","## Key Componets of the Computation Graph:\n","1. **Nodes**: Each node in the graph represents a tensor operation or function, such as addition, multiplication, or more complex functions like convolution or matrix multiplication. These operations take tensors as inputs and produce new tensors as outputs.\n","2. **Edges**: The edges between nodes represent the tensors that flow between operations. These tensors carry both data (values) and gradients.\n","3. **Leaf Nodes**: These are the entry points of the graph, typically representing the input tensors that are directly manipulated by the user, such as model parameters or input data tensors. Leaf nodes usually have no incoming edges because they are not the result of a PyTorch operation.\n","\n","\n","## How the Computaion Graph Works:\n","* **Forward Pass**: During the forward pass, your model performs calculations to compute the output (or predictions) from the input data. As each operation is performed, PyTorch dynamically constructs the computation graph by adding nodes for each operation and edges for each tensor that is used or produced. This graph represents the complete sequence of operations needed to compute the outputs from the inputs.\n","* **Backward Pass**: When the backward pass is initiated (typically by calling **.backward()** on a tensor, usually the loss tensor), PyTorch traverses this graph in the reverse direction to compute gradients. It starts from the tensor on which .backward() was called and moves backwards through the graph, applying the chain rule to compute the gradient of the loss with respect to each tensor that has **requires_grad=True**.\n","\n","\n","## Why the Computation Graph is Important\n","* **Automatic Differentiation**: The main advantage of having a computation graph is that it allows PyTorch to automatically compute the derivatives of the loss with respect to each parameter, which is critical for training neural networks using gradient-based optimization methods.\n","* **Dynamic Nature**: PyTorch's computation graph is dynamic, which means it is recreated from scratch at each iteration during the training process. This allows for high flexibility in model design, as you can change the shape, size, and operations in the model on the fly without needing to predefine any graph structures.\n","\n","\n","## Example:\n","Here's a simple example of how a computation graph is built and used:\n","```\n","import torch\n","\n","# Create tensors\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n","\n","# Perform operations\n","z = x * y  # Element-wise multiplication\n","s = z.sum()  # Summation\n","\n","# Compute gradients\n","s.backward()\n","\n","# Check gradients\n","print(\"Gradient with respect to x:\", x.grad)\n","print(\"Gradient with respect to y:\", y.grad)\n","```\n","\n","In this example:\n","* The operation **z = x * y** creats a node in teh graph where the operation is multiplication, and *x* and *y* are inputs while **z** is the output.\n","* The operation **s = z.sum()** creates another node where the operation is summation.\n","* When **s.backward()** is called, the graph is traversed backwards to compute the gradients **x.grad** and **y.grad**.\n","\n","\n","This dynamic and automatic construction and traversal of the computation graph are what make PyTorch particularly powerful and flexible for designing and training complex neural networks."],"metadata":{"id":"Tqhvzhlkue9b"}},{"cell_type":"code","source":[],"metadata":{"id":"dOzHjBKqisuo"},"execution_count":null,"outputs":[]}]}