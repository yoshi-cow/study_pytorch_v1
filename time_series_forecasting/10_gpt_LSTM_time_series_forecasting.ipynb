{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1kv2Lthbtu8BPNOIsuHR-Blb6BpJGXVSo","authorship_tag":"ABX9TyNkPS8ZC6I/mdfLkI/mln+v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LSTM_having_features_for_TimeSeries_Forecasting\n","* this code was made by chat-GPT"],"metadata":{"id":"mCQdjBjTaxkl"}},{"cell_type":"markdown","source":["For time series forecasting, especially when predicting future electricity consumption based on multiple features, deep learning models can significantly benefit from proper feature vectorization. This process involves transforming your raw data into a format that the neural network can effectively learn from. Given your scenario with 10 features, here are several strategies to vectorize these features for deep learning models:\n","\n","### 1. **Feature Scaling**\n","\n","First and foremost, normalize or standardize your features. This is crucial for models like neural networks to converge quickly. You can use Min-Max scaling to normalize the data or Z-score normalization to standardize it.\n","\n","- **Normalization (Min-Max Scaling)**: Scales the features to a fixed range, usually [0, 1].\n","- **Standardization (Z-score normalization)**: Scales the features so they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.\n","\n","### 2. **Sequence Windowing**\n","\n","For time series data, it's important to structure your input data into sequences that the model can learn from. This is often done by creating \"windows\" of past observations to predict future values.\n","\n","- **Fixed Windowing**: Create fixed-size input sequences (windows) of your 10 features. For instance, use the past 24 hours of data (assuming hourly sampling) to predict the next hour's electricity consumption.\n","- **Sliding Windows**: Similar to fixed windowing but the window slides by a certain step. For example, you might slide by one hour at a time, creating overlapping windows of data.\n","\n","### 3. **Time Embeddings**\n","\n","If your data includes explicit time stamps (e.g., hour of the day, day of the week), you can convert these into cyclical features using sine and cosine transformations. This helps the model capture time-based patterns like daily or weekly cycles.\n","\n","### 4. **Feature Embeddings for Categorical Data**\n","\n","If any of your 10 features are categorical (e.g., type of day: holiday/weekend/workday), consider using embeddings to convert these categories into continuous vectors. This can be more effective than one-hot encoding for models to capture the nuances of categorical data.\n","\n","### 5. **Lag Features**\n","\n","Create features that are lagged versions of the existing features. For instance, the electricity consumption from the previous day (or the same hour the previous day) can be a powerful feature for predicting future consumption.\n","\n","### 6. **Rolling Window Statistics**\n","\n","Generate statistical features based on rolling windows, such as the mean, median, variance, or sum of the past N hours/days. These features can capture trends and seasonality in the data.\n","\n","### 7. **Differencing**\n","\n","For non-stationary time series data, differencing can help stabilize the mean of the time series by removing changes in the level of a time series, and thus eliminate (or reduce) trend and seasonality.\n","\n","### Incorporating into a Deep Learning Model\n","\n","Once you've vectorized your features using the strategies above, you can feed them into various types of deep learning models suitable for time series forecasting, such as:\n","\n","- **Recurrent Neural Networks (RNNs)**: Good for capturing temporal dependencies.\n","- **Long Short-Term Memory (LSTM) networks**: A special kind of RNNs, effective in learning long-term dependencies.\n","- **Gated Recurrent Units (GRUs)**: Similar to LSTMs, but simpler and often faster to train.\n","- **Convolutional Neural Networks (CNNs)**: Not just for image data; 1D CNNs can be effective for sequence data.\n","- **Transformer models**: Though originally designed for natural language processing, transformers can be adapted for time series forecasting by treating the time series data as a sequence.\n","\n","Remember, the effectiveness of each vectorization technique and model architecture can vary based on the specifics of your dataset and the nature of the forecasting problem. It's often beneficial to experiment with different approaches and combinations thereof."],"metadata":{"id":"KlKR8vBzcdQq"}},{"cell_type":"markdown","source":["## **tips: When do we need to normailze target variable?**\n","The need to normalize a target variable in time series (or any other type of data) largely depends on its characteristics and the modeling approach you're using. Here are types of target variables that often require normalization:\n","\n","1. **Continuous Variables with Large Range**: If your target variable is a continuous variable that spans a large range of values, normalization can help to ensure that the optimization algorithm works efficiently. This is especially true for deep learning models, where having targets on a similar scale can significantly impact the convergence rate and stability of the learning process.\n","\n","2. **Skewed Variables**: For target variables that are highly skewed, normalization (or even log transformation, which is a form of normalization) can help make the distribution more symmetric, improving model performance by making it easier for the model to learn the underlying patterns.\n","\n","3. **Variables with Different Units and Scales**: In the context of multivariate time series forecasting, where you might be predicting multiple targets, normalization ensures that all variables contribute equally to the error term. Without normalization, a variable with a large scale can dominate the gradient updates, potentially leading to suboptimal performance.\n","\n","4. **High Magnitude Variables**: Variables with values that have a high magnitude can lead to numerical instability in deep learning models due to the way floating-point arithmetic is handled in computers. Normalizing these variables to a lower range can help prevent issues like overflow, underflow, or vanishing/exploding gradients.\n","\n","### When You Might Not Need to Normalize:\n","- **Binary or Categorical Targets**: For classification tasks where the target variable is binary or categorical (after being one-hot encoded or otherwise transformed), normalization of the target variable itself is not typically necessary. The focus would instead be on the features.\n","\n","- **Targets with Narrow Range**: If the target variable inherently falls within a narrow range and you're using a model that's less sensitive to the scale of the input (like decision trees or certain ensemble methods), normalization might not be necessary.\n","\n","- **Count Data with Low Variance**: If you're dealing with count data that doesn't vary widely, normalization might not offer significant benefits. However, for highly skewed count data, transformations like log scaling can still be beneficial.\n","\n","It’s important to consider the nature of your target variable and the requirements of your modeling approach when deciding on normalization. Also, the decision to normalize should be guided by experimentation and validation on your specific dataset, as the benefits can vary depending on the context and the peculiarities of the data at hand."],"metadata":{"id":"18IFOIub7HXY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6YAqM0fL1k9"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# LSTM basic tips\n","An LSTM (Long Short-Term Memory) model is a type of recurrent neural network (RNN) that is well-suited for sequence prediction problems, including time series forecasting. LSTMs are specifically designed to address and overcome the limitations of traditional RNNs, such as the vanishing gradient problem, which makes them less effective for learning long-term dependencies in data sequences.\n","\n","### Why LSTMs Are Good for Time Series Forecasting\n","\n","1. **Learning Long-Term Dependencies:** LSTMs can learn and remember over long sequences of inputs, which is crucial for time series data that often contains long-term patterns and dependencies. For example, in financial time series, the effects of a particular event may be felt for a long duration.\n","\n","2. **Handling Sequential Data:** Since time series data is inherently sequential, the LSTM's architecture is naturally suited for this format. LSTMs process data points in sequence, allowing for the prediction of future values based on learned patterns from past observations.\n","\n","3. **Flexibility in Sequence Length:** LSTMs can handle variable-length input sequences, which is beneficial for time series forecasting where the relevant history length may vary.\n","\n","4. **Capability to Process Multivariate Time Series:** LSTMs can handle multiple input variables (features) at each time step, making them ideal for multivariate time series forecasting, where you predict a variable based on its own past values and other covariates.\n","\n","### LSTM Model Architecture\n","\n","An LSTM unit typically consists of three gates that regulate the flow of information:\n","\n","- **Forget Gate:** Decides what information is discarded from the cell state.\n","- **Input Gate:** Updates the cell state with new information from the current input.\n","- **Output Gate:** Determines what the next hidden state should be, which is used for predictions and transferred to the next time step.\n","\n","### Example Use Case in Time Series Forecasting\n","\n","Consider a dataset where you're trying to predict future electricity demand based on past consumption patterns, weather conditions, and time indicators (like hour of the day, day of the week, etc.). An LSTM model can learn from the historical data, recognizing patterns (e.g., increased demand on hot days due to air conditioning use) and using these insights to make accurate future predictions.\n","\n","### Implementation\n","\n","In PyTorch, you can use the `torch.nn.LSTM` class to build your LSTM model. The key parameters to configure are:\n","\n","- `input_size`: The number of features in each input timestep.\n","- `hidden_size`: The number of features in the hidden state.\n","- `num_layers`: Number of layers in the LSTM.\n","- `batch_first`: Whether the input and output tensors are provided with the batch dimension first.\n","\n","### Conclusion\n","\n","LSTMs are powerful for time series forecasting because they can capture long-term dependencies, handle sequential data effectively, and process both univariate and multivariate time series. Their ability to remember and learn from historical data makes them superior for tasks where understanding the context and the sequence of events is crucial for making accurate predictions."],"metadata":{"id":"ssDTjSTzNhXe"}},{"cell_type":"markdown","source":["## Input Size and Sequence length in LSTM\n","In a PyTorch LSTM model, understanding the terms **input_size** and **sequence_length** is crucial for correctly configuring your model and preparing your data. Let's break down what each of these terms means:\n","\n","<br>\n","\n","**input_size**\n","\n","* The **input_size** parameter in an LSTM model refers to <u>the number of features</u> in each input element of the sequence. For example, if you are working with time series data where each timestep's data point includes measurements like temperature, humidity, and wind speed, and you want to include all three in your model, your **input_size** would be 3.\n","* It's important to note that **input_size** is not related to the sequence length or the batch size. It strictly refers to the dimensionality of each timestep within your input sequence.\n","\n","<br>\n","\n","**sequence_length**\n","\n","* The term **sequence_length** is not an explicit parameter you pass to the LSTM in PyTorch but is a concept you need to understand to structure your input data correctly. It reffers to <u>the length of the input sequences that your model will process</u>. This can vary depending on your specific task and how you preprocess your data.\n","* When you feed a batch of sequences into an LSTM, PyTorch expects the input tensor to have a shape of **(seq_len, batch_size, input_size)** if you are using the default settings without setting **batch_first=True**. If **batch_firtst=True** is set, the input tensor is expected to be of shape **(batch_size, seq_len, input_size)**.\n","    * **seq_len**: is the sequence length, indicating how many timesteps are in each sequence.\n","    * **batch_size**: is the batch size, representing how many sequences are processed in parallel.\n","    * **input_size**: is as described above.\n","\n","<br>\n","\n","**Practical Example**\n","\n","Imagin you're analyzing sensor data to predict future measurements, and your dataset includes features like temperature, humidity, and wind speed recorded every hour. If you decide to use the past 24 hours of data to predict the temperature in the next hour, your **input_size** would be 3 (assuming you use all three measurements as features), and your **sequence_length** would be 24. This means each input sequence fed into the LSTM would contain 24 timesteps, with each timestep containing a vector of 3 values.\n","\n","<br>\n","\n","When structuring your LSTM model and preparing your data, it's essential to align these dimensions correctly to ensure your model trains as expected.\n","\n"],"metadata":{"id":"pYWstoOHNq65"}},{"cell_type":"code","source":[],"metadata":{"id":"p960pK2lNht6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GPT4 - using Dataset and DataLoader"],"metadata":{"id":"Ie39S8AkM0bd"}},{"cell_type":"markdown","source":["## About Data Leakage\n","Data leakage occurs when information from outside the training dataset is used to create the model. This can happen during normalization if you normalize your entire dataset before splitting it into training and test sets. To prevent this, we'll split the dataset first and then apply normalization separately to each split.\n","\n","\n","Normalization will be applied within the **Dataset** class, ensuring that it's based only on the statistics of the training set when preparing both training and validation/test sets."],"metadata":{"id":"wF7-LKIDNn-c"}},{"cell_type":"markdown","source":["## Step1: Generate Sample data & split the Dataset"],"metadata":{"id":"e-1xGtI7NXm7"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","np.random.seed(42)  # For reproducibility\n","\n","# Generate a DataFrame with datetime information\n","num_hours = 365 * 24  # A year's worth of hourly data\n","date_rng = pd.date_range(start='1/1/2020', end='31/12/2020', freq='H')\n","df = pd.DataFrame(date_rng, columns=['date'])\n","df['weekday'] = df['date'].dt.weekday\n","df['hour'] = df['date'].dt.hour\n","df['season'] = df['date'].dt.month % 12 // 3 + 1\n","\n","# Generate synthetic features and target variable\n","for i in range(7):  # Additional 7 features\n","    df[f'feature_{i}'] = np.random.rand(len(df))\n","df['electricity_consumption'] = np.random.rand(len(df)) * 100  # Target variable\n","\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"NXRIPrVfNGBQ","executionInfo":{"status":"ok","timestamp":1712199055612,"user_tz":-540,"elapsed":676,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"a15753ff-303f-42fb-e7e9-9db94a9985fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 date  weekday  hour  season  feature_0  feature_1  feature_2  \\\n","0 2020-01-01 00:00:00        2     0       1   0.374540   0.671368   0.409980   \n","1 2020-01-01 01:00:00        2     1       1   0.950714   0.523158   0.838483   \n","2 2020-01-01 02:00:00        2     2       1   0.731994   0.898639   0.185176   \n","3 2020-01-01 03:00:00        2     3       1   0.598658   0.164393   0.554842   \n","4 2020-01-01 04:00:00        2     4       1   0.156019   0.804109   0.722233   \n","\n","   feature_3  feature_4  feature_5  feature_6  electricity_consumption  \n","0   0.421576   0.137686   0.120749   0.616654                 1.923384  \n","1   0.280547   0.260339   0.520433   0.003229                47.550482  \n","2   0.895044   0.489540   0.095159   0.792586                26.352564  \n","3   0.332239   0.061339   0.256357   0.243121                53.995885  \n","4   0.578596   0.095686   0.451709   0.299217                17.865769  "],"text/html":["\n","  <div id=\"df-2a8b53f2-5d3b-4d57-8f2e-12c4501c41a2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>weekday</th>\n","      <th>hour</th>\n","      <th>season</th>\n","      <th>feature_0</th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>feature_6</th>\n","      <th>electricity_consumption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-01-01 00:00:00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.374540</td>\n","      <td>0.671368</td>\n","      <td>0.409980</td>\n","      <td>0.421576</td>\n","      <td>0.137686</td>\n","      <td>0.120749</td>\n","      <td>0.616654</td>\n","      <td>1.923384</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-01-01 01:00:00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.950714</td>\n","      <td>0.523158</td>\n","      <td>0.838483</td>\n","      <td>0.280547</td>\n","      <td>0.260339</td>\n","      <td>0.520433</td>\n","      <td>0.003229</td>\n","      <td>47.550482</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-01-01 02:00:00</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.731994</td>\n","      <td>0.898639</td>\n","      <td>0.185176</td>\n","      <td>0.895044</td>\n","      <td>0.489540</td>\n","      <td>0.095159</td>\n","      <td>0.792586</td>\n","      <td>26.352564</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-01-01 03:00:00</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.598658</td>\n","      <td>0.164393</td>\n","      <td>0.554842</td>\n","      <td>0.332239</td>\n","      <td>0.061339</td>\n","      <td>0.256357</td>\n","      <td>0.243121</td>\n","      <td>53.995885</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-01-01 04:00:00</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0.156019</td>\n","      <td>0.804109</td>\n","      <td>0.722233</td>\n","      <td>0.578596</td>\n","      <td>0.095686</td>\n","      <td>0.451709</td>\n","      <td>0.299217</td>\n","      <td>17.865769</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a8b53f2-5d3b-4d57-8f2e-12c4501c41a2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2a8b53f2-5d3b-4d57-8f2e-12c4501c41a2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2a8b53f2-5d3b-4d57-8f2e-12c4501c41a2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cc2cb15d-9911-4db6-bec1-9a90052d08cd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc2cb15d-9911-4db6-bec1-9a90052d08cd')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cc2cb15d-9911-4db6-bec1-9a90052d08cd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8761,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-01 00:00:00\",\n        \"max\": \"2020-12-31 00:00:00\",\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          \"2020-04-13 12:00:00\",\n          \"2020-04-15 11:00:00\",\n          \"2020-03-11 19:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          8,\n          16,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2884343986745382,\n        \"min\": 1.1634755366141114e-05,\n        \"max\": 0.9997176732861306,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.4626756050653935,\n          0.8284692994491171,\n          0.549529535895017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28865327253328293,\n        \"min\": 5.282693229680113e-05,\n        \"max\": 0.9999248268331765,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.7733040401143754,\n          0.5538309158678235,\n          0.5050578002704087\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2876422502713278,\n        \"min\": 4.8123894311746795e-05,\n        \"max\": 0.9999009770092316,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.9612079012228597,\n          0.42544911655181694,\n          0.991956370803152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28704594194006017,\n        \"min\": 5.536675737993768e-06,\n        \"max\": 0.9997893718125953,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.33263624690907834,\n          0.8409420244918254,\n          0.7918277209316302\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2901410481097046,\n        \"min\": 1.6736257934746313e-05,\n        \"max\": 0.9999721473679823,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.06391397575305591,\n          0.776655442237305,\n          0.31420407746420054\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2887925611047968,\n        \"min\": 9.359385827234501e-05,\n        \"max\": 0.9999569543687702,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.07736641410153167,\n          0.9175483844887636,\n          0.28080288854758184\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2868698677160411,\n        \"min\": 8.432211304310044e-06,\n        \"max\": 0.9999396960457185,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.9612848944348956,\n          0.918599777596716,\n          0.0027404927727919803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"electricity_consumption\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.86914245684661,\n        \"min\": 0.027012235672785323,\n        \"max\": 99.96188950868437,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          36.180315610267165,\n          81.93904449770228,\n          43.06096058596256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Placeholder split logic (actual logic may vary based on time series considerations)\n","train_df = df[:int(0.8*len(df))]\n","test_df = df[int(0.8*len(df)):]"],"metadata":{"id":"EPRMqxs8NF-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step2: Dataset and DataLoader\n","Apply Normalization Separately\n","\n","\n","Normalization is applied within the **Dataset** class. When you create instances of this class for training and test sets, you fit the **MinMaxScaler** on the training set and then apply this fitted scaler to transform the test set data:"],"metadata":{"id":"PO886JFXOg9q"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import MinMaxScaler\n","\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, dataframe, input_steps, forecast_steps, scaler):\n","        \"\"\"\n","        Initialization of the dataset with a pre-fitted scaler.\n","        input_steps: encoder_lenght\n","        forecast_steps: forecast_length\n","        scaler: MinMaxScaler.fit()などすでにfit済みのscaler\n","        \"\"\"\n","        self.input_steps = input_steps\n","        self.forecast_steps = forecast_steps\n","        self.scaler = scaler\n","\n","        # Separate features and target\n","        features = dataframe.drop(columns=['electricity_consumption'])\n","        target = dataframe[['electricity_consumption']]\n","\n","        # Transform features using the already fitted scaler\n","        self.features = self.scaler.transform(features)\n","        self.target = target.values # Numpyにして渡す ※shape(レコード数, 1)の2次元データ\n","\n","    def __len__(self):\n","        return len(self.features) - self.input_steps - self.forecast_steps\n","\n","    def __getitem__(self, idx):\n","        X = self.features[idx:idx+self.input_steps].to_numpy()\n","        y = self.target[idx+self.input_steps:idx+self.input_steps+self.forecast_steps].flatten()\n","        # yはflattenで1次元にしている。（この1次元のデータをバッチ化して2次元にするのは、DataLoaderクラスで行われる）\n","        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n"],"metadata":{"id":"wpAWk0bCNF7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### flatten() in the __getitem__:\n","The use of **flatten()** in the **__getitem__** method of your **TimeSeriesDataset** class serves an important purpose: it ensures that the labels (targets for prediction) are in the correct shape for comparison against the model's predictions during the loss caluculation phase of training.\n","\n","<br>\n","\n","**UnderStanding the Shapes**\n","\n","* **Model's Prediction Shape**: In the modified **ElecticityConsumptionModel**, the output predictions have a shape of **[batch_size, forecast_length]**. For instance, if you're predicting electricity consumptuon for the next 24 hours **(forecasting_length = 24)** for a batch of 20 samples **(batch_size = 20)**, the output predictions will have a shape of **[20, 24]**.\n","* **Target Labels Shape**: Idealy, the targete labels should match this shape exactly for proper loss computation. However, when slicing arrays or tensors, there's a risk of introducing or retaining an unnecesssary extra dimension, resulting in a shape like **[20, 24, 1]** instead of **[20, 24]**.\n","\n","<br>\n","\n","**The role of flatten()**\n","\n","* **Flattening Labels**: By applying **flatten()**, you remove any extra dimensions in the labels, converting a potential shape of **[20, 24, 1]** to **[20, 24]**. This operation ensures that the labels are directly comparable to the model's output without dimension mismatch issues.\n","* **Why It's Necessary**: During the training phase, specifically in the loss calculation step, PyTorch expects the predictions and labels to have compatible shapes. A mismatch, such as an extra dimension in the labels, can lead to errors or incorrect loss calculations. Using **flatten()** (or similarly **squeeze()**) standardizes the shapes, facilitating correct and efficient tarining.\n","\n","<br>\n","\n","**Example**\n","\n","Suppose your lables tensor initially has a shape of **[20, 24, 1]** due to how the data was sliced or prepared. this shape indicates that each of the 24 forecasted hours has been encapsulated in its own dimension (the extra **1**), which is unnecessary for comparison with the model's output. Flattening adjusts this to **[20, 24]**, aligning it with the prediction shape and allowing for correct loss computation."],"metadata":{"id":"9bNvSJqOBG88"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","encoder_length = 168  # 7 days of hourly records (= sequence length)\n","forecast_length = 24  # Predicting the next 24 hours\n","\n","# Fit scaler on training features\n","scaler = MinMaxScaler()\n","\n","# Drop the 'date' column along with 'electricity_consumption' to prepare features for scaling\n","features_train = train_df.drop(columns=['date', 'electricity_consumption'])\n","scaler.fit(features_train)\n","\n","# When initializing your datasets, ensure the 'date' column is also excluded from the features\n","train_dataset = TimeSeriesDataset(train_df.drop(columns=['date']), encoder_length, forecast_length, scaler)\n","test_dataset = TimeSeriesDataset(test_df.drop(columns=['date']), encoder_length, forecast_length, scaler)\n","\n","# set DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"],"metadata":{"id":"BOOpdzfjNF30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for X, y in train_loader:\n","    print(X.shape)\n","    print(y.shape)\n","    break\n","\n","# By using 168 hours records with 10 features, we predict next 24 hours.\n","# 20 is the numbers of batch size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dicm0tnNntuj","executionInfo":{"status":"ok","timestamp":1712142136493,"user_tz":-540,"elapsed":393,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"15f83350-9d29-4a22-fe74-59dc1fb6d00f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([20, 168, 10])\n","torch.Size([20, 24])\n"]}]},{"cell_type":"markdown","source":["### Data Structure of `self.target` in `__getitem__`\n","The data structure of **self.target** in the **__getitem__** method is initially determined by how it's set in the **__init__** method. Since **self.target** is assigned as **target.values**, wehere **target** is a DataFrame containing only the **electicity_consumption** column, **self.target** will be a 2D numpy array with shape **(n, 1)**, where **n** is the number of rows in the DataFrame. This shape corresponds to the total number of data points in your dataset for the target variable.\n","\n","\n","When you access **self.target** within **__getitem__**, for each item, you're slicing this array to get a portion of it based on **idx**, **input_steps**, and **forecast_steps**. This slicing operation for **y**:\n","\n","```\n","y = self.target[idx+self.input_steps:idx+self.input_steps+self.forecast_steps].flatten()\n","```\n","\n","This line takes a slice of **self.target**, corresponding to the forecast period, and then flatten it. The flattening operation changes its **<u>shape from a 2D array to a 1D array</u>**. Therefore, after flattening, if **forecast_steps** were 24, for example, **y** would have a shape of **(24, )**. The flattening is done because your target varialbe (**y**) for each sample is expected to be a 1D tensor representing the series of electricity consumption values you're trying to predict for the forecast period.\n","\n","\n","to summarize, before flattening, each slice of **self.target** that corresponds to a single **y** in **__getitem__** would have a shape like **(forecast_steps, 1)**, after flattening, its shape would be **(forecast_steps, )**.\n"],"metadata":{"id":"9Gk3OpmLAOCB"}},{"cell_type":"code","source":["df[['electricity_consumption']].values.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fkS89wi-BRML","executionInfo":{"status":"ok","timestamp":1712199155955,"user_tz":-540,"elapsed":11,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bae69f88-d83b-449c-be52-8596778c6611"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8761, 1)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df[['electricity_consumption']].values.flatten().shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oazmb_bWBRIv","executionInfo":{"status":"ok","timestamp":1712199775979,"user_tz":-540,"elapsed":17,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"42aa0492-52d4-4c19-a3ff-5bf526e784ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8761,)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### Where is batch size data created?\n","The batch dimension is not explicitly created within the **__getitem__()** method of a PyTorch **Dataset** class. Instead, the batching logic is hadled by the **DataLoader**, which wraps around the **Dataset**.\n","\n","<br>\n","\n","**__getitem__() metohd**:\n","\n","* The **__getitem__()** method is responsible for retrieving a <u>single item</u> from the dataset. When you implement a custom dataset by subclassing PyTorch's **Dataset**, you define how a single sample of data is processed and returned by this method. In your case, for each index **idx**, **__getitem__()** returns a single sample (and its correspoding label or target) where both input(**x**) and target(**y**) are shaped according to the individual sample's requirements. For the target, this means a 1D tensor with th length equal to **forecast_steps**, as per your setup.\n","\n","<br>\n","\n","**DataLoader and Batching**:\n","* The **DataLoader** takes your **Dataset** instance and allows for easy iteration over the dataset in mini-batches. When you use a **DataLoader** with your dataset, it automatically gathers samples into batches. It does this by calling the **__getitem__()** method of your dataset multiple times to fetch individual samples and then stacking these samples together to form a batch.\n","* By default, <u>the **DataLoader** adds an extra dimension (the batch dimension) as the first dimension of the tensors</u> it creates. This means if your **__getitem__()** method returns a target tensor **y** with shape **(forecast_steps, )** for a single sample, and you set your **DataLoader**'s **batch_size** to **N**, the DataLoader will combine these individual samples into a batch where the shape of **y** in each batch will be **(N, forecast_steps)**. This is because it stacks **N** such 1D tensors along a new dimension, resulting in a 2D tensor.\n","\n","\n","For example, if you create a DataLoader with your **TimeSeriesDataset** like this:\n","\n","```\n","dataset = TimseSeriesDataset(dataframe=df, input_steps=12, forecast_steps=24, scaler=scaler)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","```\n","\n","For each iteration over the **dataloader**, it will yield batches where each **X** has the shape **(32, input_steps, number_of_features)** and each **y** has the shape **(32, forecast_steps)**, assuming **input_steps** is the length of the input sequence and **number_of_features** is the number of features per tiemstep.\n","\n","<br>\n","\n","This batching mechanism is crucial for training neural network efficiently, as it allows for parallel processing of multiple data smples, reducing trainig time and leveraging optimization techniques like mini-batch gradient descent."],"metadata":{"id":"bh2pJjp3QYPF"}},{"cell_type":"code","source":[],"metadata":{"id":"XocCYDAa56O_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step3: Define the Model\n","* compared to GPT4(No Dataset) edition, My `ElectricityConsumptionModel` might need some modifications to work effectively with the **train_loader** and **test_loader** from the modified dataset and batching approach. Specifically, the modifications will adress how the model processes batches of data and integrates with the changed shape of input sequences and labels generated by the **DataLoader**."],"metadata":{"id":"x--TuAMnmzbn"}},{"cell_type":"markdown","source":["### Key considerations:\n","1. **Batch Processing**: Ensure the model can handle batches of sequences as input. This involves correctly handling the input dimensions expected by the LSTM layer.\n","2. **Output Size**: Since you're predicting the next 7 days (168 hours) but your model's current output is set for the last 24 hours (this is menthion about No Dataset edition model below), you need to adjust the output size if your intention is to predict a different time frame.\n","3. **Sequence Dimensions**: LSTM in PyTorch expects inputs of the shape **(seq_len, batch, input_size)**, but if you're using batch_first=True, it exptects **(batch, seq_len, input_size)**. Make sure your data conforms to these expectations.\n","\n","\n","Given these points, here is an updated version of your model assuming you wish to predict the next 24 hours and that your datase sequences are prepared accordingly:"],"metadata":{"id":"r0exZxlDnjXD"}},{"cell_type":"code","source":["class ElectricityConsumptionModel(nn.Module):\n","    def __init__(self, input_size, hidden_layer_size, output_size=24):\n","        super(ElectricityConsumptionModel, self).__init__()\n","        self.hidden_layer_size = hidden_layer_size\n","\n","        # Assuming your data is batch_first\n","        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n","\n","        # Adjust the linear layer to output one value per time step\n","        self.linear = nn.Linear(hidden_layer_size, output_size)\n","\n","    def forward(self, input_seq):\n","        # No need to manually reshape if your DataLoader already structures batches correctly\n","        lstm_out, _ = self.lstm(input_seq)\n","\n","        # lstm_out shape is [batch_size, seq_len, hidden_layer_size]\n","        # We want to apply the linear layer to each time step, so we reshape accordingly\n","        # mistake\n","        # predictions = self.linear(lstm_out.contiguous().view(-1, self.hidden_layer_size))\n","        # Reshape predictions to [batch_size, seq_len, output_size]\n","        # Assuming output_size corresponds to the forecast length, e.g., 168 for 7 days\n","        # predictions = predictions.view(input_seq.size(0), -1, self.linear.out_features)\n","\n","        # Taking the output of the last step from LSTM which is relevant for the prediction\n","        # Note: lstm_out[:, -1, :] gives us the last step output for all batches\n","        predictions = self.linear(lstm_out[:, -1, :])\n","\n","        return predictions\n"],"metadata":{"id":"0bPlrVFFedro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muJmFpR3uCxz","executionInfo":{"status":"ok","timestamp":1712148281699,"user_tz":-540,"elapsed":404,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"f31a08cd-fec6-4562-e8dc-ed1184f71a84"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 168, 10])"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["print('y.shape: ', y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlyO9Foi7CUE","executionInfo":{"status":"ok","timestamp":1712148283581,"user_tz":-540,"elapsed":688,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bbede269-5038-4ace-8803-7ae542062c60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["y.shape:  torch.Size([20, 24])\n"]}]},{"cell_type":"code","source":["lstm_layer = nn.LSTM(input_size=10, hidden_size=100)\n","liner_layer = nn.Linear(100, 24)"],"metadata":{"id":"jIOf3EcQedo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm_out, _ = lstm_layer(X)\n","print(lstm_out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdQhABV0edmS","executionInfo":{"status":"ok","timestamp":1712147127268,"user_tz":-540,"elapsed":364,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"13cafd75-a746-4ca5-924e-ecb0b18cfcd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([20, 168, 100])\n"]}]},{"cell_type":"code","source":["predictions = liner_layer(lstm_out[:, -1, :])\n","print(predictions.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3WRg9Va6-B2","executionInfo":{"status":"ok","timestamp":1712147195247,"user_tz":-540,"elapsed":12,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bcffd7cb-9667-44ff-b463-fccbb33713bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([20, 24])\n"]}]},{"cell_type":"code","source":["### 以下は、forward() stepにて、データ構造を間違えたバージョン\n","# 参考用にとっておく\n","\n","# predictions = self.linear(lstm_out.contiguous().view(-1, self.hidden_layer_size))\n","liner_input = lstm_out.contiguous().view(-1, 100)\n","print(liner_input.shape)\n","# 20 * 168 = 3360\n","'''\n","torch.Size([3360, 100])\n","'''\n","\n","predictions = liner_layer(liner_input)\n","print(predictions.shape)\n","'''\n","torch.Size([3360, 24])\n","'''\n","\n","print('input_seq.size(0)= X.size(0): ',X.size(0) ) # <- batchの数を取り出し\n","print('self.linear.out_features: ',liner_layer.out_features) # <- linear層の出力要素数\n","# predictions = predictions.view(input_seq.size(0), -1, self.linear.out_features)\n","print('shape before view(input_seq.size(0), -1, self.linear.out_features): ',predictions.shape)\n","predictions = predictions.view(X.size(0), -1, liner_layer.out_features)\n","print('shape after predictions.view(): ', predictions.shape)\n","'''\n","input_seq.size(0)= X.size(0):  20\n","self.linear.out_features:  24\n","shape before view(input_seq.size(0), -1, self.linear.out_features):  torch.Size([3360, 24])\n","shape after predictions.view():  torch.Size([20, 168, 24])\n","\n","upper result is fault by gpt4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rwO0YnBetN6t","executionInfo":{"status":"ok","timestamp":1712144063792,"user_tz":-540,"elapsed":4,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"9233068f-8443-4883-c330-9666f566f2be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_seq.size(0)= X.size(0):  20\n","self.linear.out_features:  24\n","shape before view(input_seq.size(0), -1, self.linear.out_features):  torch.Size([3360, 24])\n","shape after predictions.view():  torch.Size([20, 168, 24])\n"]}]},{"cell_type":"markdown","source":["* Modifications Expained\n","    * **Batch Handling**: The model's LSTM layer is set with **batch_first=True**, meaning the input sequences should be shaped as **(batch, seq_len, input_size)**, which aligns with how the data is prepared by the **DataLoader**.\n","    * **Output Size**: The ``output_size** parameter in the **__init__**method defaults to 24, assuming you want to predict the next 24 hours. The **Linear** layer is adjusted accordingly.\n","    * **Forward Pass Adjustments**: The forward pass doesn't need to reshape the input sequence if your **DataLoader** correctly batches the data. The LSTM output is directly passed to the **Linear** layer after reshaping it to match the expected dimensions. The output predictions are then reshaped to ensure they're in the format of **(batch_size, seq_len,output_size)** before being returned.\n","\n","\n","This model is now prepared to work with your batched data for both training and validation steps, assuming the sequences and labels are structures and labels are structured correclty by your custom **Dataset** and **DataLoader** setup."],"metadata":{"id":"CZJFbBZgvVBC"}},{"cell_type":"markdown","source":["## Step4: Training Step & Test(Validation) Step"],"metadata":{"id":"-cINzJ1m1nK0"}},{"cell_type":"markdown","source":["### Training Step\n","The training step involves iterating over the train_loader, passing each batch of data through the model, calculating the loss, and updating the model's parameters based on this loss."],"metadata":{"id":"8dLQxdOZ3N4l"}},{"cell_type":"code","source":["def train_model(model, train_loader, loss_function, optimizer, epochs=10):\n","    model.train() # Set the model to training mode\n","\n","    for epoch in range(epochs):\n","        total_loss = 0.0\n","        for seq, labels in train_loader:\n","            optimizer.zero_grad() # Clears existing gradients\n","            y_pred = model(seq) # Generate prediction\n","            loss = loss_function(y_pred, labels) # Calculate loss\n","            loss.backward() # Backpropagation\n","            optimizer.step() # Update model parameters\n","\n","            total_loss += loss.item()\n","        average_loss = total_loss / len(train_loader)\n","        if epoch % 2 == 0:\n","            print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}\")\n"],"metadata":{"id":"VGJpmh0htN34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test (Validation) Step\n","The test step involves iterating over the test_loader to evaluate the model's performance on unseen data. It's crucial to disable gradient computations during this phase since we're only interested in assessing the model, not training it."],"metadata":{"id":"KgTtFq-_3Qr8"}},{"cell_type":"code","source":["def validate_model(model, test_loader, loss_function):\n","    model.eval() # Set the model to evaluation mode\n","    total_loss = 0\n","    with torch.no_grad(): # Disable gradient computation\n","        for seq, labels in test_loader:\n","            y_pred = model(seq) # Generate prediction\n","            loss = loss_function(y_pred, labels) # Calculate loss\n","            total_loss += loss.item()\n","\n","    average_loss = total_loss / len(test_loader)\n","    print(f\"Test Loss: {average_loss:.4f}\")"],"metadata":{"id":"qxcigsdAvGwW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Integrating the Training and Validation Steps\n","Now, let's integrate these steps with the rest of your setup. We assume the ElectricityConsumptionModel, loss_function, and optimizer are already defined and instantiated based on your provided model class and settings."],"metadata":{"id":"DxDxF_OG32wo"}},{"cell_type":"code","source":["# Model instantiation\n","input_size = 10\n","hidden_layer_size = 100\n","output_size = forecast_length\n","model = ElectricityConsumptionModel(input_size, hidden_layer_size, output_size)\n","\n","# Loss and optimizer\n","loss_function  = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training the model\n","train_model(model, train_loader, loss_function, optimizer, epochs=10)\n","\n","# Validating the model\n","validate_model(model, test_loader, loss_function)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRuBPmusvGtK","executionInfo":{"status":"ok","timestamp":1712148877585,"user_tz":-540,"elapsed":196544,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"a7d5a5e7-2dd0-438f-bcd2-ee85aa76e6a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 1978.8811\n","Epoch 3/10, Loss: 840.8022\n","Epoch 5/10, Loss: 834.6953\n","Epoch 7/10, Loss: 834.7721\n","Epoch 9/10, Loss: 834.8583\n","Test Loss: 834.9776\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2gopplDpBBnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m4S4mQ5HBBfC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GPT4 - No Dataset and DataLoader edition\n","* **caution**\n","    * This section's code has data leakage!!!"],"metadata":{"id":"14wHlrGSxYN7"}},{"cell_type":"markdown","source":["## Step1: Environment Setup\n","First, ensure you have PyTorch installed in your environment. If not, you can install it using pip:"],"metadata":{"id":"7ngSnLWqxonf"}},{"cell_type":"code","source":["!pip install torch torchvision"],"metadata":{"id":"NuFKwosnxZ1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Generate Sample Data\n","We'll create synthetic electricity consumption data with the specified features and hourly records. For simplicity, our features will be randomly generated but will follow a logical pattern for a time series scenario."],"metadata":{"id":"Ww6-y3TCxqVd"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","np.random.seed(42)  # For reproducibility\n","\n","# Generate a DataFrame with datetime information\n","num_hours = 365 * 24  # A year's worth of hourly data\n","date_rng = pd.date_range(start='1/1/2020', end='31/12/2020', freq='H')\n","df = pd.DataFrame(date_rng, columns=['date'])\n","df['weekday'] = df['date'].dt.weekday\n","df['hour'] = df['date'].dt.hour\n","df['season'] = df['date'].dt.month % 12 // 3 + 1\n","\n","# Generate synthetic features and target variable\n","for i in range(7):  # Additional 7 features\n","    df[f'feature_{i}'] = np.random.rand(len(df))\n","df['electricity_consumption'] = np.random.rand(len(df)) * 100  # Target variable\n","\n","df.head()\n"],"metadata":{"id":"Ce1fxEk0xr-t","colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"status":"ok","timestamp":1712031752625,"user_tz":-540,"elapsed":509,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"ea674ad1-8f1f-4fe7-eae9-7e694856f4ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-06ab439b6234>:8: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n","  date_rng = pd.date_range(start='1/1/2020', end='31/12/2020', freq='H')\n"]},{"output_type":"execute_result","data":{"text/plain":["                 date  weekday  hour  season  feature_0  feature_1  feature_2  \\\n","0 2020-01-01 00:00:00        2     0       1   0.374540   0.671368   0.409980   \n","1 2020-01-01 01:00:00        2     1       1   0.950714   0.523158   0.838483   \n","2 2020-01-01 02:00:00        2     2       1   0.731994   0.898639   0.185176   \n","3 2020-01-01 03:00:00        2     3       1   0.598658   0.164393   0.554842   \n","4 2020-01-01 04:00:00        2     4       1   0.156019   0.804109   0.722233   \n","\n","   feature_3  feature_4  feature_5  feature_6  electricity_consumption  \n","0   0.421576   0.137686   0.120749   0.616654                 1.923384  \n","1   0.280547   0.260339   0.520433   0.003229                47.550482  \n","2   0.895044   0.489540   0.095159   0.792586                26.352564  \n","3   0.332239   0.061339   0.256357   0.243121                53.995885  \n","4   0.578596   0.095686   0.451709   0.299217                17.865769  "],"text/html":["\n","  <div id=\"df-41679591-3975-4040-80d6-12072992ee1b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>weekday</th>\n","      <th>hour</th>\n","      <th>season</th>\n","      <th>feature_0</th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>feature_6</th>\n","      <th>electricity_consumption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-01-01 00:00:00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.374540</td>\n","      <td>0.671368</td>\n","      <td>0.409980</td>\n","      <td>0.421576</td>\n","      <td>0.137686</td>\n","      <td>0.120749</td>\n","      <td>0.616654</td>\n","      <td>1.923384</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-01-01 01:00:00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.950714</td>\n","      <td>0.523158</td>\n","      <td>0.838483</td>\n","      <td>0.280547</td>\n","      <td>0.260339</td>\n","      <td>0.520433</td>\n","      <td>0.003229</td>\n","      <td>47.550482</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-01-01 02:00:00</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.731994</td>\n","      <td>0.898639</td>\n","      <td>0.185176</td>\n","      <td>0.895044</td>\n","      <td>0.489540</td>\n","      <td>0.095159</td>\n","      <td>0.792586</td>\n","      <td>26.352564</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-01-01 03:00:00</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.598658</td>\n","      <td>0.164393</td>\n","      <td>0.554842</td>\n","      <td>0.332239</td>\n","      <td>0.061339</td>\n","      <td>0.256357</td>\n","      <td>0.243121</td>\n","      <td>53.995885</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-01-01 04:00:00</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0.156019</td>\n","      <td>0.804109</td>\n","      <td>0.722233</td>\n","      <td>0.578596</td>\n","      <td>0.095686</td>\n","      <td>0.451709</td>\n","      <td>0.299217</td>\n","      <td>17.865769</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41679591-3975-4040-80d6-12072992ee1b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-41679591-3975-4040-80d6-12072992ee1b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-41679591-3975-4040-80d6-12072992ee1b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-893bacb1-420d-46a9-bfd4-367cb9c0b086\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-893bacb1-420d-46a9-bfd4-367cb9c0b086')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-893bacb1-420d-46a9-bfd4-367cb9c0b086 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8761,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-01 00:00:00\",\n        \"max\": \"2020-12-31 00:00:00\",\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          \"2020-04-13 12:00:00\",\n          \"2020-04-15 11:00:00\",\n          \"2020-03-11 19:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 23,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          8,\n          16,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2884343986745382,\n        \"min\": 1.1634755366141114e-05,\n        \"max\": 0.9997176732861306,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.4626756050653935,\n          0.8284692994491171,\n          0.549529535895017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28865327253328293,\n        \"min\": 5.282693229680113e-05,\n        \"max\": 0.9999248268331765,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.7733040401143754,\n          0.5538309158678235,\n          0.5050578002704087\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2876422502713278,\n        \"min\": 4.8123894311746795e-05,\n        \"max\": 0.9999009770092316,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.9612079012228597,\n          0.42544911655181694,\n          0.991956370803152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28704594194006017,\n        \"min\": 5.536675737993768e-06,\n        \"max\": 0.9997893718125953,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.33263624690907834,\n          0.8409420244918254,\n          0.7918277209316302\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2901410481097046,\n        \"min\": 1.6736257934746313e-05,\n        \"max\": 0.9999721473679823,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.06391397575305591,\n          0.776655442237305,\n          0.31420407746420054\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2887925611047968,\n        \"min\": 9.359385827234501e-05,\n        \"max\": 0.9999569543687702,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.07736641410153167,\n          0.9175483844887636,\n          0.28080288854758184\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2868698677160411,\n        \"min\": 8.432211304310044e-06,\n        \"max\": 0.9999396960457185,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.9612848944348956,\n          0.918599777596716,\n          0.0027404927727919803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"electricity_consumption\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.86914245684661,\n        \"min\": 0.027012235672785323,\n        \"max\": 99.96188950868437,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          36.180315610267165,\n          81.93904449770228,\n          43.06096058596256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["print(df['date'].min())\n","print(df['date'].max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfeeskCCIVDQ","executionInfo":{"status":"ok","timestamp":1712033206201,"user_tz":-540,"elapsed":261,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"40e3cefb-67f8-4f93-b77e-35911fe26ca0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2020-01-01 00:00:00\n","2020-12-31 00:00:00\n"]}]},{"cell_type":"markdown","source":["## Step 3: Data Preprocessing\n","We'll need to normalize our features and create sequences of data that our model can learn from. We will split the data into training and test sets as well."],"metadata":{"id":"dIeeDKrNxuEn"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Normalize features\n","scaler = MinMaxScaler()\n","df.iloc[:,1:-1] = scaler.fit_transform(df.iloc[:,1:-1])\n","\n","# Function to create sequences\n","def create_sequences(input_data, target_data, input_steps, forecast_steps):\n","    X, y = [], []\n","    for i in range(len(input_data) - input_steps - forecast_steps):\n","        X.append(input_data.iloc[i:(i+input_steps)].values) # レコードをinput_steps分取り出し\n","        y.append(target_data.iloc[i+input_steps:i+input_steps+forecast_steps].values)\n","    return np.array(X), np.array(y)\n","\n","encoder_length = 168  # 7 days of hourly records (= sequence length)\n","forecast_length = 24  # Predicting the next 24 hours\n","\n","# Creating sequences\n","X, y = create_sequences(df.iloc[:,1:-1], df[['electricity_consumption']], encoder_length, forecast_length)\n","# X, y = create_sequences(df.iloc[:,0:-1], df[['date']], encoder_length, forecast_length)\n","\n","# Splitting dataset\n","# !!! data leaking occer here!!! We should split data first, after that, we normalize each split data!!!\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"],"metadata":{"id":"KMyaDLOQxvxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZcZ0qGvCAXT","executionInfo":{"status":"ok","timestamp":1712031806927,"user_tz":-540,"elapsed":291,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"8ae0817a-fae1-4a6e-f5de-e0efef631308"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8761, 12)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["X.shape\n","# 特徴量11個で1レコードを構成し、168レコードを一回の予測に使う"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByD6JZP2pKRL","executionInfo":{"status":"ok","timestamp":1712031808820,"user_tz":-540,"elapsed":282,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"75dad67e-d4d7-43ba-a97b-061f604b2110"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8569, 168, 11)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4GDfYiCkDqqy","executionInfo":{"status":"ok","timestamp":1712031971269,"user_tz":-540,"elapsed":243,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"e9c2e5d0-0a8d-4c53-f3f5-e422db521e7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8569, 24, 1)"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["from PIL import Image\n","im = Image.open('/content/drive/MyDrive/study_DeepLearning/Pytorchによる時系列予測(forecasting使わない)/data_structure.jpg')\n","im"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":868,"output_embedded_package_id":"1fGgJbKEsCUPr3L-DKi6LiH4FV821dqlh"},"id":"hDyOPa8mK0-p","executionInfo":{"status":"ok","timestamp":1712034013600,"user_tz":-540,"elapsed":3598,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"34652f50-afa8-4cf4-e732-f76025fa7beb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["* ※ 上記では、batch処理しないので、バッチサイズは考慮されていない！！！"],"metadata":{"id":"zkTbFZJHaXAe"}},{"cell_type":"code","source":[],"metadata":{"id":"L0rGgoBPLjbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.iloc[:,1:-1].columns\n","# 日付と電気消費量(y)列以外を抽出"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CHKEgjApKYH","executionInfo":{"status":"ok","timestamp":1712031230866,"user_tz":-540,"elapsed":277,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"6d455542-09a8-4b86-831c-88c133edc1cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['weekday', 'hour', 'season', 'feature_0', 'feature_1', 'feature_2',\n","       'feature_3', 'feature_4', 'feature_5', 'feature_6'],\n","      dtype='object')"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Step 4: Define the Model\n","For transfer learning, let's assume we have a pre-trained model that we want to adapt. We'll create a simple LSTM model for demonstration. The adaptation will happen in the final layers, where we adjust the model to predict the next 24 hours of electricity consumption."],"metadata":{"id":"zPBUDvJSxxLO"}},{"cell_type":"markdown","source":["CLASS **torch.nn.LSTM(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0, device=None, dtype=None)**\n","<br><br>\n","* Parameters\n","    * input_size – The number of **expected features** in the input x\n","    * hidden_size – The number of features in the hidden state h\n","    * num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n","    * bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n","    * batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n","    * dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n","    * bidirectional – If True, becomes a bidirectional LSTM. Default: False\n","    * proj_size – If > 0, will use LSTM with projections of corresponding size. Default: 0"],"metadata":{"id":"NT9eWJedM7g2"}},{"cell_type":"markdown","source":["モデル呼出し時と出力時のデータ構造\n","* Inputs: input, (h_0, c_0)\n","    * **input**: tensor of shape **(L, H_in)** for unbatched input, **(L, N, H_in)** when `batch_first=False` or **(N, L, H_in)** when `batch_first=True` containing features of the input sequence. The input can also be a packed variable length  sequence.\n","    * **h_0**: tensor of shape (D * num_layers, H_out) for unbatched input or (D * num_layers, N, H_out) containing the initial hidden state for each element in the input sequence. Defaults to zeros if (h_0, c_0) is not provided.\n","    * **c_0**: tensor of shape (D * num_layers, H_cell) for unbatched input or (D * num_layers, N, H_cell) containing the initial cell state for each element in the input sequence. Defaluts to zeros if (h_0, c_0) is not provided.\n","    * where:\n","        * N = batch size\n","        * L = sequence length\n","        * D = 2 if bidirectional = True otherwise 1\n","        * H_in = input_size\n","        * H_cell = hidden_size\n","        * H_out = proj_size if proj_size > 0 otherwise hidden_size\n","* Outputs: output, (h_n, c_n)\n","    * **output**: tensor of shape (L, D * H_out) for unbathed input, (L, N, D * H_out) when `batch_first=False` or (N, L, D * H_out) when `batch_first=True` containing the output features (h_t) from the last layer of the LSTM, for each t . if a `torch.nn.utils.rnn.PackedSequence` has been given as the input, the output will also be a packed sequence. When `bidrectional=True`, output will contain a concatenation of the forward and reverse hidden states at each time step in the sequence.\n","    * **h_n**: 省略\n","    * **c_n**: 省略\n"],"metadata":{"id":"-FLaW5_dNk1o"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class ElectricityConsumptionModel(nn.Module):\n","    def __init__(self, input_size, hidden_layer_size, output_size):\n","        super(ElectricityConsumptionModel, self).__init__()\n","        self.hidden_layer_size = hidden_layer_size\n","\n","        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n","\n","        self.linear = nn.Linear(hidden_layer_size, output_size)\n","\n","    def forward(self, input_seq):\n","        # input_seq = X_trainのイテレーションデータ:Size(encoder_length, features)=(168, 10)\n","        # input_seq.view(len(input_seq) ,1, -1)により(168,10)にバッチの次元を追加している->(168, 1, 10)\n","        # ↑ lstmでbatch_firstを設定していないので、lstmへの入力値のshapeは(seqeunce_length, batch_size, features)\n","        #　　のため、その3次元にしている\n","        lstm_out, _ = self.lstm(input_seq.view(len(input_seq) ,1, -1))\n","        # lstm_outのshape ->(168, 1, 100) この最後の100はhidden_layer_size\n","        # lstm_out.view(len(input_seq), -1)により、shapeを(168, 10)に変換している\n","        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n","        # predictionsのshape -> (168, 1)\n","        return predictions[-24:]  # We're interested in the last 24 hours\n","\n","# Model instantiation\n","model = ElectricityConsumptionModel(input_size=10, hidden_layer_size=100, output_size=1)\n"],"metadata":{"id":"MG63xFq5xy91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Train the Model\n","For simplicity, we'll outline a basic training loop. In a real-world scenario, you'd include validation checks, possibly adjust learning rates, and perform more complex model evaluations."],"metadata":{"id":"vj73PYuKx0tx"}},{"cell_type":"code","source":["def train_model(model, X_train, y_train, epochs=10):\n","    loss_function = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    model.train()\n","    for i in range(epochs):\n","        for seq, labels in zip(X_train, y_train):\n","            optimizer.zero_grad()\n","            y_pred = model(torch.tensor(seq, dtype=torch.float32))\n","            single_loss = loss_function(y_pred, torch.tensor(labels, dtype=torch.float32))\n","            single_loss.backward()\n","            optimizer.step()\n","\n","        if i%2 == 0:\n","            print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n","\n","# Training the model (This will not run here due to computational limitations)\n","train_model(model, X_train, y_train, epochs=2)\n"],"metadata":{"id":"VLu069pvx3fm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 6: Validate the Model\n","After training, you should validate your model's performance on the test dataset to ensure it generalizes well."],"metadata":{"id":"R6b1siY1x480"}},{"cell_type":"code","source":["def validate_model(model, X_test, y_test):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for seq, labels in zip(X_test, y_test):\n","            y_pred = model(torch.tensor(seq, dtype=torch.float32))\n","            predictions.append(y_pred.numpy())\n","            actuals.append(labels)\n","    # Calculate accuracy or any other performance metrics\n","    # This is a placeholder for actual performance calculation\n","    print(\"Validation complete - model performance metrics here\")\n","    return predictions, actuals\n","\n","# Validate the model (This will not run here due to computational limitations)\n","predictions, actuals = validate_model(model, X_test, y_test)\n"],"metadata":{"id":"FdaVxIvux7mF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(predictions))\n","print(len(actuals))"],"metadata":{"id":"cjNiRp4Hx9K6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSTM by gpt3.5"],"metadata":{"id":"IXZfFMlGeErG"}},{"cell_type":"markdown","source":["## Step 1: Data Preparation\n","First, you need to prepare your dataset. This includes loading your data, normalizing it, and creating input sequences and their corresponding labels."],"metadata":{"id":"X9C4J6cFeHZJ"}},{"cell_type":"markdown","source":["### Generate Sample Data\n","This data will consists of 10 features, with each row representing an hourly record."],"metadata":{"id":"5KU11PHA7RmS"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","def generate_sample_data(num_records=1000):\n","    # Generate random data for 10 features\n","    data = np.random.rand(num_records, 10)\n","\n","    # Assume the last feature is related to electricity consumption\n","    # and use it to create a target variable\n","    # The actual consumption is some combination of the features plus noise\n","    consumption = data[:, -1] * 0.5 + np.random.normal(0, 0.02, size=num_records)\n","\n","    return pd.DataFrame(data, columns=[f'feature{i}' for i in range(1, 11)]), consumption\n","\n","features, consumption = generate_sample_data()"],"metadata":{"id":"7vxcj4An7ROT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"Rs7_lt7L7cDw","executionInfo":{"status":"ok","timestamp":1711946156119,"user_tz":-540,"elapsed":4,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"6fcf64bd-d59c-410f-b652-25b4302b1950"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n","0    0.772502  0.481144  0.788958  0.073508  0.191594  0.551534  0.947088   \n","1    0.641153  0.398993  0.825552  0.071933  0.196414  0.300026  0.626730   \n","2    0.341044  0.713169  0.524208  0.222758  0.846228  0.818238  0.396072   \n","3    0.309027  0.844079  0.217385  0.014803  0.268508  0.191052  0.508286   \n","4    0.706968  0.578471  0.986250  0.999901  0.869526  0.759983  0.386456   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","995  0.500430  0.394782  0.557026  0.298789  0.485754  0.334745  0.195208   \n","996  0.874752  0.704462  0.687498  0.056676  0.109779  0.812563  0.232951   \n","997  0.775417  0.261860  0.449035  0.151671  0.677930  0.728270  0.361692   \n","998  0.729965  0.968657  0.232322  0.093710  0.263035  0.122862  0.169694   \n","999  0.195616  0.971759  0.810153  0.881471  0.929444  0.753241  0.788780   \n","\n","     feature8  feature9  feature10  \n","0    0.074758  0.043084   0.156551  \n","1    0.685173  0.124203   0.943805  \n","2    0.588608  0.257826   0.689852  \n","3    0.203703  0.763190   0.241371  \n","4    0.753277  0.956676   0.023378  \n","..        ...       ...        ...  \n","995  0.649367  0.673436   0.829232  \n","996  0.355565  0.145297   0.195152  \n","997  0.784747  0.239907   0.691904  \n","998  0.334115  0.413991   0.903724  \n","999  0.437818  0.510450   0.932485  \n","\n","[1000 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-d415ae3f-1847-43ed-aa11-8c423a0b82b8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature1</th>\n","      <th>feature2</th>\n","      <th>feature3</th>\n","      <th>feature4</th>\n","      <th>feature5</th>\n","      <th>feature6</th>\n","      <th>feature7</th>\n","      <th>feature8</th>\n","      <th>feature9</th>\n","      <th>feature10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.772502</td>\n","      <td>0.481144</td>\n","      <td>0.788958</td>\n","      <td>0.073508</td>\n","      <td>0.191594</td>\n","      <td>0.551534</td>\n","      <td>0.947088</td>\n","      <td>0.074758</td>\n","      <td>0.043084</td>\n","      <td>0.156551</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.641153</td>\n","      <td>0.398993</td>\n","      <td>0.825552</td>\n","      <td>0.071933</td>\n","      <td>0.196414</td>\n","      <td>0.300026</td>\n","      <td>0.626730</td>\n","      <td>0.685173</td>\n","      <td>0.124203</td>\n","      <td>0.943805</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.341044</td>\n","      <td>0.713169</td>\n","      <td>0.524208</td>\n","      <td>0.222758</td>\n","      <td>0.846228</td>\n","      <td>0.818238</td>\n","      <td>0.396072</td>\n","      <td>0.588608</td>\n","      <td>0.257826</td>\n","      <td>0.689852</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.309027</td>\n","      <td>0.844079</td>\n","      <td>0.217385</td>\n","      <td>0.014803</td>\n","      <td>0.268508</td>\n","      <td>0.191052</td>\n","      <td>0.508286</td>\n","      <td>0.203703</td>\n","      <td>0.763190</td>\n","      <td>0.241371</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.706968</td>\n","      <td>0.578471</td>\n","      <td>0.986250</td>\n","      <td>0.999901</td>\n","      <td>0.869526</td>\n","      <td>0.759983</td>\n","      <td>0.386456</td>\n","      <td>0.753277</td>\n","      <td>0.956676</td>\n","      <td>0.023378</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>0.500430</td>\n","      <td>0.394782</td>\n","      <td>0.557026</td>\n","      <td>0.298789</td>\n","      <td>0.485754</td>\n","      <td>0.334745</td>\n","      <td>0.195208</td>\n","      <td>0.649367</td>\n","      <td>0.673436</td>\n","      <td>0.829232</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>0.874752</td>\n","      <td>0.704462</td>\n","      <td>0.687498</td>\n","      <td>0.056676</td>\n","      <td>0.109779</td>\n","      <td>0.812563</td>\n","      <td>0.232951</td>\n","      <td>0.355565</td>\n","      <td>0.145297</td>\n","      <td>0.195152</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>0.775417</td>\n","      <td>0.261860</td>\n","      <td>0.449035</td>\n","      <td>0.151671</td>\n","      <td>0.677930</td>\n","      <td>0.728270</td>\n","      <td>0.361692</td>\n","      <td>0.784747</td>\n","      <td>0.239907</td>\n","      <td>0.691904</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>0.729965</td>\n","      <td>0.968657</td>\n","      <td>0.232322</td>\n","      <td>0.093710</td>\n","      <td>0.263035</td>\n","      <td>0.122862</td>\n","      <td>0.169694</td>\n","      <td>0.334115</td>\n","      <td>0.413991</td>\n","      <td>0.903724</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>0.195616</td>\n","      <td>0.971759</td>\n","      <td>0.810153</td>\n","      <td>0.881471</td>\n","      <td>0.929444</td>\n","      <td>0.753241</td>\n","      <td>0.788780</td>\n","      <td>0.437818</td>\n","      <td>0.510450</td>\n","      <td>0.932485</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 10 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d415ae3f-1847-43ed-aa11-8c423a0b82b8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d415ae3f-1847-43ed-aa11-8c423a0b82b8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d415ae3f-1847-43ed-aa11-8c423a0b82b8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-75250445-fb5a-41f5-b356-686115295f3a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75250445-fb5a-41f5-b356-686115295f3a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-75250445-fb5a-41f5-b356-686115295f3a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_76d660f8-5354-45c2-b4c7-bc579ce34ffd\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('features')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_76d660f8-5354-45c2-b4c7-bc579ce34ffd button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('features');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"features","summary":"{\n  \"name\": \"features\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28328026097687625,\n        \"min\": 0.00012513875657549356,\n        \"max\": 0.9989337371045391,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.7874257393381833,\n          0.2875595650906989,\n          0.4383038804020907\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28830651868153045,\n        \"min\": 6.749764657443258e-05,\n        \"max\": 0.9997320842956724,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.44711140748178246,\n          0.2745646772243131,\n          0.5633312538407954\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27873388863965365,\n        \"min\": 0.0003070750337554884,\n        \"max\": 0.9995983822000198,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.32323357706785427,\n          0.22238158017909282,\n          0.29438035887794045\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2948428193168789,\n        \"min\": 0.00024529051199306817,\n        \"max\": 0.9999009770092316,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.01354432392440208,\n          0.33252835975457073,\n          0.3025220925138664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28592528855985067,\n        \"min\": 0.001447220520999326,\n        \"max\": 0.9973824938253695,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.7971050223340935,\n          0.3531330751434124,\n          0.4244765018489495\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2934815645956547,\n        \"min\": 4.8123894311746795e-05,\n        \"max\": 0.9987804057487236,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.32793865903598807,\n          0.3087757763619975,\n          0.314145615150606\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28121585053780246,\n        \"min\": 0.0014649930691902346,\n        \"max\": 0.9996956093547604,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.3573094694080502,\n          0.978752715719509,\n          0.9153315835685167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2928483428850217,\n        \"min\": 0.0016524881419711646,\n        \"max\": 0.9991370762165098,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.33600616309881226,\n          0.9327310855094871,\n          0.04195457129062896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28611714584352466,\n        \"min\": 0.00011023180182245795,\n        \"max\": 0.9995087956637745,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.4987393308123381,\n          0.5745100995248622,\n          0.23457015987027463\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2871148375662239,\n        \"min\": 0.0014503165911271543,\n        \"max\": 0.9981301159238253,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.7024995732190771,\n          0.9810548384103098,\n          0.8085527962274213\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["consumption[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VkfXDRJ7cBJ","executionInfo":{"status":"ok","timestamp":1711946172472,"user_tz":-540,"elapsed":8,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"59e61307-2635-498b-8858-e650c10d55c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.06933425,  0.46952594,  0.35327824,  0.15068412, -0.01577216,\n","        0.17310564,  0.32834341,  0.18863397,  0.32956524,  0.29764606,\n","        0.27623026,  0.30566959,  0.36660977,  0.00204014,  0.14995074,\n","        0.02140257,  0.2976937 ,  0.15681663,  0.47826454,  0.03816285])"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["### Data Preprocessing\n","For LSTM models, we need to format our data into sequences. We'll also split the data into training and testing sets."],"metadata":{"id":"juzMJkm-8d4c"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader"],"metadata":{"id":"g8qeD1NP8dla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# suqenceデータの作成\n","# twを5にすると、3次元のデータ構造で、x方向に10個(特徴量数), y方向に5個(時系列数), z方向に995個（len(data_normalized) - 5)の\n","# データが作られる。これは、LSTM用に、各yに対して5時点分のsequenceデータを用意している作業\n","\n","def create_sequnces(featurs, targets, time_steps=1):\n","    Xs, ys = [], []\n","    for i in range(len(features) - time_steps):\n","        Xs.append(features[i:(i+time_steps)])\n","        ys.append(targets[i+time_steps])\n","    return np.array(Xs), np.array(ys)\n","\n","# Normalize data\n","# ※データをtraintとtestに分割する前に標準化を適用しているので、データリーク生じているので注意！！！！！！\n","scaler = MinMaxScaler()\n","features_scaled = scaler.fit_transform(features)\n","\n","# Create sequences\n","time_steps = 5\n","X, y = create_sequnces(features_scaled, consumption, time_steps)"],"metadata":{"id":"5acq2Jjc8diS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1iQDx3l68dfq","executionInfo":{"status":"ok","timestamp":1711948610252,"user_tz":-540,"elapsed":296,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7a3e6858-f825-487c-ed8e-cb37eac6e30e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(995, 5, 10)"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtZkLfeL8ddU","executionInfo":{"status":"ok","timestamp":1711948611943,"user_tz":-540,"elapsed":271,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"c239584b-24e4-472c-ad2f-c21ff0938e80"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(995,)"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Convert to Pytorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n","\n","# Create TensorDatasets and DataLoaders\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"RlHF56RY8da6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Define the LSTM Model\n","Here's a simple LSTM model in PyTorch. The model takes sequences of data with 10 features and outputs a prediction for the future electricity consumption."],"metadata":{"id":"merXDpOG_M4g"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ElectricityConsumptionLSTM(nn.Module):\n","    def __init__(self, input_size=10, hidden_layer_size=100, output_size=1):\n","        super(ElectricityConsumptionLSTM, self).__init__()\n","        self.hidden_layer_size = hidden_layer_size\n","        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n","        self.linear = nn.Linear(hidden_layer_size, output_size)\n","\n","    def forward(self, input_seq):\n","        # Get batch size\n","        batch_size = input_seq.size(0)\n","\n","        # 最初のhidden_stateの値設定\n","        # h0 = torch.zeros(1, input_seq.size(0), self.hidden_layer_size)\n","        # c0 = torch.zeros(1, input_seq.size(0), self.hidden_layer_size)\n","        h0 = torch.zeros(1, batch_size, self.hidden_layer_size)\n","        c0 = torch.zeros(1, batch_size, self.hidden_layer_size)\n","\n","        # lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1), (h0, c0))\n","        lstm_out, _ = self.lstm(input_seq.transpose(0, 1), (h0, c0))\n","        # predictions = self.linear(lstm_out.view(len(input_seq), -1))\n","        predictions = self.linear(lstm_out[-1])  # Take the last output from the sequence\n","\n","        return predictions.squeeze()  # Squeeze to remove any unnecessary dimensions\n","\n","# Instantiate the model, define the loss function and the optimizer\n","model = ElectricityConsumptionLSTM(input_size=10)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"ge81tn1e8dYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" hidden_layer_size=100\n"," h0 = torch.zeros(1, batch_size, hidden_layer_size)\n"," h0.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obGgw27kHkaP","executionInfo":{"status":"ok","timestamp":1711949170604,"user_tz":-540,"elapsed":9,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"238588ef-5478-4c65-82a0-b055f9d20207"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 64, 100])"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["tmp_data = next(iter(train_loader))\n","tmp_data[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H_Yr3dvI4-d","executionInfo":{"status":"ok","timestamp":1711949455043,"user_tz":-540,"elapsed":274,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"845e474a-2ebe-4326-cd37-2f400463b7c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 5, 10])"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["tmp_data[0].transpose(0, 1).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0I7bVVCYI7mF","executionInfo":{"status":"ok","timestamp":1711949464379,"user_tz":-540,"elapsed":258,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"1ca5cdf1-d8a3-4d55-f39c-cb71bc690840"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 64, 10])"]},"metadata":{},"execution_count":94}]},{"cell_type":"markdown","source":["You can use `torch.transpose()` to rearrange the dimensions of a tensor according to your specific needs, such as converting a batched sequence tensor from **(batch_size, seq_len, input_size)** to **(seq_len, batch_size, input_size)** for compatibility with certain PyTorch modules like LSTM, as we did in the previous example."],"metadata":{"id":"YoNCthGHKZGU"}},{"cell_type":"code","source":["model = ElectricityConsumptionLSTM(input_size=10)\n","lstm_out, _ = model.lstm(tmp_data[0].transpose(0, 1))"],"metadata":{"id":"D0u92E9VJHEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm_out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u51lAqPjJTi4","executionInfo":{"status":"ok","timestamp":1711949564883,"user_tz":-540,"elapsed":9,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"5dace418-bdbf-4b09-cecf-1570792f03e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 64, 100])"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["lstm_out[-1].shape # -> 5つのシーケンスのうちの最後のデータが予測値ということ？"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHFTI5xNJarB","executionInfo":{"status":"ok","timestamp":1711949606296,"user_tz":-540,"elapsed":275,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"d4424f4a-3a88-4226-8903-e1945db7a114"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 100])"]},"metadata":{},"execution_count":97}]},{"cell_type":"markdown","source":["In this modification:\n","\n","* We use `input_seq.transpose(0, 1)` to transpose the input sequence tensor so that the batch size becomes the first dimension. This ensures that the input tensor has the shape `(seq_len, batch_size, input_size)`, which is compatible with the expected input shape for the LSTM layer.\n","* We take only the last output from the LSTM sequence `(lstm_out[-1])` since we're interested in predicting the next value based on the entire sequence."],"metadata":{"id":"oobRYFs-FVm4"}},{"cell_type":"markdown","source":["## Step3: Training the Model"],"metadata":{"id":"MkzCnyBMCmeq"}},{"cell_type":"code","source":["epochs = 5\n","\n","for epoch in range(epochs):\n","    for inputs, targets in train_loader:\n","        optimizer.zero_grad()\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-JbsRyD7b-R","executionInfo":{"status":"ok","timestamp":1711948622334,"user_tz":-540,"elapsed":727,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7e4dfddc-6418-4b15-959b-b53977159cf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.0259\n","Epoch 2, Loss: 0.0212\n","Epoch 3, Loss: 0.0211\n","Epoch 4, Loss: 0.0148\n","Epoch 5, Loss: 0.0230\n"]}]},{"cell_type":"markdown","source":["## Step6: Evaluating the Model\n","After training, you should evaluate the model's performance on the test set. This basic example doesn't include evaluation steps, but you would typically predict on the test set and compare it against the true values using a suitable metric (e.g., MSE for regression tasks)."],"metadata":{"id":"nechAds4F_Ya"}},{"cell_type":"code","source":["with torch.no_grad():\n","    predictions = []\n","    for inputs, _ in test_loader:\n","        predictions.append(model(inputs).numpy())\n","\n","# flatten the list of predictions\n","predictions = np.concatenate(predictions, axis=0)"],"metadata":{"id":"ElwmMcL-kR4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = criterion(torch.tensor(predictions), torch.tensor(y_test))\n","print(f\"Test Loss: {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOz3g0pjGVMx","executionInfo":{"status":"ok","timestamp":1711948999353,"user_tz":-540,"elapsed":268,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"8c8e7dd5-b3de-498c-b60f-9458d11c12aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.0211\n"]}]},{"cell_type":"code","source":["len(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vICCn_T9GTjq","executionInfo":{"status":"ok","timestamp":1711948922047,"user_tz":-540,"elapsed":8,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"f55e3d49-3361-4932-d9f7-c4abb15e982b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["199"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["len(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPNO-OLIGfES","executionInfo":{"status":"ok","timestamp":1711948946966,"user_tz":-540,"elapsed":8,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"2bff6853-25e6-4d23-ebb7-1482c6cc8627"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["199"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["### tips: about batch_first=True"],"metadata":{"id":"Yc6ezIJ-V6M2"}},{"cell_type":"markdown","source":["In PyTorch's LSTM (and generally in RNN-based modules), the **batch_first=True** argument is important for defining how the input data should be structured. By default, PyTorch expects the input tensor to the LSTM layer to have the shape **(seq_len, batch, features)**, where:\n","\n","* **seq_len** is the length of the sequence,\n","* **batch** is the batch size, and\n","* **features** is the number of features per time step.\n","\n","<br><br>\n","When you set **batch_first=True**, it changes the expected input shape to **(batch, seq_len, features)**, which is a format more familiar to those who work with other types of neural networks (like CNNs) and might be more intuitive depending on how you preprocess or think about your data.\n","<br><br>\n","You should add **batch_first=True** to the LSTM initialization in the model definition if your input data is formatted with the batch size as the first dimension. Given the way we structured the input data in our example (especially in the DataLoader), you would typically want to set **batch_first=True** to align with the **(batch, seq_len, features)** format.\n","<br><br>\n","Here's a revised snippet of the LSTM model definition with **batch_first=True**:"],"metadata":{"id":"_9-boqwnWAbt"}},{"cell_type":"code","source":["# batch_first=Trueの場合のコード\n","class ElectricityConsumptionLSTM(nn.Module):\n","    def __init__(self, input_size=10, hidden_layer_size=100, output_size=1):\n","        super().__init__()\n","        self.hidden_layer_size = hidden_layer_size\n","\n","        # Notice the batch_first=True here\n","        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n","\n","        self.linear = nn.Linear(hidden_layer_size, output_size)\n","\n","    def forward(self, input_seq):\n","        lstm_out, _ = self.lstm(input_seq)  # No need to reshape the input_seq here\n","        predictions = self.linear(lstm_out[:, -1, :])  # Adjusted for batch_first=True\n","        return predictions\n"],"metadata":{"id":"oWPby7AAGbgq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With **batch_first=True**, note how the input to the LSTM no longer needs to be reshaped in the forward pass, and when accessing the output from the LSTM to pass to the linear layer, we use **lstm_out[:, -1, :]** to get the last time step's output for all elements in the batch. This change simplifies handling sequence data when your batches are the first dimension of the tensor."],"metadata":{"id":"aj5HvI3QW-Yv"}},{"cell_type":"code","source":[],"metadata":{"id":"4gmwIoDDW5fZ"},"execution_count":null,"outputs":[]}]}