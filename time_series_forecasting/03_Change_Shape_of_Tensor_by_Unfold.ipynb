{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOyqvKAuVzWYgyCCjXjWSJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# tensor.unfold()\n","The **unfold()** function in PyTorch is a powerful method for extracting sliding windows (also known as \"rolling\" or \"moving\" windows) form a tensor. this can be especially useful in tasks involving sequences where you need to generate overlapping sub-tensors from a larger tensor. Here, I'll explain how to use **unfold()** with a detailed example relevant to your need.\n","\n","\n"],"metadata":{"id":"cEe9UpyxH6nw"}},{"cell_type":"markdown","source":["## Basic Syntax of `unfold()`\n","**unfold()** takes three arguments:\n","* **dimension**: The dimension along which to unfold.\n","* **size**: The size of each sub-tensor to be extracted.\n","* **step**: The step between starts of each sub-tensor."],"metadata":{"id":"3s2UoYErJnUN"}},{"cell_type":"markdown","source":["## Example: Creating Overlapping Windows from a Time Series\n","Suppose you have a time series of data points, and you want to prepare it for an LSTM model, where each input should be a sequence of consecutive data points."],"metadata":{"id":"2BfGrzKFJ8mD"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AugAhf_nHt9j","executionInfo":{"status":"ok","timestamp":1713862495372,"user_tz":-540,"elapsed":4760,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7d6d5c91-707d-4f31-b26f-7e4aee6cc3f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original data: \n","tensor([[ 1.,  2.,  3.],\n","        [ 4.,  5.,  6.],\n","        [ 7.,  8.,  9.],\n","        [10., 11., 12.],\n","        [13., 14., 15.],\n","        [16., 17., 18.],\n","        [19., 20., 21.],\n","        [22., 23., 24.],\n","        [25., 26., 27.],\n","        [28., 29., 30.]])\n"]}],"source":["import torch\n","\n","# Sample data: A time series of 10 data points (let's assume each data point has 3 features).\n","data = torch.arange(1, 31).reshape(10, 3).float() # A (10 * 3) tensor\n","print(\"Original data: \")\n","print(data)"]},{"cell_type":"code","source":["# Using unfold to create overlapping sequences\n","sequence_length = 4  # The length of each sub-tensor\n","step = 1  # The step between starts of each sub-tensor\n","\n","# Unfold the data\n","# We unfold along the first dimension (time dimension), which has size 10 in this case.\n","unfolded_data = data.unfold(0, sequence_length, step)\n","\n","# Print the unfolded data\n","print(\"\\nUnfolded data: \")\n","print(unfolded_data)\n","print(\"Shape of unfolded data: \", unfolded_data.shape)\n","\n","# The result will be a tensor where each slice is a sequence of length 4\n","# and the shape of unfolded_data is [7, 4, 3]:\n","# - 7 sequences\n","# - 4 is the sequence length\n","# - 3 features per time point"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GdlMOAcKjeA","executionInfo":{"status":"ok","timestamp":1713862674855,"user_tz":-540,"elapsed":7,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"6bfdc7c0-bacb-4130-99a6-496b9a2bc731"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Unfolded data: \n","tensor([[[ 1.,  4.,  7., 10.],\n","         [ 2.,  5.,  8., 11.],\n","         [ 3.,  6.,  9., 12.]],\n","\n","        [[ 4.,  7., 10., 13.],\n","         [ 5.,  8., 11., 14.],\n","         [ 6.,  9., 12., 15.]],\n","\n","        [[ 7., 10., 13., 16.],\n","         [ 8., 11., 14., 17.],\n","         [ 9., 12., 15., 18.]],\n","\n","        [[10., 13., 16., 19.],\n","         [11., 14., 17., 20.],\n","         [12., 15., 18., 21.]],\n","\n","        [[13., 16., 19., 22.],\n","         [14., 17., 20., 23.],\n","         [15., 18., 21., 24.]],\n","\n","        [[16., 19., 22., 25.],\n","         [17., 20., 23., 26.],\n","         [18., 21., 24., 27.]],\n","\n","        [[19., 22., 25., 28.],\n","         [20., 23., 26., 29.],\n","         [21., 24., 27., 30.]]])\n","Shape of unfolded data:  torch.Size([7, 3, 4])\n"]}]},{"cell_type":"markdown","source":["**Explanation**\n","* **Initial Data**: The **data** tensor is a (10 * 3) tensor, simulating a dataset with 10 time points and 3 features at each time point.\n","* **Unfolding**: We apply **unfold()** to create a new dimension for the sequences. By specifying **dimension=0** (the first dimension, which has length 10), **size=4** (each window contains 4 time points), and **step=1** (the window moves one time point at a time), we generate overlapping windows of the data.\n","* **Shape of Unfolded Data**: the output shape **[7, 4, 3]** means there are 7 windows, each containing 4 time points, and each time points retains its 3 features.\n","\n","\n","This method is particularly useful for preparing batches of sequential data for time series forecasting or any other sequence processing tasks in machine learning, where the model (like an LSTM) expects inputs in sequences of fixed length. By using **unfold()**, you can efficiently create these sequences directly from your data tensors in PyTorch without needing for-loops or manual slicing, which can be error-prone and  less efficient."],"metadata":{"id":"2VNWtVEILZ0f"}},{"cell_type":"code","source":[],"metadata":{"id":"TCfnKAlrLQN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# example of using unfold\n","```\n","# create input tensor\n","input_data = torch.tensor(test_data_add_train_sequence.values, dtype=torch.float32)\n","input_sequences = input_data.unfold(dimension=0, size=sequence_length, step=1)\n","\n","# transpose input_sequences to match the expected shape of LSTM (batch_size, sequence_length, num_features)\n","input_sequences = input_sequences.transpose(dim0=1, dim1=2)\n","```\n","\n","the method you found using **torch.unfold** is an excellent and efficient approach to creating input sequences for your LSTM model directly in PyTorch. This method leverages PyTorch's built-in functionality to efficiently generate sliding windows (or sequences) from your dataset. Let's review the code and explain each step:"],"metadata":{"id":"73qpvHsjNn-R"}},{"cell_type":"markdown","source":["## Explanation of the Code:\n","\n","1. **Create Input Tensor:**\n","   - `input_data = torch.tensor(test_data_add_train_sequence.values, dtype=torch.float32)`\n","   - This line converts the DataFrame (`test_data_add_train_sequence`) into a PyTorch tensor. The data from the DataFrame is accessed via `.values`, which returns a numpy array that is then converted to a tensor.\n","\n","2. **Unfolding to Create Sequences:**\n","   - `input_sequences = input_data.unfold(dimension=0, size=sequence_length, step=1)`\n","   - The `unfold` function is used here to create overlapping sequences. The `dimension=0` argument specifies that the unfolding should occur along the first dimension (time series data points in this case). The `size=sequence_length` sets the size of each window (50 in your case), and `step=1` ensures that the window slides one timestep at a time, creating overlapping sequences.\n","\n","3. **Transposing the Tensor:**\n","   - `input_sequences = input_sequences.transpose(dim0=1, dim1=2)`\n","   - The output from `unfold` will have a shape of `[number of sequences, num_features, sequence_length]`. However, LSTM models in PyTorch expect the input shape to be `[batch_size, sequence_length, num_features]`, so transposing dimensions 1 and 2 corrects the shape to match the expected input format of the LSTM.\n","\n","## Completeness of the Code:\n","\n","This code snippet is well-suited for your needs, assuming that the original DataFrame (`test_data_add_train_sequence`) is correctly prepared with the expected features and indices. It's a more concise and potentially faster method than manually looping through indices or using list comprehensions due to its utilization of native PyTorch operations which are highly optimized.\n","\n","Here is how you can use this snippet in context:\n","\n"],"metadata":{"id":"fqZSIFB6OIKl"}},{"cell_type":"code","source":["# Assuming test_data_add_train_sequence is your DataFrame prepared with the correct data\n","input_data = torch.tensor(test_data_add_train_sequence.values, dtype=torch.float32)\n","input_sequences = input_data.unfold(dimension=0, size=50, step=1)\n","input_sequences = input_sequences.transpose(dim0=1, dim1=2)\n","\n","# You can now feed `input_sequences` to your LSTM model\n","with torch.no_grad():\n","    lstm_model.eval()  # Ensure the model is in evaluation mode\n","    predictions = lstm_model(input_sequences)\n","    # Handle predictions as required"],"metadata":{"id":"hgylFyRnN4wX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This method is efficient and should work seamlessly for batch processing with your LSTM model."],"metadata":{"id":"mMvHsYFCOU32"}},{"cell_type":"markdown","source":["# if don't use unfold() then following code\n","\n","Certainly! You can accomplish the same sequence preparation directly in PyTorch, which can be more efficient especially if you're dealing with large datasets. This approach leverages PyTorch's tensor manipulation capabilities to create overlapping sequences without explicitly looping over the dataset.\n","\n","Here's how you can rewrite the `prepare_sequences` function using PyTorch:\n","\n","```python\n","import torch\n","import pandas as pd\n","\n","def prepare_sequences_torch(data, sequence_length=50):\n","    # Convert DataFrame to PyTorch tensor\n","    data_tensor = torch.tensor(data.to_numpy(), dtype=torch.float32)\n","    \n","    # Number of sequences\n","    num_sequences = data_tensor.size(0) - sequence_length + 1\n","    \n","    # Create an array of indices for starting points of sequences\n","    start_indices = torch.arange(0, num_sequences)\n","    \n","    # Use indexing to create the sequences\n","    sequences = torch.stack([data_tensor[i:i+sequence_length] for i in start_indices])\n","    \n","    return sequences\n","\n","# Usage example:\n","dates = pd.date_range(start=\"2022-01-01\", periods=200, freq='B')  # Business days\n","prices = torch.randn(200, 8) * 100  # Simulated prices\n","data = pd.DataFrame(prices.numpy(), columns=['feat1', 'feat2', 'feat3', 'feat4', 'feat5', 'feat6', 'feat7', 'feat8'], index=dates)\n","\n","# Convert data to sequences\n","test_sequences = prepare_sequences_torch(data, sequence_length=50)\n","print(\"Shape of test_sequences:\", test_sequences.shape)\n","```\n","\n","### Explanation\n","\n","1. **Tensor Conversion:** First, the function converts the input DataFrame to a PyTorch tensor. This allows you to leverage PyTorch's fast tensor manipulation methods.\n","\n","2. **Sequence Generation:**\n","   - We calculate the number of possible sequences as `num_sequences`.\n","   - We then create an array of starting indices for these sequences using `torch.arange`.\n","   - For each starting index, we slice the `data_tensor` to get a sequence of length `sequence_length` and stack all such sequences using `torch.stack`.\n","\n","This method avoids the explicit Python loop for creating sequences by leveraging PyTorch operations, which should generally be faster especially on large datasets."],"metadata":{"id":"tupQhZIKOcdk"}},{"cell_type":"code","source":[],"metadata":{"id":"uCAQy3HiOVUc"},"execution_count":null,"outputs":[]}]}