{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPq7wSQcxEh1Oh7QfM/CTYE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sum from [64, 50 ,4] to [64, 50]\n","Q: I have torch.tensor shaped ([0th, 1st, 2nd] = [64, 50, 4]. And I want to summalize each records of 2nd dimension into [64, 50]. Could you teach me how to do that?\n","\n","A: To sum along the 2nd dimension of a tensor in PyTorch, you can use the **torch.sum** function, specifying the dimension over which you want to perform the summation. In your case, you want to sum over the 2nd dimension (dimension index 2) of a tensor with the shape **[64, 50, 4]**, resulting in a new tensor of shape **[64, 50]**.\n","\n","Here's how you can do it:\n"],"metadata":{"id":"O5k0OMC_Ujb3"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"6y5jTMvhUYbc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713596950706,"user_tz":-540,"elapsed":10,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"41b220ca-b7e7-4d66-fdf2-7db90cfb6e93"},"outputs":[{"output_type":"stream","name":"stdout","text":["original tensor shape:  torch.Size([64, 50, 4])\n","\n","Resultant tensor: \n","tensor([[-1.4522,  2.6385,  1.0691,  ...,  0.9187, -2.2074,  6.4091],\n","        [ 2.3085, -2.6075,  0.9538,  ..., -2.0158, -5.5409, -1.4783],\n","        [ 0.2099,  2.6535, -4.2383,  ..., -1.2818, -0.2977,  0.8539],\n","        ...,\n","        [ 0.2493,  1.9232, -1.2999,  ..., -1.1626, -0.5215,  1.0411],\n","        [ 1.2811, -1.4719, -1.9279,  ..., -0.6036, -0.6796, -1.2994],\n","        [-1.7605,  0.2696, -0.0853,  ...,  1.4801,  1.0746, -0.7454]])\n","Shape of the resultanlt tensor torch.Size([64, 50])\n"]}],"source":["import torch\n","\n","# Create a tensor of shape [64, 50, 4]\n","x = torch.randn(64, 50, 4) # Random data for example purpose\n","print(\"original tensor shape: \", x.shape)\n","print()\n","\n","# Sum over the 2nd dimension (dimension index is 2)\n","result = torch.sum(x, dim=2)\n","\n","print(\"Resultant tensor: \")\n","print(result)\n","print(\"Shape of the resultanlt tensor\", result.shape)"]},{"cell_type":"markdown","source":["**Explanation**\n","* **torch.randn(64, 50, 4)**: This function generates a tensor filled with random numbers from a standard normal distribution. Here, it creates a tensor of shape **[64, 50, 4]**.\n","* **torch.sum(x, dim=2)**: This function sums all the values along the specified dimension (2nd dimension in this case). Since the tensor shape is **[64, 50, 4]**, summing along the 2nd dimension collapses this dimension, resulting in a new tensor of shape **[64, 50]**.\n","\n","\n","This operation is very common in data processing, especially when you need to aggregate data along a specific axis or dimension for further analysis or as part of a preprocessing step in machine leaning workflows."],"metadata":{"id":"kI4hej04Vs8P"}},{"cell_type":"markdown","source":["# Sum from [64, 50 ,4] to [64, 4]\n","Q: If I want to sum from [64, 50, 4] to [64, 4], then I run torch.sum(x, dim=1). Is that right?\n","\n","A: Yes, that's correct! If you want to sum the tensor with shape **[64, 50, 4]** along the 1st dimension (the middle dimension, which has index 1), resulting in a tensor of shape **[64, 4]**, you would use the **torch.sum** function and specify **dim=1**. This operation will sum across the 50 elements of each **[50, 4]** slice within the tensor, leaving you with summed values along each of the 4 columns for each of the 64 rows.\n","\n","\n","Here's how you can do it:"],"metadata":{"id":"FFF2RC-fWgGQ"}},{"cell_type":"code","source":["import torch\n","\n","# Create a tensor of shape [64, 50, 4]\n","x = torch.randn(64, 50, 4)  # Random data for example purposes\n","\n","# Sum over the 1st dimension (dimension index is 1)\n","result = torch.sum(x, dim=1)\n","\n","# Print the result and check the shape\n","print(\"Resultant tensor:\")\n","print(result)\n","print(\"Shape of the resultant tensor:\", result.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgYadpRnVcPG","executionInfo":{"status":"ok","timestamp":1713597380720,"user_tz":-540,"elapsed":11,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"3d073968-8ad3-4d78-8933-67aa18fdf89c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Resultant tensor:\n","tensor([[ 6.5989e+00, -7.5399e+00, -1.1997e+01, -3.8994e+00],\n","        [-9.1334e+00, -3.7948e+00,  4.1790e+00, -1.3990e+01],\n","        [ 1.1495e+01, -1.7465e+00, -8.8008e+00,  5.8538e-01],\n","        [ 4.9833e+00,  5.6684e+00,  5.3880e+00,  4.9267e+00],\n","        [ 7.9842e+00,  1.0287e+01,  1.2935e+01,  5.0620e+00],\n","        [-1.2257e+00,  1.4610e+00, -3.5248e+00, -3.2163e+00],\n","        [-7.8314e+00, -4.9129e+00, -5.2432e+00, -4.3035e-02],\n","        [ 7.2529e+00,  1.9609e+00, -2.3003e+00, -8.1904e+00],\n","        [-1.7504e+00, -8.2595e-01,  2.6410e+00,  1.0226e+01],\n","        [ 1.6158e+00, -7.4794e+00, -3.6662e+00, -3.4958e+00],\n","        [ 6.8873e+00, -2.4567e+00,  5.0042e+00,  1.7813e+00],\n","        [-3.1620e+00,  1.6397e-01, -8.9514e+00, -2.5724e+00],\n","        [-1.7892e+01, -1.5154e+01,  6.5249e+00, -7.7931e+00],\n","        [-1.3645e+01,  2.4476e+00,  3.6529e+00,  3.7774e+00],\n","        [-3.9983e+00, -3.5449e+00,  1.5751e+01, -1.1885e-01],\n","        [-5.4638e+00,  1.1790e+01, -1.5542e+00,  6.6032e+00],\n","        [ 1.4228e-01,  7.3419e+00,  1.0857e+01, -1.0386e+01],\n","        [-6.4622e+00, -6.6462e+00,  2.8362e+00, -6.1458e-01],\n","        [ 1.2993e+01, -1.0042e+00,  3.4051e+00, -1.9876e+00],\n","        [-1.5515e+00,  1.8767e-02,  5.8177e+00, -2.4447e+00],\n","        [-4.4132e-01, -3.7844e+00, -4.6338e+00,  1.5266e+00],\n","        [ 1.1338e+01,  4.1761e+00, -3.0902e-01, -2.4099e+00],\n","        [ 7.3338e-01,  3.0361e+00,  3.4072e+00,  5.4181e+00],\n","        [ 1.1088e+01,  5.7157e+00,  1.4208e+00, -5.0549e+00],\n","        [ 1.0088e+01,  6.7192e+00, -1.3369e+00,  1.0771e+01],\n","        [-1.6991e+01,  6.5776e+00,  7.8828e+00,  1.7925e+00],\n","        [ 3.3756e+00,  2.9006e+00,  8.8861e-01, -1.0810e+01],\n","        [-1.0786e+01,  6.6057e+00,  4.9896e+00,  7.1809e+00],\n","        [ 2.2774e+00,  1.1577e+01, -2.6912e+00,  2.2030e+00],\n","        [-9.7557e+00,  7.2330e-01,  2.7650e+00,  3.2237e+00],\n","        [ 9.4411e-01, -7.3253e+00,  5.2211e-01, -2.5965e+00],\n","        [ 3.7450e-01,  1.5836e+00,  2.7939e+00, -6.9357e+00],\n","        [ 4.7855e+00, -1.0270e+01, -2.7300e+00, -6.8583e+00],\n","        [ 1.0487e+00,  3.7757e+00, -9.8066e+00, -7.4081e-01],\n","        [ 1.3753e+01, -1.7185e+00, -3.1299e+00,  1.5371e+00],\n","        [-8.5229e+00,  1.7411e+01, -9.9326e+00,  1.3377e+01],\n","        [-3.3469e-01,  1.0167e+01,  6.7530e+00,  3.8102e+00],\n","        [-3.5138e+00,  6.1906e+00,  8.4557e+00, -7.2845e+00],\n","        [ 1.8947e+00, -1.4308e+01,  1.1835e+01, -8.7420e+00],\n","        [ 1.2337e+00, -3.4588e+00, -1.6290e+00,  2.3534e+00],\n","        [-1.9569e+00, -2.3998e+00, -1.0280e+00, -4.1100e+00],\n","        [-3.8765e+00, -9.4429e+00, -1.4233e+01, -3.0589e+00],\n","        [-1.0922e+00,  3.8265e+00, -1.1018e+01,  7.5062e+00],\n","        [ 2.3371e+00,  8.6919e+00,  1.7376e+01,  2.0393e+00],\n","        [-6.2779e-01, -6.4112e+00, -6.2008e-01,  3.4366e+00],\n","        [-3.1100e+00, -9.6798e+00,  3.4834e+00,  2.6163e+00],\n","        [ 6.3284e+00, -1.5896e+00,  4.5805e+00, -4.0121e+00],\n","        [ 3.8264e+00,  3.4791e+00, -7.2168e+00, -5.4433e+00],\n","        [-1.4453e+01,  3.2375e+00,  1.0198e+01, -1.3969e+00],\n","        [ 9.3342e+00, -5.5082e+00,  2.4836e+01, -3.7275e+00],\n","        [-1.5683e-01, -1.6955e+00, -3.7125e+00, -3.5015e+00],\n","        [ 7.0615e+00, -1.2673e-01, -7.0008e+00, -1.2728e+01],\n","        [ 1.9080e+00, -2.6870e+00,  8.2411e+00,  1.1672e+01],\n","        [-1.7309e+01, -7.8863e+00, -3.8993e+00,  1.9137e+00],\n","        [ 3.3078e+00,  8.0697e+00, -3.3501e+00, -3.4077e+00],\n","        [-1.9749e+00, -6.5908e+00,  1.4828e+01,  1.3368e+00],\n","        [ 4.7524e+00,  6.8698e+00,  1.0537e+01,  7.9076e+00],\n","        [-5.1729e+00,  3.9212e+00, -1.7854e+00,  3.7002e+00],\n","        [-7.3338e+00,  2.5938e+00, -2.1668e+00, -2.9268e+00],\n","        [ 7.8448e+00, -6.6053e+00, -5.4119e+00, -9.8042e+00],\n","        [ 1.1470e+01,  1.2860e+01,  1.8590e+00,  6.7595e-01],\n","        [-4.7002e+00,  1.3007e+00, -2.0985e+00,  2.8175e+00],\n","        [-1.1449e+00,  1.5156e+00, -3.5245e+00, -1.0832e+01],\n","        [-9.6599e-01,  2.5133e+00,  9.5925e-01, -9.7781e+00]])\n","Shape of the resultant tensor: torch.Size([64, 4])\n"]}]},{"cell_type":"markdown","source":["## What This Does:\n","* **torch.randn(64, 50, 4)**: This creates a tensor filled with random floats. These numbers are normally distributed, making this a good example for statistical operations.\n","* **torch.sum(x, dim=1)**: This sums the tensor **x** along the 1st dimension. Each **[50, 4]** block within the tensor is reduced to a single **[4}** vector by summing up all the 50 elements in each of the 4 columns across the 50 rows. This transform the entire tensor shape from **[64, 50, 4]** to **[64, 4]**.\n","\n","\n","\n","This operatio effectively aggregates the data in the middle dimension, which is often useful in scenarios where you need to condense a set of measurements or features into a simpler form for each sample or data point."],"metadata":{"id":"Awbkbb9CXVKW"}},{"cell_type":"code","source":[],"metadata":{"id":"vxsa9K9IXPXk"},"execution_count":null,"outputs":[]}]}