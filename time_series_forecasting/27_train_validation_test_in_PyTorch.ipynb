{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1DM8_j7r1bjX97iVn0an8zBjSy5xDfTzl","authorship_tag":"ABX9TyOMcOCYrGMuY2M/QqHuQ5lO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# train, validation, test in PyTorch"],"metadata":{"id":"L-JTI1wo3jnv"}},{"cell_type":"markdown","source":["# v1. from blog\n","https://blog.paperspace.com/training-validation-and-accuracy-in-pytorch/\n","\n","When it comes to Neural Networks it becomes essential to set optimal architecture and hyper parameters. While training a neural network the training loss always keeps reducing provided the learning rate is optimal. But it's important that our network performs better not only on data it's trained on but also data that it has never seen before. One way to measure this is by introducing a validation set to keep track of the testing accuracy of the neural network. In this article we'll how we can keep track of validation accuracy at each training step and also save the model weights with the best validation accuracy."],"metadata":{"id":"X_3ZFLJL3qUx"}},{"cell_type":"markdown","source":["## import data"],"metadata":{"id":"-54Iafxj9EWJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00zF_xit1KT-","executionInfo":{"status":"ok","timestamp":1712802053515,"user_tz":-540,"elapsed":14298,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"47e66ab4-5e90-4c61-b76c-efdae73fce8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 79766759.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 18921539.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 30194861.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 4293560.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n","\n"]}],"source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","\n","transforms = transforms.Compose([\n","                                 transforms.ToTensor()\n","])\n","\n","train = datasets.MNIST('', train = True, transform = transforms, download = True)\n","train, valid = random_split(train,[50000,10000])\n","\n","trainloader = DataLoader(train, batch_size=32)\n","validloader = DataLoader(valid, batch_size=32)"]},{"cell_type":"markdown","source":["## Building Model"],"metadata":{"id":"9T7schJk9S3s"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","class Network(nn.Module):\n","    def __init__(self):\n","        super(Network,self).__init__()\n","        self.fc1 = nn.Linear(28*28, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = x.view(1,-1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","model = Network()\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()"],"metadata":{"id":"CMNZhGC39WHf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the above code, we defined a neural network with the following architecture:-\n","\n","* Input Layer: 784 nodes, MNIST images are of dimension 28*28 which have 784 pixels so when flatted it’ll become the input to the neural network with 784 input nodes.\n","* Hidden Layer 1: 256 nodes\n","* Hidden Layer 2: 128 nodes\n","* Output Layer: 10 nodes, for 10 classes i.e. numbers 0-9"],"metadata":{"id":"ju2QZO0d9bWC"}},{"cell_type":"markdown","source":["## Defining Criterion and Optimizer"],"metadata":{"id":"NF9u5U739ujX"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"],"metadata":{"id":"Kofjt4D09tqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Neural Network with Validation\n","The training step inPyTorch is almost identical(同一の) almost every time you train it. But before implementing that let's learn about 2 modes of the model object:\n","* **Training Mode**: Set by **`model.train()`**, it tells your model that you are training the model. So layers like dropout etc. which behave differently while training and testing can behave accordingly.\n","* **Evaluation Mode**: Set by **`model.eval()`**, it tells your model that you are testing the model.\n","\n","\n","If you add the validation loop it'll be the same but with forward pass and loss calculation only. But it may happen that <u>your last iteration is'nt the one that gave you the least (最も少ない) validation loss</u>. To tackle this we can set a max valid loss which can be **np.inf** and if the current valid loss is lesser than we can save the **state dictionary** of the model which we can load later, like a checkpoint. **state dict** is an OrderedDict object that maps each layer to its parameter tesor."],"metadata":{"id":"PLu9dTkD9647"}},{"cell_type":"code","source":["import numpy as np\n","epochs = 5\n","min_valid_loss = np.inf\n","\n","for epoch in range(epochs):\n","    train_loss = 0.0\n","    model.train() # Optional when not using Model Specific layer\n","    for data, labels in trainloader:\n","        if torch.cuda.is_available():\n","            data, labels = data.cuda(), labels.cuda()\n","        optimizer.zero_grad()\n","        target = model(data)\n","        loss = criterion(target, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    valid_loss = 0.0\n","    model.eval() # Optional when not using Model Specific layer\n","    for data, labels in validloader:\n","        if torch.cuda.is_available():\n","            data, labels = data.cuda(), labels.cuda()\n","\n","        target = model(data)\n","        loss = criterion(target, labels)\n","        valid_loss += loss.item()\n","\n","    train_loss = train_loss / len(trainloader)\n","    valid_loss = valid_loss / len(validloader)\n","    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n","\n","    # validation lossが低いモデルを保存する\n","    if min_valid_loss > valid_loss:\n","        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n","        min_valid_loss = valid_loss\n","        # Saving State Dict\n","        torch.save(model.state_dict(), 'save_model.pt')"],"metadata":{"id":"7mlqd3LR_0ad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train, valid, test explanation by gpt4\n","These three stages(train, validation, test) are pivotal in developing, tuning, and evaluating machine learning models to ensure they perform well on unseen data, not just on the data they were trained on.\n","\n","## 1. Train Step\n","Role in PyTorch:\n","The training step involves using a subset of your data (the training set) to teach your model to make predictions or perform its intended task. In PyTorch, this step usually involves defining a model architecture (using **torch.nn** module), a loss functioon (like MSE or CrossEntropyLoss), and an optimizer (such as SGD or Adam). During training, the model makes predictions on the training data, calculates the error (loss) by comparing these predictions to the actual targets, and then adjusts the model's weights through backpropagation to reduce this error.\n","\n","\n","Why It's Needed:\n","Training is crucial because it's how your model learns the relationship between inputs and outputs. It's the phase where your model iteratively improves its accuracy on the dataset provided.\n","\n","## 2. Validation Step\n","Role in PyTorch:\n","Validation involves periodically(定期的に) evaluationg the model on a separate subset of the data (the validation set) that is not used for training. This step is crucial for hyperparameter tuning and for avoiding overfitting. Overfitting occurs when the model performs well on the training data but poorly on unseed data, meaning it has learned the training data too well, including its noise and outliers. In PyTorch, you typically run the validation step after certain number of training epochs to check the model's performance and make decisions about which model to save, when to stop training, or it's time to <u>adjust learning rates or other hyperparameters</u>.\n","\n","\n","Why It's Needed:\n","The validation step is essential for making sure your model generalizes well and doesn't just memorize the training data. It provides a reliable estimate of the performance of your model on unseen data without touching the test set.\n","\n","\n","## 3. Test Step\n","Role in PyTorch:\n","After training and validation, the final step is to evaluate your model on the test set, another separate subset of data that the model has never seen during training or vaidation. This step gives you the final mesure of how well your model is expected to perform in the real world on completely unseen data. In PyTorch, you'd typically load your best model and run it on the test dataset to assess its performance using appropriate metrics(accuracy, precision, recall, F1 score, etc.), depending on your task.\n","\n","\n","Why It's Needed:\n","The test step is critila for assessing the final model's performance. It's your best estimate of how the model will perform in real-world scenarios or on completely new, unseen data.\n","\n","\n","## Conclusion\n","Separating into training, validation, and testing phases allows you to train models effectively, tune hyperparameters without bias, and evaluate performance with an unbiased estimate of real-world performance. This approach helps in developing robust models that generalize well beyond the specific data they were trained on, ensureing reliability and effectiveness in practical applications."],"metadata":{"id":"T-0Qy4r-B9ce"}},{"cell_type":"code","source":[],"metadata":{"id":"7sSfAyvjB_9Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tuning Parameters in validation for LSTM\n","Adjusting hyperparameters is a crucial part of tuning models, especially complex ones like LSTMs, for tasks such as predicting future stock prices based on features like opening price, closing price, volume, and high/low values. Here are some key hyperparameters you might consider adjusting during the validation step when working with LSTMs and linear layers in such a predictive model:\n","\n","\n","## For LSTM Layers\n","1. **Number of Layers**: The depth of the LSTM can significantly impact performance. Adding more layers can help the model learn complex patterns but also increases the risk of overfitting and requires more computational power.\n","2. **Hidden Units**: The number of units in the LSTM layers. More units can increase the model's capacity but also its complexity and likelihood of overfitting.\n","3. **Dropout Rate**: Dropout is a regularization technique where randomly selected neurons are ignored during training, reducing the risk of overfitting. Adjusting the dropout rate can help find a balance between model complexity and generalizatioon.\n","4. **Learning Rate**: This controls how much the model's weights are adjusted with respect to the loss gradient. Too high a learning rate can cause the model to converge too quickly to a suboptimal solution, while too low a rate can make training excessively slow.\n","5. **Batch Size**: The number of training examples used in one itertion. Smaller batch sizes can offer a regularizing effect and more stable convergence, while larger batches provide computational efficiency.\n","\n","\n","## For Linear Layers\n","1. **Output Features**: In linear layers, particularly the final one, you need to adjust the output features to match your prediction target. For stock price prediction, this might be 1 (predicting the future price) or more if predictiong multiple future points or features.\n","\n","\n","## For Both LSTM and Linear Layers\n","1. **Optimizer**: The coice of optimizer (e.g., Adam, SGD, RMSprop) can affect training dynamics and final model performance.\n","2. **Learning Rate Schedule**: Adjusting the learning rate over time (e.g., reducing it when the validation loss plateaus) can help fine-tuning the model.\n","\n","\n","## General Hyperparameters\n","* **Epochs**: While technically not adjusted during training, deciding when to stop training (early stopping) based on validation performance is critical.\n","* **Sequence Length**: For time-series data like stock prices, the length of the input sequences can impact model performance.\n","\n","\n","## Model Architecture Specifics\n","* **Combining LSTM and Linear Layers**: How you structure the model - such as the sequence of LSTM and linear layers, use of dropout or batch normalization between layers, and whether to stack multiple LSTM layers before feeding the output to linear layers - can all be considered hyperparameters to some extent.\n","\n","\n","Adjusting these hyperparameters requires a careful balance. Typically, this is done through a process of experimentation, guided by performance on the validation set. Tools and techniques like grid search, random search, or more advanced methods like Bayesian optimization can be used to explore the hyperparameter space efficiently.\n","\n","\n","Remember, the goal is to improve the model's ability to generalize to unseed data, not just to perform well on the training or validation set. Monitoring validation performance while adjusting these hyperparameters is key to finding a wel-tuned model for predicting future stock prices.\n"],"metadata":{"id":"A1TlEFD5TRDa"}},{"cell_type":"code","source":[],"metadata":{"id":"SfFMI39900kH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Example code of train, validation, and test step\n","Could you make sample program runing in Jupyter notebook for me understand each step of train, validatio, and test steps? Please realize following condition, when you make code.\n","\n","---condition---\n","1. Please make sample daily data having 1 target (\"stock price\") and 4 features (\"yesterday price\", \"last change rate\", \"Oil price\", \"CPI\").\n","2. Please separate data into train, validation, and test.\n","3. Use pytorch and create LSTM model to predict tomorrow stock price.\n","4. Sequence length of LSTM is 60 days.\n","5. Make code to run jupyter notebook.\n","6. Make code in accordance with general manner of pytorch.\n","7. Please make code step by step such \"step1-generate sample data\", \"step2-set dataset & dataloader\", ....\n","8. Please add evaluation step when training model.\n","9. Pease add test step.\n","10. Afeter finishing test step, please predict tomorrows stock price giving last 60 days data. (make Synthetic Data of 60 days)"],"metadata":{"id":"l1JrWsp-03HX"}},{"cell_type":"markdown","source":["**Step1: Generate Sample Data**\n","First, we'll generate synthetic daily data for a hypothetical stock. This data will include 1 target variable (stock price) and 4 features ('yesterday price', 'last change rate', 'Oil price', 'CPI').\n","\n","\n","**Step2: Set Dataset & DataLoader**\n","We'll define a PyTorch **`Dataset`** to handle our synthetic stock data, then create **`DataLoader`** for training, validation, and testing.\n","\n","\n","**Step3: Define the LSTM Model**\n","We will design a simple LSTM model suitable for our task.\n","\n","\n","**Step4: Training, Validation, and Test Steps**\n","We'll write the training loop, including validation within the loop to adjust hyperparameters or for early stopping. After training, we will evaluate the model on the test set.\n","\n","\n","**Step5: Prediction**\n","Finally, we'll predict tomorrow's stock price using the last 60 days of data."],"metadata":{"id":"ctcAcAkb1msO"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"ULK3IHTA0_ZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step1: Generate Sample Data"],"metadata":{"id":"3fmUX1XjKWDr"}},{"cell_type":"code","source":["# Step 1: Generate Sample Data\n","np.random.seed(0)  # For reproducibility\n","data_size = 365  # Let's generate data for one year as an example\n","dates = pd.date_range('2023-01-01', periods=data_size)\n","features = np.random.randn(data_size, 4)  # Generating random features\n","target = features[:, 0] * 0.5 + np.random.randn(data_size) * 0.1  # A simple relation to 'yesterday price' plus noise\n","print(features.shape, target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YY298ELA6bNh","executionInfo":{"status":"ok","timestamp":1712818119028,"user_tz":-540,"elapsed":382,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"5e872b1e-e640-4814-d385-b862b8b8e19d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(365, 4) (365,)\n"]}]},{"cell_type":"code","source":["features.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVGfC91S_i_a","executionInfo":{"status":"ok","timestamp":1712819427297,"user_tz":-540,"elapsed":441,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"9486a7d8-cafb-405d-ba39-876db7b032f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["365"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Step2: Set Dataset & DataLoader"],"metadata":{"id":"IRhN37QSKbBI"}},{"cell_type":"code","source":["# Step2: Set Dataset & DataLoader\n","class StockDataset(Dataset):\n","    def __init__(self, features, target, seq_length=60):\n","        self.features = features\n","        self.target = target\n","        self.length = seq_length\n","\n","    def __len__(self):\n","        # 使えるdata数を返す\n","        return len(self.target) - self.length\n","\n","    def __getitem__(self, index):\n","        return self.features[index:index+self.length], self.target[index+self.length]\n","        # (self.features[index:index+self.seq_length], self.target[index+self.seq_length])\n","\n","# Splitting the dataset into train, validation, test\n","train_size = int(0.6 * data_size)\n","val_size = int(0.2 * data_size)\n","test_size = data_size - train_size - val_size\n","\n","# make dataset object\n","train_dataset = StockDataset(features[:train_size+60], target[:train_size+60])\n","val_dataset = StockDataset(features[train_size:train_size+val_size+60], target[train_size:train_size+val_size+60])\n","test_dataset = StockDataset(features[-(test_size+60):], target[-(test_size+60):])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"UMsFiqvf6bKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(features[-(test_size+60):].shape, target[-(test_size+60):].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NLj6UbIJK9C","executionInfo":{"status":"ok","timestamp":1712821990672,"user_tz":-540,"elapsed":279,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"f091decb-394a-460e-f653-6781f8f9021e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(133, 4) (133,)\n"]}]},{"cell_type":"markdown","source":["## Step3: Define the LSTM Model"],"metadata":{"id":"WKZVFdW1Kf10"}},{"cell_type":"code","source":["for x, y in train_loader:\n","    print(x.shape, y.shape)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5NKREtRf6bHk","executionInfo":{"status":"ok","timestamp":1712822579176,"user_tz":-540,"elapsed":290,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"56d8de77-fa7a-48b5-d615-d89edcc3d663"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 60, 4]) torch.Size([32])\n"]}]},{"cell_type":"code","source":["# Step3: Define the LSTM Model\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size=4, hidden_layer_size=100, output_size=1):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_layer_size = hidden_layer_size\n","        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n","        self.linear = nn.Linear(hidden_layer_size, output_size)\n","\n","    def forward(self, input_seq):\n","        # batch_first=True時の入力順は、lstm(batch_size, sequence_length, features)\n","        lstm_out, _ = self.lstm(input_seq)\n","        predictions = self.linear(lstm_out.contiguous().view(-1, self.hidden_layer_size))\n","        return predictions[-input_seq.size(0):]"],"metadata":{"id":"_2ALd6s6Az6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### lstm_out.contiguous().view(-1, self.hidden_layer_size)in self.linear()\n","**`lstm_out.contiguous().view(-1, self.hidden_layer_size)`** plays an important role in preparing the output of the LSTM layer for the linear layer that follows. Let's break down what each part of this line does and why it's necessary:\n","\n","#### Understanding **`lstm_out`**\n","First, **lstm_out** is the output tensor from the LSTM layer. When **batch_first=True** is used, <u>the shape of **lstm_out** will be **[batch_size, seq_len, hidden_layer_size]**</u>. This tensor contains the output features from the LSTM layer for each time step in the sequence, for each sequence in the batch.\n","\n","\n","#### Purpose of **`.contiguous()`**\n","* **.contiguous()**: This method is used to ensure that the tensor is stored in a contiguous(連続した) block of memory. After certain operations, such as **permute**(次元の並び替え), the layout of the tensor in memory can becom disorganized, meaning that the tensor is not stored in a linear sequence in memory. This can happen even if you don't explicitly use **permute** but after other operations that change tensor shape or structure. Calling **.contiguous()** rearranges the tensor in memory to ensure it's stored sequentially, which is required for some operations like **.view()** that expect tensors to be contiguous. (contiguousにより、データがメモリ上に要素順に再配置される)\n","\n","#### Function of **`.view(-1, self.hidden_layer_size)`**\n","* **.view(-1, self.hidden_layer_size)**: This method is used to reshape the tensor. The **-1** tells PyTorch to infer(推測する) the size of this dimension based on the original size of the tensor and the other specified dimensions. Gives that **lstm_out** originally has the shape **[batch_size, seq_len, hidden_layer_size]**, reshaping it to <u>**[-1, self.hidden_layer_size]**</u> essentially <u>flattens the first two dimensions (batch and sequence length)</u> while preserving the **hidden_layer_size** dimension. This results in a 2D tensor where the first dimension is **batch_size * seq_len**, and the second dimension is **hidden_layer_size**. this reshaping is necessary because the linear layer expects inputs of a fixed size (in this case, **hidden_layer_size**), and we're preparing the LSTM outputs (from all timesteps of all sequences) to be processed by this layer.\n","\n","\n","#### In the Context of the Model\n","The reshaping is crucial for passing the LSTM's output to the linear layer because:\n","* It allows processing the outputs from all time steps of all sequences through the linear layer in a single batch operation, which is efficient and leverages parallelism.\n","* It prepares the data correctly if you want to make predicitons or transformations on the outputs of each time step individually.\n","\n","\n","Here's simplified representation to give you an idea:\n","\n","\n","Before **.view()**, you have a tensor shaped **[batch_size, seq_len, hidden_layer_size]** representing the LSTM'S output for each time step of each sequence. After applying **.contiguous().view(-1, self.hidden_layer_size)**, you get a 2D tensor where each row corresponds to an output from a specific time step of a specific sequence, ready to be fed into the linear layer.\n","\n","\n","\n","This operation effectively transforms the output for each time step into a format that can bbe individually processed by the linear layer, enabling the model to learn from the temporal features extracted by the LSTM for every single time step across all sequences."],"metadata":{"id":"N4tLftk8MK6M"}},{"cell_type":"markdown","source":["## Step4: Training and Validation"],"metadata":{"id":"YjAY4k00KiJ_"}},{"cell_type":"code","source":["# Step4: Training and Validation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = LSTMModel().to(device)\n","loss_function = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    # training step\n","    model.train()\n","    for inputs, targets in train_loader:\n","        inputs = inputs.float().to(device)\n","        targets = targets.float().to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = loss_function(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation step\n","    with torch.no_grad():\n","        model.eval()\n","        val_loss = 0.0\n","        for inputs, targets in val_loader:\n","            inputs = inputs.float().to(device)\n","            targets = targets.float().to(device)\n","            outputs = model(inputs)\n","            val_loss += loss_function(outputs, targets).item()\n","        val_loss /= len(val_loader)\n","        print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E21AVL2HfkM","executionInfo":{"status":"ok","timestamp":1712821998498,"user_tz":-540,"elapsed":2506,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"c3b91cc8-bc45-4b5b-9e9f-bacbd440918e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.1979, Val Loss: 0.2085\n","Epoch 2, Train Loss: 0.1885, Val Loss: 0.2072\n","Epoch 3, Train Loss: 0.1818, Val Loss: 0.2067\n","Epoch 4, Train Loss: 0.1784, Val Loss: 0.2065\n","Epoch 5, Train Loss: 0.1771, Val Loss: 0.2063\n","Epoch 6, Train Loss: 0.1763, Val Loss: 0.2059\n","Epoch 7, Train Loss: 0.1747, Val Loss: 0.2053\n","Epoch 8, Train Loss: 0.1717, Val Loss: 0.2044\n","Epoch 9, Train Loss: 0.1672, Val Loss: 0.2031\n","Epoch 10, Train Loss: 0.1626, Val Loss: 0.2025\n"]}]},{"cell_type":"markdown","source":["## Step5: Evaluate on Test Set"],"metadata":{"id":"3d4jkESLKlnr"}},{"cell_type":"code","source":["# Step5: Evaluate on Test Set\n","with torch.no_grad():\n","    model.eval()\n","    test_loss = 0.0\n","    for inputs, targets in test_loader:\n","        inputs = inputs.float().to(device)\n","        targets = targets.float().to(device)\n","        outputs = model(inputs)\n","        test_loss += loss_function(outputs, targets).item()\n","    test_loss /= len(test_loader)\n","\n","print(f\"Test Loss: {test_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkeK51pkIked","executionInfo":{"status":"ok","timestamp":1712822002945,"user_tz":-540,"elapsed":405,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"d3bf06b4-34fe-4c00-d316-39d66917dff9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.2756\n"]}]},{"cell_type":"code","source":["for x, y in test_loader:\n","    print(x.shape, y.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_HaFDADJg2P","executionInfo":{"status":"ok","timestamp":1712822094619,"user_tz":-540,"elapsed":291,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"b5814c45-a74c-4a80-859f-739ebf2b94c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 60, 4]) torch.Size([32])\n","torch.Size([32, 60, 4]) torch.Size([32])\n","torch.Size([9, 60, 4]) torch.Size([9])\n"]}]},{"cell_type":"code","source":["outputs.shape\n","# batch size の分予測される"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43_yXou8JFsn","executionInfo":{"status":"ok","timestamp":1712822045056,"user_tz":-540,"elapsed":11,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"6292d13f-eb20-4d0c-d2d0-2d6b71609787"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([9, 1])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Step6: Predict Tomorrow's Stock Price"],"metadata":{"id":"-yHuYL8OKnd8"}},{"cell_type":"code","source":["# Step6: Predict Tomorrow's Stock Price\n","last_60_days_features = features[-60:]  # Assuming this is new unseen data\n","last_60_days_features.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mb5io2b7Jwg_","executionInfo":{"status":"ok","timestamp":1712822231987,"user_tz":-540,"elapsed":303,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7bab8811-e7f0-427d-a3e0-cd1b6eceb7ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60, 4)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["with torch.no_grad():\n","    model.eval()\n","    features_added_batch_dimension = torch.tensor(last_60_days_features).float().to(device).unsqueeze(0)\n","    # last_60_days_featuresのshape: (60, 4)\n","    # features_added_batch_dimensionのshape: (1, 60, 4)\n","    prediction = model(features_added_batch_dimension)\n","    print(f\"Predicted stock price for tomorrow: {prediction.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPZ9DHCnJ8IJ","executionInfo":{"status":"ok","timestamp":1712822507089,"user_tz":-540,"elapsed":433,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"a500d5b9-f1ac-472e-df9b-13b20169ab10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted stock price for tomorrow: 0.07109205424785614\n"]}]},{"cell_type":"code","source":["torch.tensor(last_60_days_features).float().to(device).unsqueeze(0).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0WRCa8gK0dA","executionInfo":{"status":"ok","timestamp":1712822383655,"user_tz":-540,"elapsed":307,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bbb22e0d-46f8-47d1-c603-1f47d1dee075"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 60, 4])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["last_60_days_features.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvj8Zw5BK16A","executionInfo":{"status":"ok","timestamp":1712822451507,"user_tz":-540,"elapsed":402,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"a688b084-fc3b-4aa7-cb29-4ada7a1a910c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60, 4)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["## add: Saving and Reloading the Model"],"metadata":{"id":"IliR5KLbhETM"}},{"cell_type":"markdown","source":["### Saving the Model\n","After the training process, you can save the model to a file. PyTorch offers two main ways to save a model: saving the entire model using **torch.save()** or saving just the model state dictionary. Saving the state dictionary is the recommended way, asi it allows for more flexibility if you need to change the model architecture later but still want to use the pretrained weights.\n","\n","\n","Here's how to save the model state dictionaly:"],"metadata":{"id":"-EM7AyRZhL1M"}},{"cell_type":"code","source":["# Save the model state\n","model_path = '/content/drive/MyDrive/study_DeepLearning/data/train_val_test_in_pytorch_model.pth'\n","torch.save(model.state_dict(), model_path)"],"metadata":{"id":"AGrShbt4LGrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code will save the model's weights to a file named \"train_val_test_in_pytorch_model.pth\""],"metadata":{"id":"qDqOCbIKiCLr"}},{"cell_type":"markdown","source":["### Loading the Model\n","To load the model for future predictions, you first need to recreate the model structure and then load the state dictionary into this structure. Here's how:"],"metadata":{"id":"T6Uu79nTiJSO"}},{"cell_type":"code","source":["# Difine the model structure again\n","loaded_model = LSTMModel()\n","\n","# Load the model state\n","model_path = '/content/drive/MyDrive/study_DeepLearning/data/train_val_test_in_pytorch_model.pth'\n","\n","# Load the model state\n","loaded_model.load_state_dict(torch.load(model_path))\n","\n","# Ensure the model is in evaluation mode\n","loaded_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hd7Vzb_ziBrW","executionInfo":{"status":"ok","timestamp":1712829025335,"user_tz":-540,"elapsed":271,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"8b674f52-9c08-4c19-80f0-8dc718e6a4da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LSTMModel(\n","  (lstm): LSTM(4, 100, batch_first=True)\n","  (linear): Linear(in_features=100, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["### Making a Predcition"],"metadata":{"id":"Dr5klCOJkOMb"}},{"cell_type":"code","source":["# Predict tomorrow's stock price using the loaded model\n","with torch.no_grad():\n","    loaded_model.eval()\n","    features_added_batch_dimension = torch.tensor(last_60_days_features).float().to(device).unsqueeze(0)\n","    prediction = loaded_model(features_added_batch_dimension)\n","    print(f\"Predicted stock price for tomorrow: {prediction.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MbspBCbkMmJ","executionInfo":{"status":"ok","timestamp":1712829119611,"user_tz":-540,"elapsed":265,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"3c19d6cf-1902-4ead-8aa7-21c160f26ee3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted stock price for tomorrow: 0.07109205424785614\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5ou4MpjEkT7k"},"execution_count":null,"outputs":[]}]}