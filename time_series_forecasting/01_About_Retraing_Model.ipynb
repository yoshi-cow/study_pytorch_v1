{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhz7riqjyuU3PoyPIupyKM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# About retraining model.\n","The sentence you're referring to, \"We retrain our model at every two years and use all data available up to that point to update parameters\", outlines a specific approach to maintaining and inproving the accuracy and relavance of their deep learning model over time. Let me break it down:\n","\n","## Retraining the Model Every Two Years\n","Retraining a model at regular intervals means that the model is not static; it undergoes periodic updates. Specifically, \"every two years\" incdicates that the authors take the data up until that two-year mark and train their model anew(改めて). This is a common practice in machine learinig and finance for several reasons:\n","1. **Data Drift**: Over time, the underlying patterns in financial markets can change due to countless factors like economic shifts, policy changes, and market sentiment. Retraining ensures the model adapts to these changes.\n","2. **Model Freshness**: Continuously updating the model with new data keeps its predictions relevant and accurate, as it learns from the most recent trends and information.\n","\n","## Using All Data Available Up to That Poin\n","This part means that with each retraining cycle, they <u>don't just use the last two years of data</u>; they use all the data collectecd from the start of their dataset in 2006 up to the current retraining period. For instance, if they retrain in 2012, they use all data from 2006 through 2012. This cumulative(累積的) approach allows the model to learn from a broader historical context, making its understanding of market behaviors more robust.\n","\n","## Sequence Lenghts and Features\n","The model's structure, with 50 days of sequence lengths and 8 features, implies it's designed to look at 50-day windows of data at a time, using 8 distince types of information (features) within each window to make predictions. This doesn't necessarily conflict with the retraining strategy. Each training session likely involves looking at multiple 50-day sequences from the start of the dataset until the retraining point, rather than training on a single sequence from 2006 to 2010.\n","\n","<br>\n","So, in essence, the retraining process meands:\n","* Every two years, the model updated.\n","* Each update uses the entirely of the dataset available up to that point, not just the data since the last update.\n","* Withi9n these datasets, the model operates on 50-day windows, applying its 8 features to understand and predict asset allocation.\n","\n","It's not about trainig just once using data from 2006 to 2010 but continuously updating with all available data in two-year cycles to stay as accurate and relevant as possible."],"metadata":{"id":"11M7zXKMiDXO"}},{"cell_type":"markdown","source":["# Example of retraining loop of two years\n","Let's clarify retraining with a simple example. Imagine you have a dataset that starts in 2006 and goes through 2020, and you're following the procedoure mentioned in the essay. Here's how the retraining loop would work, step by step:\n","\n","<br>\n","\n","### Initial Training\n","* **Start Date**: 2006\n","* **End Date**: Initial training doesn't have an end date; you all the data available up to the first retraining point.\n","* **Data Used for Training**: All available data from 2006 up to just before 2008.\n","* **purpose**: Train the initial model to make predictions based on the learned patterns.\n","\n","### First Retraining(2010)\n","* **Retraining Point**: Every two years from the start, so the first retraining occurs in 2008, but for the tessting period and data availability, we're focusing on the time from 2011 onwards. So, the first relevant retraining for our scenario would be in 2010 for usage in predictions thereafter.\n","* **Data Used for Retraining**: All available data from 2006 through 2010.\n","* **Purpose**: Update the model to incorporate new data from 2008 to 2010, adapting to any changes in market behavior observed during this period.\n","\n","### Subsequent Retrainings\n","#### Second Retraining(2012)\n","* **Data Used**: 2006 - 2012\n","* **Purpose**: Integrate additional two years of data, adjusting to newer trends and information.\n","\n","#### Third Retraining(2014)\n","* **Data Used**: 2006 - 2014\n","* **Purpose**: Continuous to refine and update the model's parameters with the latest data.\n","#### And so on,\n","every two yeas unitl the end of your dataset or testing period (2020 in your case).\n","\n","\n","## Visualizing the Process\n","Imagine it as a sliding window that expands rather than moves:\n","* **Initial Phase**: [2006 to 2008] - Train with this data.\n","* **First Update**: [2006 to 2010] - Retrain with the entire dataset up to 2010.\n","* **Second Update**: [2006 to 2012] - Retraining with data up to 2012.\n","* **Etc..**:, incorporating all new data up to the point of retraining.\n","\n","\n","## Key Points\n","* **Cumulative Data**: With each retraining cycle, you're not dicarding old data; you're adding new data to the pool you already have and retraining the model on the entire cumulative dataset.\n","* **Adaption**: This method helps the model adapt over time to long-term trends, sudden market shifts, and emerging patterns.\n","* ** Testing Period**: Although retraining happens every two years, the actual testing of the model's performance, as mentioned, starts from 2011 to the end of April 2020. This means they're particularly interested in how the model, updated with this method, performs during that specific period.\n","\n","<br>\n","\n","This approach ensures that the model's understanding of the data is as up-to-date as possible, considering both recent trends and historical data to inform its predictions."],"metadata":{"id":"afTaZo8Jn-HQ"}},{"cell_type":"markdown","source":["# Difference between udating the model by every two years and learining the model from 2006 through 2010.\n","\n","The key difference between updating a model every two years and learning a model from 2006 to 2010 once involves how frequenctly the model's parameters are adjusted and the range of data used to inform these adjustments. Let's break down these two approaches to hightlight their their differences:\n","\n","## Updating Every Two Years\n","* **Dynamic and Adaptive**: By retraining the model every two years, you continuously update it to reflect the latest data. This means the model's understanding of the data evolves as new information becomes available.\n","* **Cumulative Data Use**: Each retraining session uses all the data available up to that point, not just the last two years. This way, the model learns from both the long-term trends and the more immediate patterns in the data.\n","* **Advantages**: This approach helps the model stay relevant and adaptive to new trends, economic cycles, and anomalies in the data. It's particularly useful in environments like financial markets, which are highly volatile and influenced by a wide range of every-changing factors.\n","\n","## Learnin From 2006 to 2010 (One-Time Training)\n","* **Static Model**: If you train a model only once using data from 2006 to 2010, the model learns to make predictions based on the patterns present in that specific timeframe.\n","* **Limited Data Use**: The model does not benefit from any data or emerging trends after 2010. Its understanding is fixed based on the historical data it was trained on, withou adapting to any changes that occur in the market after that period.\n","* **Disadvantage**: This approach might limit the model's effectiveness over time, as it cannot adjust that the patterns learned from the 2006-2010 dataset will continue to apply in the future without deviation(偏差), which is rarely the case in dynamic environments like financial markets.\n","\n","## Summary\n","* **Updating every two years** is a more dynamic, resposive approach that keeps the model current with the latest data and trends. It's well-suited to environments that change ove time, like financial markets.\n","* **Learning from 2006 to 2010(one-training) creates a static model that might quckly become outdated as new data emerges and market conditions evolves.\n","\n","\n","For financial applications, where market conditions can change rapidly, updating the model regularly with all available data tends to offer better performance and more relevant predictions."],"metadata":{"id":"SzGLHJjAsg00"}},{"cell_type":"markdown","source":["# difference of retrainig model between updating  former parameters and making new model by expanded data\n","\n","## 1. Training a New Model with Expanded Data\n","This approach involves using the expanded dataset to train a new instance of the model from scratch. It's you're starting the training process anew, but with more data than was available during the previous training cycle.\n","\n","### Advantages:\n","* **Fresh Start**: Each retraining cycle is an opportunity to learn the patterns in the data without any biases from previous training cycles.\n","* **Flexibility**: You can easily incorporate changes in the model architectoure or training process if new insights or techniques become available.\n","* **Simplicity**: conceptually straightforward; you're essentially repeating the initial training process with more data.\n","\n","### Use Case:\n","Particularly useful when the additional data might significantly change the relationships within the data, or when you want to ensure that the model is not constrained by any potential biases from its earlier version.\n","\n","## 2. Incrementally Updating the Existing Model's Parameters\n","Alternatively, some models are designed to be updated incrementally with new data rather than retrained from scratch. This can involve updating the model's parameters using just the new data, possibly with techniques like online learning or transfer learning, where the model is \"fine-tuned\" rather than completely retrained.\n","\n","### Advatages:\n","* **Efficiency**: Generally requires less computational resource and time than training a new model from scratch, especially if the dataset grows incremetally.\n","* **Countinuity**: Maintains a continuity in learning, potentially leveraging the stability and insights gained from previous training cycles.\n","\n","### Use Cases:\n","Suitable for scenarios where the new data is expected to refine rather than overhaul the model's understanding, or computational resources are limited.\n","\n","## Which is More Common in Finance?\n","In finance modeling, **training a new model with expanded data** is a commonly preferred approach, especially for complex models and dynamic environments. Financial markets are influenced by a wide array of factors that can change over time, including economic indicators, policy cahnges, and market sentiment. Training a new model with the expanded dataset helps ensure that the model can adapt to these changes by learning from a more comprehensive dataset that includes the most recent information.\n","\n","\n","This approach is favored because it allows the model to ingegrate the latest trends and data **without being overly(過度に) constrained by past assumptions**. It's crucial in finance, where the relevance of historical data can change as market conditions evolve.\n","\n","\n","However, the choice between these strategies depends on the specifit context, the model's complexity, the frequency and nature of new data, and the computational resources available. Incremental updates might still be used in specific contexts where the data changes are minor or computational efficiency is a priority."],"metadata":{"id":"aU-GBzTaBebQ"}},{"cell_type":"markdown","source":["# Retraining and Testing Process Described in the Essay\n","-from essay-\n","\n","Our data set ranges from 2006 to 2020 and contains daily observations. We retrain our model at every two years and use all data available up to that point to update parameters. Overall, our testing period from 2011 to the end of April 2020.\n","\n","<br>\n","The process described in the essay involves periodic retraining of the model every two years, using all available data up to that point. This means:\n","\n","1. **Initial Training and Subsequent Retraining**: The model is initially trained with data available <u>**from the start(2006) up to two years before the first testing period(=2010)**</u>. Then, every two years, it is retrained with all the data avalilable up to that point. This process involves expanding the dataset with new data accumulated since the last retraining.\n","2. **Final Model for Testing**: The \"final\" model, which is used for testing from 2011 to the end of April 2020, is not merely the model trained on data from 2006 to 2010. Instead, it is the most recently retrained model that includes all data available up to the last retraining cycle before or during the testing period. This means the model used for testing in 2011 would have been trained on data from 2006 up to 2010, but as new data becomes available (2012, 2014, etc.), the model is retrained to include this new information.\n","3. **Continuous Update**: Therefore, the testing period does not rely on a static model trained only once. It uses a model that has been periodically updated to incorporate all available data, ensuring that the model's predictions are based on the most comprehensive and recent information.\n","\n","## Clarification\n","* **Periodic Retraining**: The essay describes a process where the model is not static but is retrained every two years with an expanded dataset that includes all data available up to that retraining point. This ensures that the model's parameters are updated to reflect new data and potentially new market dynamics.\n","* **Final Model**: The \"final\" model for any given point during the testing period is the latest retrained model, which includes all data up to the last retraining cycle before or within the testing period. This means the model's parameters have indeed been updated from the original training based on the cumulative data.\n","\n","<br>\n","In summry, in the context provided, retraining the model means periodically updating it with expanded data to ensure it remains relevant and accurate for the testing period. The final model used for testing is the outcome of the latest retraining cycle, which includes all data available up to that point, not just data from 2006 to 2010."],"metadata":{"id":"9eHU5mkZMW_o"}},{"cell_type":"markdown","source":["# Example Code"],"metadata":{"id":"UgDJ_V5niQjH"}},{"cell_type":"code","source":["from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","\n","# Hypothetical function to simulate loading dataset for a given period\n","def load_data_for_period(start_year, end_year):\n","    # This function is a placeholder. In practice, you'd load your dataset from a file or database.\n","    # For simplicity, we're generating a synthetic dataset with make_regression.\n","    X, y = make_regression(n_samples=1000, n_features=8, noise=0.1, random_state=42)\n","    return X, y\n","\n","# Function to train or retrain the model with data up to the specified end_year\n","def train_or_retrain_model(model, start_year, end_year):\n","    X, y = load_data_for_period(start_year, end_year)\n","    model.fit(X, y)\n","    return model\n","\n","# Initial training period\n","start_year = 2006\n","first_end_year = 2008\n","\n","# Initialize the model\n","model = LinearRegression()\n","\n","# Train the model with the initial dataset\n","model = train_or_retrain_model(model, start_year, first_end_year)\n","\n","# Retraining loop: retrain the model every two years with all data up to that point\n","for end_year in range(2010, 2021, 2):  # Assuming data is available up to 2020\n","    model = train_or_retrain_model(model, start_year, end_year)\n","    print(f\"Model retrained with data from {start_year} to {end_year}\")\n","\n","# At this point, `model` is the latest version, retrained with data up to 2020.\n","# This model would be used for testing or making predictions in the context described.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_z9lyTfXMPsG","executionInfo":{"status":"ok","timestamp":1712576872559,"user_tz":-540,"elapsed":292,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"9f864dbb-1715-42ea-8032-490368b9191d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model retrained with data from 2006 to 2010\n","Model retrained with data from 2006 to 2012\n","Model retrained with data from 2006 to 2014\n","Model retrained with data from 2006 to 2016\n","Model retrained with data from 2006 to 2018\n","Model retrained with data from 2006 to 2020\n"]}]},{"cell_type":"markdown","source":["This code simulates the retraining process by:\n","* Starting with an initial training period using data from 2006 to 2008.\n","* Entering a loop where the model is retrained every two years using all available data from 2006 to the current year in the loop.\n","* Pringing a message each time the model is retrained to indicate the data range used.\n"],"metadata":{"id":"tKkGP7Blinv7"}},{"cell_type":"markdown","source":["# Training and Testing Periods on the base of essay\n","-essay-\n","\n","Our ata set ranges from 2006 to 2020 and contains daily observations. We retrain our model at every two years and use all data available up to that point t oupdate parameters. Overall, our testing period from 2011 to the end of April 2020.\n","\n","<br>\n","For simplicity, we'll assume the retraining occurs at the beginning of every odd-numbered year, and testing occurs after each retraining. The testing period for each cycle is from the retraining point until the end of April 2020, but it starts in 2011 as specified.\n","\n","<br><br>\n","\n","| Retraining Year | Training Terms | Testing Terms |\n","| ---- | ---- | ---- |\n","| 2008 | 2006 - 2007 | 2011 - 2020 |\n","| 2010 | 2006 - 2009 | 2011 - 2020 |\n","| 2012 | 2006 - 2011 | 2012 - 2020 |\n","| 2014 | 2006 - 2013 | 2014 - 2020 |\n","| 2016 | 2006 - 2015 | 2016 - 2020 |\n","| 2018 | 2006 - 2017 | 2018 - 2020 |\n","| 2020 | 2006 - 2019 | 2020 - |\n","\n","<br><br>\n","\n","## Summary\n","The model is retrained every two years with all available data up to that point. After each retraining, the model can be tested on new data that comes in until the end of April 2020. Note that the actual testin starts in 2011, according to the specifications, so while the model is retrained with data up to certain years before 2011, testing using this model effectively starts from 2011 onward.\n","\n","<br>\n","\n","This approach ensure that the model is continuously updated with the most comprehensive dataset available, allowing for improvements in prediction accuracy and adaptability to new trends and patterns in the data."],"metadata":{"id":"wZg_lsW1j_-i"}},{"cell_type":"code","source":[],"metadata":{"id":"xqwteMVPnioE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Different from Online Learning (Incremental Learning)\n","## In incremental learining:\n","1. A model is initially trained with a subset of the data (e.g., 2006 to 2008).\n","2. The model is then updated with only new data that comes in, without reusing the old data for training. For instance, after the initial training, the model is updated with data from 2009 to 2010, and then again with data from 2011 to 2012, and so on.\n","3. this approach modifies the existing model parameters based on new data only.\n","\n","### Advantages:\n","* Efficient in terms of computation and memory, as it doesn't retraining the model from scratch with all the accumulated data.\n","* Suitable for scenarios where the data stream continuously and you need to keep the model up-to-date in almost real-time.\n","\n","## In Retraining (Previously Described Approach) approach:\n","1. A model is initially trained with a subset of the data (e.g., 2006 to 2008).\n","2. The model is periodically retrained with all the accumulated data up to that point. For example, in the next cycle, it's retrained with data from 2006 to 2010, then with data from 2006 to 2012, and so on.\n","3. This process effectively creates a new version of the model during each retraining cycle, incorporating both new and old data to adjust the model's parameters.\n","\n","### Advantages:\n","* Allows the model to relearn from the entire dataset, ensureing it captures all the underlying patterns and relationships, which might be especially relevant in changing environments like financial markets.\n","* It's particularly useful when the addition of new data might significantly change the model's understanding of the problem space.\n","\n","### Clarification\n","* If by #updating\" the model, you mean using only new data to incrementally adjust the model's parameters without revisiting the old data, then yes, your description aligns more closely with incremental learning or online learning.\n","* The explanation I provided earlier about retraining every two years with all the data available up to that point is not incremental learning; it's a full retraining approach that uses both new and existing data to update the model.\n","\n"],"metadata":{"id":"PKAHCGtEniK-"}},{"cell_type":"code","source":[],"metadata":{"id":"XMw9QYvujDW3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Differenc between Retraining and Fine-tuning\n","\n","## Retraining\n","Retraining refers to the process of training a model again, typically on a new or expanded dataset. This can mean:\n","\n","* **Training from Scratch**: Starting the learning process over again with the initial parameters reset. This approach is often used when significant new data becomes available, or when the model needs to bbe adapted to changes in the data distribution (concept drift).\n","* **Continued Training**: continuing the training process from the current state of the model but on new or additional data. In this scenario, the model's learned parameters serve as the starting point, and then the model aims to adjust these parameters to account for the new information.\n","\n","\n","Retraining is generally used to ensure that a model remains accurate and relevalt as the data it's intended to represent changes over time.\n","\n","<br>\n","\n","## Fine-Tuning\n","Fine-tuning, while similar to retraining in that it also involves furether training of an already-trained model, usually has a more specific focus:\n","\n","* **Staring Point**: Begins with a model that has been pre-trained on a large dataset. This model already has learned features or patters that are broadly applicable across related tasks.\n","* **Ojbective**: The goal is to adjust or \"fine-tune\" the model's parameters slightly so it performs better on a specific task or dataset. This usually involves a smaller, more specific dataset than the one used for initial training.\n","* **Process**: Often, fine-tuning freezes the earlier layers of deep neural network (which capture more general features) and only trains some of the later layers (which capture more specific features). This approach leverages the general understanding the model has developed during its initial training, adjusting it to perform better on related but distinct task.\n","\n","\n","## Key Differences\n","* **Scope and Purpose**: Retraining is often brader, aimed at maintaining or improving model performance over time or on a similar task with new data. Fine-tuning is more focused , aiming to adapt a model to a specific task, oftern with a smaller or more targeted dataset.\n","* **Data Used**: Retraining can use a completely new dataset or an expanded version of the original dataset. Fine-tuning typically uses a different, usually smaller dataset that's specific to the task of interest.\n","* **Parameter Adjustment*** Retraining can involve adjusting all the model's parameters or continuing training on all parameters. Fine-tuning often involves selective adjustment of parameters, especially in the context of deep learning, where early layers are often freeze.\n","\n","\n","In summary, while both retraining and fine-tuning are methods for updating a model's parameters post-inital training, they serve different purposes and are implemented differently. Retraining aims to update or maintain the model's preformance in light of new data or changing conditions, while fine-tuning aims to adapt a model that's already performing well on a general task to excel on a more specific task."],"metadata":{"id":"9Si_8c8orBoW"}},{"cell_type":"code","source":[],"metadata":{"id":"-wBQtOz_u_ZI"},"execution_count":null,"outputs":[]}]}