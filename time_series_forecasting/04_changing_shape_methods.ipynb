{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnTIoEQG00weIzNwEYmjLN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Change Shape methods\n","1. **reshape()**: This method returns a new tensor with the specified shape. If possible, <u>it shares the data with the original tensor to avoid copying</u>.\n","2. **view()**: Similar to **reshape()**, but it strictly returns <u>a vew of the original tensor with the new shape</u>. It requires the new shape to be compatible with the original tensor's layout.\n","3. **squeeze()**: This removes all dimensions of size 1 from the tensor.\n","4. **unsqueeze()**: This adds a dimension of size 1 at a specified index.\n","5. **transpose()**: This swaps two dimensions of the tensor.\n","6. **permute()**: This rearranges the dimensions of the tensor according to a given order."],"metadata":{"id":"2ur6eMUINe7X"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOqqAjsXNVse","executionInfo":{"status":"ok","timestamp":1713579207167,"user_tz":-540,"elapsed":346,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"8c32801e-cf0c-48c5-e2bf-d8074c6328a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[ 1.8675,  0.1820, -0.1222],\n","        [-0.1989, -0.0191, -1.2608]])\n","Shape of original tensor:  torch.Size([2, 3])\n"]}],"source":["import torch\n","\n","# Create an example tensor\n","x = torch.randn(2, 3) # Shape is (2, 3)\n","print(\"Original tensor:\")\n","print(x)\n","print(\"Shape of original tensor: \", x.shape)"]},{"cell_type":"code","source":["# Using reshape to change shape\n","x_reshaped = x.reshape(3, 2)\n","print(\"Reshaped tensor (3, 2):\")\n","print(x_reshaped)\n","print(\"Shape of reshaped tensor: \", x_reshaped.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1VFGD_DPDzY","executionInfo":{"status":"ok","timestamp":1713579209361,"user_tz":-540,"elapsed":418,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"06c5401c-c9de-45b7-e9b1-7341ba8a59d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reshaped tensor (3, 2):\n","tensor([[ 1.8675,  0.1820],\n","        [-0.1222, -0.1989],\n","        [-0.0191, -1.2608]])\n","Shape of reshaped tensor:  torch.Size([3, 2])\n"]}]},{"cell_type":"code","source":["# Using view to change shape\n","x_view = x.view(6)\n","print(\"View tensor (6):\")\n","print(x_view)\n","print(\"Shape of view tensor: \", x_view.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkFQyL7ePSfl","executionInfo":{"status":"ok","timestamp":1713579223642,"user_tz":-540,"elapsed":288,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"14abf912-b6d6-46d0-ca9a-ec73390ed021"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["View tensor (6):\n","tensor([ 1.8675,  0.1820, -0.1222, -0.1989, -0.0191, -1.2608])\n","Shape of view tensor:  torch.Size([6])\n"]}]},{"cell_type":"code","source":["# Using squeeze to remove dimensions of size 1\n","x = torch.randn(1, 2, 3, 1)\n","print(\"Original tensor:\")\n","print(x)\n","print(\"Shape of original tensor: \", x.shape)\n","\n","x_squeeze = x.squeeze()\n","print(\"Squeezed tensor:\")\n","print(x_squeeze)\n","print(\"Shape of squeezed tensor: \", x_squeeze.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ni9tzLauR-eo","executionInfo":{"status":"ok","timestamp":1713579273282,"user_tz":-540,"elapsed":284,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"0338c69c-bf77-4e38-84fc-8e7f41b4a920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor:\n","tensor([[[[-0.6028],\n","          [ 0.5040],\n","          [ 0.4118]],\n","\n","         [[-0.6902],\n","          [-0.3452],\n","          [-1.0556]]]])\n","Shape of original tensor:  torch.Size([1, 2, 3, 1])\n","Squeezed tensor:\n","tensor([[-0.6028,  0.5040,  0.4118],\n","        [-0.6902, -0.3452, -1.0556]])\n","Shape of squeezed tensor:  torch.Size([2, 3])\n"]}]},{"cell_type":"code","source":["# Using unsqueese to add a dimension of size 1\n","x_unsqueezed = x.squeeze().unsqueeze(0)\n","print(\"Unsqueezed tensor (add dimension at index 0):\")\n","print(x_unsqueezed)\n","print(\"Shape of unsqueezed tensor: \", x_unsqueezed.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZL5_VlWSKmC","executionInfo":{"status":"ok","timestamp":1713579341440,"user_tz":-540,"elapsed":268,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"a12b445f-ac10-484d-f8e5-448ae8abdf96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unsqueezed tensor (add dimension at index 0):\n","tensor([[[-0.6028,  0.5040,  0.4118],\n","         [-0.6902, -0.3452, -1.0556]]])\n","Shape of unsqueezed tensor:  torch.Size([1, 2, 3])\n"]}]},{"cell_type":"code","source":["print(x.shape)\n","# Using transpose to swap dimensions\n","x_transposed = x.transpose(1, 2) # Swap dimensions 1 and 2\n","print(\"Transposed tensor:\")\n","print(x_transposed)\n","print(\"Shape of transposed tensor: \", x_transposed.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1dLOcxhSbPp","executionInfo":{"status":"ok","timestamp":1713579413163,"user_tz":-540,"elapsed":335,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"801d9df3-1c17-4751-884d-f74397c07e6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2, 3, 1])\n","Transposed tensor:\n","tensor([[[[-0.6028],\n","          [-0.6902]],\n","\n","         [[ 0.5040],\n","          [-0.3452]],\n","\n","         [[ 0.4118],\n","          [-1.0556]]]])\n","Shape of transposed tensor:  torch.Size([1, 3, 2, 1])\n"]}]},{"cell_type":"code","source":["print(x.shape)\n","# Using permute to rearrange dimenions\n","x_permuted = x.permute(2, 1, 3, 0)\n","print(\"Permuted tensor (rearrange dimensions):\")\n","print(x_permuted)\n","print(\"Shape of permuted tensor: \", x_permuted.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qR1ubEjaSsvK","executionInfo":{"status":"ok","timestamp":1713579518076,"user_tz":-540,"elapsed":308,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"636538c2-71c2-4e44-d3b7-361a8e338ab5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2, 3, 1])\n","Permuted tensor (rearrange dimensions):\n","tensor([[[[-0.6028]],\n","\n","         [[-0.6902]]],\n","\n","\n","        [[[ 0.5040]],\n","\n","         [[-0.3452]]],\n","\n","\n","        [[[ 0.4118]],\n","\n","         [[-1.0556]]]])\n","Shape of permuted tensor:  torch.Size([3, 2, 1, 1])\n"]}]},{"cell_type":"markdown","source":["# reshape() vs. view()\n","\n","## Difference between reshape() and view()\n","1. **Memory Layout Compatibility**:\n","    * **view()**: It requires the requested view to be contiguous(連続した) in memory. This means that the tensor and the desired new shape must preserve the order of the data such that each element is next to the previous element in the original tensor. If the original tensor is not contiguous, you will need to call **tensor.contiguous()** before calling **view()**.\n","    * **reshape()**: It can handle non-contiguous tensors directly by returning a view if possible, or otherwise copying the data to make it contiguous. This makes **reshape()** more versatile(多用途) if you're unsure about the memory layout of your tensor.\n","2. **Error Handling**:\n","    * **view()**: Throws an error if the tensor cannot be viewed in the new shape without altering its continuity.\n","    * **reshape()**: Attempts to return a tensor with the desired shape, and will <u>silently handle issues related to non-contiguity by copying the data</u> if necessary.\n","\n","\n","## When to Use `reshape()` vs. `view()`\n","* **Use `view()`**: When you are sure that the tensor is contiguous and you want to avoid any potential overhead from handling non-contiguous tensors. It's a bit stricter, enduring you're aware of the tensor's layout, which can help prevent subtle bugs in complex systems.\n","* **Use `reshape()`**: When you might be dealing with tensors that could be non-contiguous, or you're not sure about the tensor's memory layout. It's safer and more flexible because it handles the rearrangement of the data if necessary.\n","\n","\n","## Example to Illustrate\n","Consider a situation where you perform an operation that might make a tensor non-contiguous, like transposing dimensions. Here's how you might decide between **view()** and **reshape()**:"],"metadata":{"id":"a4JhAjjZT9TK"}},{"cell_type":"code","source":["import torch\n","\n","# Creating tensor\n","x = torch.randn(2, 3)\n","\n","# Transpose makes the tensor non-contiguous\n","x_t = x.transpose(0, 1)\n","\n","try:\n","    # This will fail because x_t is not contiguous\n","    x_view = x_t.view(6)\n","except RuntimeError as e:\n","    print(\"Error using veiw:\", e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5emGqaWTGWh","executionInfo":{"status":"ok","timestamp":1713582057451,"user_tz":-540,"elapsed":396,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"491851bb-78c2-4976-bc48-0aa5a46c105d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error using veiw: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"]}]},{"cell_type":"code","source":["# This will work because reshape handles non-contiguity\n","x_reshaped = x_t.reshape(6)\n","print(\"Reshaped tensor:\", x_reshaped)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMOLA0XQctJ0","executionInfo":{"status":"ok","timestamp":1713582113368,"user_tz":-540,"elapsed":268,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"e0ee2ec7-2318-42d9-89ae-4de203dbcd1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reshaped tensor: tensor([-1.4226,  0.4395,  0.9415, -2.6417,  0.6951, -0.3336])\n"]}]},{"cell_type":"markdown","source":["In this example, **reshape()** is the appropriate choice after transposing the tensor, as it seamlessly handles the non-contiguity. Use **view()** when you're dealing with simpler transformations or when you're certain of the tensor's continuity, such as after flattening operations that do not alter the order of the underlying data."],"metadata":{"id":"C85e8uCTdA2u"}},{"cell_type":"markdown","source":["# Importance of Continuity of data in Memory\n","\n","In PyTorch, the continuity of data in memory plays a crucial role because it directly affects how efficiently operations on tensors can be performed. Let's explore why data continuity is important and how it impacts tensor operations:\n","\n","\n","## 1. Efficiency of Memory Access\n","* **Contiguous tensors** store their elements in a single uninterrupted block of memory. This means that accessing the elements in sequence (the way many tensor operations do) is very fast because the elements are laid out sequentially in memory.\n","* **Non-contiguous tensors** do not store their elements in a single, linear block of memory. Instead, their storage maight skip over some parts of memory. This occurs, for example, after transposing a tensor or selecting a non-contiguous slice from a tensor. Accessing the elements of non-contiguous tensors can involve jumping around in memory, which is less efficient than linear memory access.\n","\n","\n","## 2. Optimization of Operations\n","* Many underlying libraries that PyTorch uses, such as those for BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package), are optimized for data that is contiguous in memory. These optimizations assume that data is stored in a predictable, sequential manner, which can significantly speed up operations like matrix multiplication, addition, and more.\n","\n","\n","## 3. Simplicity of Implementation\n","* Keeping data contiguous simplifies the implementation of complex operations. Developers writing low-level code can make assumptions about data layout that allow for simpler, faster code.\n","* Operations that change tensor shape, like reshaping, slicing, or transposing, can alter the contiguity of a tensor. Being able to handle these operations effectively while managing memory layout is a key part of designing efficient tensor computations.\n","\n","\n","## 4. Use of `view()** and Contiguity\n","* The **view()** operation in PyTorch is a light weight method to change the shape of a tensor without copying the data. It can <u>only be used when the new shape arrangement allows for a contiguous view of the data</u>. If this condition isn't met, attempting to use **view()** will result in an error.\n","** This restriction ensures that all **view()** operations are not only fast (as they avoid data copying) but also safe, in that they will not unintentionally lead to incorrect computations or inefficient memory access patterns.\n","\n","\n","## Practical Implications\n","When working with PyTorch, it's often important to check the contiguity of your tensors, especially after performing operations that might disrupt this property. You can check if a tensor is contiguous using **tensor.is_contiguous()** and make it contiguous with **tensor.contiguous()**. This can be particularly important before using **view()**, or when you are preparing tensors for operations that require high computational efficiency.\n","\n","<br>\n","\n","Understanding these aspects can help you optimize your PyTorch code for performance and correctness, especially in complex systems or deep learning models where large-scale data manipulation is common."],"metadata":{"id":"p_JUCOXAd-Za"}},{"cell_type":"markdown","source":["# Operation to break contiguity of data\n","\n","## 1. Transposing Dimensions\n","* When you transpose a tensor, the underlying data is not rearranged; only the way the data is accessed changes. This usually results in a non-contiguous tensor.\n","* Example: **x.transpose(0, 1)**\n","\n","## 2. Permuting Dimensions\n","* Similar to transposing, permuting the dimensions of a tensor changes how the data is indexed without moving the data itself, often resulting in a non-contiguous layout.\n","* Example: **x.permute(1, 0, 2)**\n","\n","## 3. Selecting Non-sequential Slices\n","* When you slice a tensor in a non-standard step size or in multiple dimensions, the result might not conver a continuous block of memory.\n","* Example: **x[:, 0::2]** (slicing every other element)\n","\n","## 4. Narrowing\n","* Narrowing a tensor to a subset of its original dimensions can create a non-contiguous tensor if the slice doesn't align with the original memory layout.\n","* Example: **x.narrow(0, 1, 3)**\n","\n","## 5. Advanced Indexing\n","* Using non-sequential indices or masks to index into a tensor can result in a non-contiguous tensor because the resulting tensor does not correspond to a regular block of the original tensor.\n","* Example: **x[[2, 0. 3], :]** (indexing with a non-sequenctial list of indices)\n","\n","## 6. Unfolding\n","* The unfold operation extracts sliding local blocks from a batched input tensor, which often results in non-contiguous output due to the way blocks are laid out in memory.\n","* Example: **x.unfold(dimension=0, size=2, step=1)**\n","\n","## 7. Expanding\n","* When you expand a tensor, the size of one or more dimensions is increased without copying data, leading to repeated usage of the same data in memory. This generally results in a non-contiguous tensor because the logical structure of the tensor no longer matches a linear progression in memory.\n","* Example: **x.expand(-1, 4)**\n","\n","## Practical Consideration\n","After performing any of these operations, if you need to ensure that a tensor is contiguous (for example, before using **view()**), you can use **tensor.contiguous()** to get a contiguous version of the tensor. This might involve copying the data if necessary. Always check tensor contiguity with **tensor.is_contiguous()** when performance and memory layout are critical concerns in your applidation."],"metadata":{"id":"NITJhq2Vi0Uw"}},{"cell_type":"markdown","source":["# squeeze() & unsqueeze()'s effect to contiguity\n","Using **tensor.squeeze()** and **tensor.unsqueeze()** generally **does not** lead to loss of contiguity in the tensor data. These operations are exceptions to some of the other reshaping methods because they only modify the shape of the tensor without changing the order or the layout of the underlying data. Let's clarify how these work:\n","\n","## tensor.unsuqeeze()\n","* **Adding a Dimension**: This operation adds a new dimension of size one at the specified index. Since it does not alter the order of existing elements and merely introduces an new axis, the data's contiguity is preserved. For example, if you have a tensor of shape **(3, 4)** and you use **unsqueeze(0)**, the new shape becomes **(1, 3, 4)**, but the underlying data layout in memory remains the same.\n","\n","## tensor.squeeze()\n","* **Removing Single-Dimension Entries**: This operation removes all dimensions of size one, or a specific dimension if specified. Removing these singleton dimensions does not require rearrangeing the data; it just changes how the tensor's shape is interpreted. Thus, the data remains contiguous as long as the tensor was contiguous before the operation.\n","\n","Here is a quick example to demonstrate that **unsqueese()** and **squeeze()** prserve contiguity:\n"],"metadata":{"id":"UYdKnAYRrp_s"}},{"cell_type":"code","source":["import torch\n","\n","# Create a contiguous tensor\n","x = torch.randn(2, 3)\n","print(x.shape)\n","\n","# Use unsuqeeze to add a dimension\n","x_unsqueezed = x.unsqueeze(0)\n","print(\"Is the unsqueeze tensor contiguous?\", x_unsqueezed.is_contiguous())\n","\n","# Use squeeze to remove single-dimension entries\n","x_squeezed = x_unsqueezed.squeeze(0)\n","print(\"Is the squeeze tensor contiguous?\", x_squeezed.is_contiguous())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SA1uI84Uc_-x","executionInfo":{"status":"ok","timestamp":1713589059919,"user_tz":-540,"elapsed":442,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"be7694ac-15f6-4bd5-da93-0cde40e20a22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3])\n","Is the unsqueeze tensor contiguous? True\n","Is the squeeze tensor contiguous? True\n"]}]},{"cell_type":"markdown","source":["In the example, both **unsqueeze()** and **squeeze()** retain the contiguity of the tensor. This behavior makes these operations very efficient for adjusting tensor shapes, particularly when preparing data for input to neural networks or when aligning tensor dimensions for operations like broadcasting."],"metadata":{"id":"wWjuZC9R3huZ"}},{"cell_type":"markdown","source":["# Order of Index (Dimension)\n","Understanding the order of dimensions (or indices) in tensors when performing operations like reshaping is fundamental in PyTorch, as well as in most other rensor-manipulating libraries.\n","\n","## Dimension Indexing in PyTorch\n","In PyTorch, tensors are indexed starting from 0. Here's how the dimensions are typically ordered:\n","* **0th Dimension**: This is often referred to as the \"batch\" dimension in machine learning contexts, where each element along this axis represents a separate instance in a batch of data.\n","* **1st Dimension**: In many applications, particularly those dealing with images, this could represent the \"channel\" dimension (e.g., RGB color channels in an image).\n","* **2nd and Higher Dimensions**: These usually represent the spatial dimensions (height, width) in images, or sequence length in time series or natural language data.\n","\n"],"metadata":{"id":"ELOT0iWD4zx4"}},{"cell_type":"markdown","source":["## Example of Operations\n","Let's illustrate this with a few examples to show how you would use these indices in practice with operations like **reshape**, **unsqueeze**, and **squeeze**\n","\n","1. **Reshaping a Tensor**:\n","    * Suppose you have a 2D tensor representing multiple data points, each with several features, and you want to reshape this into a high-dimensional tensor for a deep learning model."],"metadata":{"id":"vLO1l8q8-yuE"}},{"cell_type":"code","source":["import torch\n","\n","# A tensor woth shape (batch_size, features) = (10, 16)\n","x = torch.randn(10, 16)\n","\n","# Reshape the tensor to shape (batch_size, channels, height, width) = (10, 4, 2, 2)\n","x_reshaped = x.reshape(10, 4, 2, 2)\n","\n","print(\"Reshaped tensor shape: \",x_reshaped.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jOXTG_s3fsk","executionInfo":{"status":"ok","timestamp":1713591147237,"user_tz":-540,"elapsed":450,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"e65519b9-5c9d-431f-a83e-b16553f164b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reshaped tensor shape:  torch.Size([10, 4, 2, 2])\n"]}]},{"cell_type":"markdown","source":["2. **Adding and Removing Dimensions**:\n","    * You might want to add a singleton dimension to represent the channel for a grayscale image, or squeeze out unnecessary singleton dimensions after some operations."],"metadata":{"id":"GtE38qR6_fL3"}},{"cell_type":"code","source":["# Suppose x is a tensor representing ten grayscale images, shape=(10,28,28)\n","x = torch.randn(10, 28, 28)\n","print('original shape: ', x.shape)\n","\n","# Add a channel dimension\n","x_with_channel = x.unsqueeze(1) # New shape(10, 1, 28, 28)\n","print('shape after adding channel dimension: ', x_with_channel.shape)\n","\n","# Squeeze out the channel dimension\n","x_without_channel = x_with_channel.squeeze(1) # Returns to original shape = (10, 28, 28)\n","print('shape after squeezing channel dimension: ', x_without_channel.shape)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzaBZxI4_dcw","executionInfo":{"status":"ok","timestamp":1713591449307,"user_tz":-540,"elapsed":462,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"3e7f86c0-417f-42be-da79-de6f5d300729"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["original shape:  torch.Size([10, 28, 28])\n","shape after adding channel dimension:  torch.Size([10, 1, 28, 28])\n","shape after squeezing channel dimension:  torch.Size([10, 28, 28])\n"]}]},{"cell_type":"markdown","source":["**Index order is calculated by left side(0, 1, ,2, 3, ...).**\n"],"metadata":{"id":"LoAzCIFeAzFH"}},{"cell_type":"markdown","source":["## General Rule for Shape Operations\n","When you're working with these operations, the indices you provide follow the order in which dimensions are listed in your tensor's shape:\n","* **Index 0** refer to the first dimension (often the batch size).\n","* **Index 1** refers to the second dimension (often channels in image data or features in tabuler data).\n","* **Higer indices** continue in the order they appear.\n","\n","\n","\n","This consistent indexing method allows you to manipulate the shape and dimensions of tensors predictably. Always be sure that any new shape you <u>provide still accounts for the same total number of elements as the the original tensor</u> unless you're adding or removing singleton dimensions, which don't change the total element count."],"metadata":{"id":"hNo9izJFBUXu"}},{"cell_type":"markdown","source":["# How to manipulate `reshape()`\n","The **reshape** function in PyTorch is a powerful tool for changing the shape of tensors, and using **-1** as one of the dimensions during reshaping is a common and useful technique. The **-1** argument in the **reshape** function acts as a placeholder for an unspecified dimension, and it allows PyTorch to automatically calculate the necessary size for that dimension based on the other specified dimensions and the total number of elements in the tensor."],"metadata":{"id":"_v0PYDtdC5IU"}},{"cell_type":"markdown","source":["## How `-1` Works in `reshape`\n","When you use **-1** in the **reshape** function, PyTorch calculates the size of this dimension so that the total number of elements in the tensor remains constant. You can only use **-1** for one dimension, as having it in multiple places would lead to ambiguity i the shape.\n","\n","\n","Here's a step-by-step explanation:\n","1. **Calculate the Total Number of Elements**: First, PyTorch computes the total number of elements in the original tensor.\n","2. **Fixed Dimensions**: Then, it considers the other dimensions you've specified in the **reshape** call.\n","3. **Infer the `-1` Dimension**: PyTorch divides the total number of elements by the product of the specified dimensions to determine the size of the dimension given as **-1**."],"metadata":{"id":"YqzCyHn6DhTr"}},{"cell_type":"markdown","source":["## Example Usage of `reshape` with `-1`\n","Let's go through a few examples ti illustrate how you can use **reshape** with **-1**:"],"metadata":{"id":"Qp3zAO4lEh8D"}},{"cell_type":"code","source":["import torch\n","\n","# Create a tensor of shape (4, 6)\n","x = torch.arange(24).reshape(4, 6)\n","print(\"Original tensor shape:\", x.shape)\n","\n","# Reshape using -1 (infer one dimension)\n","# Reshape to have 2 rows and let PyTorch calculate the necessary number of culumns\n","y = x.reshape(2, -1)\n","# print(\"\\nReshaped tensor(2*?) where ? is calculated automatically:\")\n","print(y)\n","print(\"Reshaped tensor shape:\", y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJqY8XqZA9PX","executionInfo":{"status":"ok","timestamp":1713592760059,"user_tz":-540,"elapsed":272,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"15d26df0-3e37-414f-ec65-ee6d77dfb9bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor shape: torch.Size([4, 6])\n","tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n","        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n","Reshaped tensor shape: torch.Size([2, 12])\n"]}]},{"cell_type":"code","source":["# Reshape to have 3 columns and let PyTorch calculate the number of rows\n","z = x.reshape(-1, 3)\n","print(\"\\nReshaped tensor(?, 3) where ? is calculated automatically:\")\n","print(z)\n","print(\"Reshaped tensor shape:\", z.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q310Yf5JA9LB","executionInfo":{"status":"ok","timestamp":1713592813258,"user_tz":-540,"elapsed":342,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"ed81718c-169c-4d12-cf9e-27133fc93f86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Reshaped tensor(?, 3) where ? is calculated automatically:\n","tensor([[ 0,  1,  2],\n","        [ 3,  4,  5],\n","        [ 6,  7,  8],\n","        [ 9, 10, 11],\n","        [12, 13, 14],\n","        [15, 16, 17],\n","        [18, 19, 20],\n","        [21, 22, 23]])\n","Reshaped tensor shape: torch.Size([8, 3])\n"]}]},{"cell_type":"code","source":["# More complex rehsaping\n","# Suppose we want a 3D tensor with the last dimension being 4, PyTorch calculates the rest\n","print(\"original shape: \", x.shape)\n","w = x.reshape(2, 3, -1)\n","print(\"Reshaped tensor shape: \", w.shape)\n","\n","\n","# Reshape to (-1, 2, 6)\n","b = x.reshape(-1, 2, 6)\n","print(\"\\nReshaped tensor to (-1, 2, 6):\")\n","print(b)\n","print(\"Shape of reshaped tensor:\", b.shape)\n","\n","# Reshape to (4, -1, 3)\n","c = x.reshape(4, -1, 3)\n","print(\"\\nReshaped tensor to (4, -1, 3):\")\n","print(c)\n","print(\"Shape of reshaped tensor:\", c.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imcd5yQEAnMr","executionInfo":{"status":"ok","timestamp":1713593282998,"user_tz":-540,"elapsed":373,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"142973df-0700-49ae-f05c-6a364fd0f36f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["original shape:  torch.Size([4, 6])\n","Reshaped tensor shape:  torch.Size([2, 3, 4])\n","\n","Reshaped tensor to (-1, 2, 6):\n","tensor([[[ 0,  1,  2,  3,  4,  5],\n","         [ 6,  7,  8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15, 16, 17],\n","         [18, 19, 20, 21, 22, 23]]])\n","Shape of reshaped tensor: torch.Size([2, 2, 6])\n","\n","Reshaped tensor to (4, -1, 3):\n","tensor([[[ 0,  1,  2],\n","         [ 3,  4,  5]],\n","\n","        [[ 6,  7,  8],\n","         [ 9, 10, 11]],\n","\n","        [[12, 13, 14],\n","         [15, 16, 17]],\n","\n","        [[18, 19, 20],\n","         [21, 22, 23]]])\n","Shape of reshaped tensor: torch.Size([4, 2, 3])\n"]}]},{"cell_type":"markdown","source":["## Things to Keep in Mind\n","* The new shape must be compatible with the number of elements in the original tensor. the product of the new shape dimensions must equal the total number of elements.\n","* Using **-1** is especially useful when you don't know the exact size of a particular dimension or when the dimension size might change depending on the situation (e.g., variable batch sizes in data processing pipelines).\n","* You can only use **-1** for one dimension in the shape. If used for more than one dimension, PyTorch will throw an error due to the ambiguity in determining the sizes."],"metadata":{"id":"Tp5mp93gHyIF"}},{"cell_type":"markdown","source":["# How to use `view`\n","**view** function is similar to **reshape** but requires the tensor to be <u>contifuous</u> for the operation to work. As with **reshape**, **view** allows you to change the shape of a tensor without changing its data, but it operates directly on the tensor's storage if the requested view is compatible with the tensor's original stride."],"metadata":{"id":"VfGeC4jyJGqd"}},{"cell_type":"markdown","source":["## Basic Usage of `view`\n","Here are some practical examples using the **view** function to change the shape of tensors. I'll use a tensor with a shape of **(4, 6)** as in the previous examples:\n","\n","1. **Flattening a Tensor**:\n","    * You can flatten a tensor to a single dimension. This is often used in neural networks to transform a multi-dimensional input a flat vector for a fully connected layer.\n","2. **Changing to a Different 2D Shape**:\n","    * Transform the shape for compatiblility with different operations, such as feeding it into a neural newtwork layer that expects a certain input shape.\n","3. **Expanding to a 3D Tensor**:\n","    * Useful for adding a dimension, for instance, to create a batch or a channel dimension.\n","\n","\n","Let's demonstrate each with the **view** function:"],"metadata":{"id":"UHaF0gljJoAA"}},{"cell_type":"code","source":["import torch\n","\n","# Create a tensor of shape (4, 6)\n","x = torch.arange(24).reshape(4, 6)\n","print(x)\n","print(\"Original tensor shape:\", x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7RWI_wDAqen","executionInfo":{"status":"ok","timestamp":1713594244441,"user_tz":-540,"elapsed":288,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"556b44e6-bb85-4dfd-89c0-5301c3b3c0b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  1,  2,  3,  4,  5],\n","        [ 6,  7,  8,  9, 10, 11],\n","        [12, 13, 14, 15, 16, 17],\n","        [18, 19, 20, 21, 22, 23]])\n","Original tensor shape: torch.Size([4, 6])\n"]}]},{"cell_type":"code","source":["# Flattening the tensor\n","flat_x = x.view(-1)\n","print(flat_x)\n","print(\"Flattened tensor shape:\", flat_x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rHcKltAKxH5","executionInfo":{"status":"ok","timestamp":1713594176236,"user_tz":-540,"elapsed":9,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"e637e73a-347f-4cb5-9d99-21968b0f29bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19, 20, 21, 22, 23])\n","Flattened tensor shape: torch.Size([24])\n"]}]},{"cell_type":"code","source":["# Changing to a different 2D shape (12, 2)\n","x_view_2d = x.view(12, 2)\n","print(x_view_2d)\n","print(\"Reshaped tensor shape:\", x_view_2d.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5aP3RKpcLA-_","executionInfo":{"status":"ok","timestamp":1713594224107,"user_tz":-540,"elapsed":339,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"54dfa942-ef94-4370-815c-a805ca7df25a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  1],\n","        [ 2,  3],\n","        [ 4,  5],\n","        [ 6,  7],\n","        [ 8,  9],\n","        [10, 11],\n","        [12, 13],\n","        [14, 15],\n","        [16, 17],\n","        [18, 19],\n","        [20, 21],\n","        [22, 23]])\n","Reshaped tensor shape: torch.Size([12, 2])\n"]}]},{"cell_type":"code","source":["# Expanding to a 3D tensor (2, 4, 3)\n","x_view_3d = x.view(2, 4, 3)\n","print(x_view_3d)\n","print(\"Reshaped tensor shape:\", x_view_3d.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLp0R-BQLMpf","executionInfo":{"status":"ok","timestamp":1713594335963,"user_tz":-540,"elapsed":408,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"58c8f79d-bad7-4e32-ee77-d886e53732c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0,  1,  2],\n","         [ 3,  4,  5],\n","         [ 6,  7,  8],\n","         [ 9, 10, 11]],\n","\n","        [[12, 13, 14],\n","         [15, 16, 17],\n","         [18, 19, 20],\n","         [21, 22, 23]]])\n","Reshaped tensor shape: torch.Size([2, 4, 3])\n"]}]},{"cell_type":"markdown","source":["## Notes on Using `view`\n","* **Contiguity Requirement**: The tensor must be contiguous for **view** to work without errors. If a tensor operation makes the tensor non-contiguous (like transpose or cetrain slices), you may need to call **.contiguous()** before using **view**.\n","* **`view` vs. `reshape`**: Unlike **reshape**, which can handle non-contiguous tensors by possibly creating a copy, **view** will throw an error if it can't provide a view on the existing data due to contiguity issues.\n","\n","\n","Here's is how you can ensure contiguity:"],"metadata":{"id":"jl-bodckLs-Y"}},{"cell_type":"code","source":["# Suppose x was midified to be non-contiguous\n","print('Original tensor shape: ', x.shape)\n","x_transposed = x.transpose(0, 1)\n","print('transposed tensor: ')\n","print(x_transposed)\n","print(\"transposed tensor shape:\", x_transposed.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nrhawx1dLn9w","executionInfo":{"status":"ok","timestamp":1713594637932,"user_tz":-540,"elapsed":378,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"35a95b52-aea5-4f7a-e322-4907b7a906e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original tensor shape:  torch.Size([4, 6])\n","transposed tensor: \n","tensor([[ 0,  6, 12, 18],\n","        [ 1,  7, 13, 19],\n","        [ 2,  8, 14, 20],\n","        [ 3,  9, 15, 21],\n","        [ 4, 10, 16, 22],\n","        [ 5, 11, 17, 23]])\n","transposed tensor shape: torch.Size([6, 4])\n"]}]},{"cell_type":"code","source":["# Trying to view a non-contiguos tensor\n","try:\n","    non_contig_view = x_transposed.view(12, 2)\n","except Exception as e:\n","    print(e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVgD7FdwMsJ_","executionInfo":{"status":"ok","timestamp":1713594684525,"user_tz":-540,"elapsed":383,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"7dd96a40-17e2-4dc4-94dd-f315fadd04fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"]}]},{"cell_type":"code","source":["# Making it contiguous\n","x_transposed_contiguous = x_transposed.contiguous()\n","contig_vew = x_transposed_contiguous.view(12, 2)\n","print(contig_vew)\n","print('Contiguous and reshaped tensor shape: ', contig_vew.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYaFwLT4M9EB","executionInfo":{"status":"ok","timestamp":1713594772181,"user_tz":-540,"elapsed":520,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"685a3aa6-4bdd-4f71-d013-a90d8da78d6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  6],\n","        [12, 18],\n","        [ 1,  7],\n","        [13, 19],\n","        [ 2,  8],\n","        [14, 20],\n","        [ 3,  9],\n","        [15, 21],\n","        [ 4, 10],\n","        [16, 22],\n","        [ 5, 11],\n","        [17, 23]])\n","Contiguous and reshaped tensor shape:  torch.Size([12, 2])\n"]}]},{"cell_type":"markdown","source":["This example demonstrates the importance of ensuring tensor contiguity when using **view** and how to troubleshoot and fix issues related to non-contiguous tensors."],"metadata":{"id":"gFLF2wyjNdjz"}},{"cell_type":"code","source":[],"metadata":{"id":"XA3jgmt2NPKR"},"execution_count":null,"outputs":[]}]}