{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLFGT26LS4bv7XMSwO5I+q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer_having_features_for_TimeSeries_Forecasting\n","* this code was made by chat-GPT"],"metadata":{"id":"mCQdjBjTaxkl"}},{"cell_type":"markdown","source":["For time series forecasting, especially when predicting future electricity consumption based on multiple features, deep learning models can significantly benefit from proper feature vectorization. This process involves transforming your raw data into a format that the neural network can effectively learn from. Given your scenario with 10 features, here are several strategies to vectorize these features for deep learning models:\n","\n","### 1. **Feature Scaling**\n","\n","First and foremost, normalize or standardize your features. This is crucial for models like neural networks to converge quickly. You can use Min-Max scaling to normalize the data or Z-score normalization to standardize it.\n","\n","- **Normalization (Min-Max Scaling)**: Scales the features to a fixed range, usually [0, 1].\n","- **Standardization (Z-score normalization)**: Scales the features so they have the properties of a standard normal distribution with a mean of 0 and a standard deviation of 1.\n","\n","### 2. **Sequence Windowing**\n","\n","For time series data, it's important to structure your input data into sequences that the model can learn from. This is often done by creating \"windows\" of past observations to predict future values.\n","\n","- **Fixed Windowing**: Create fixed-size input sequences (windows) of your 10 features. For instance, use the past 24 hours of data (assuming hourly sampling) to predict the next hour's electricity consumption.\n","- **Sliding Windows**: Similar to fixed windowing but the window slides by a certain step. For example, you might slide by one hour at a time, creating overlapping windows of data.\n","\n","### 3. **Time Embeddings**\n","\n","If your data includes explicit time stamps (e.g., hour of the day, day of the week), you can convert these into cyclical features using sine and cosine transformations. This helps the model capture time-based patterns like daily or weekly cycles.\n","\n","### 4. **Feature Embeddings for Categorical Data**\n","\n","If any of your 10 features are categorical (e.g., type of day: holiday/weekend/workday), consider using embeddings to convert these categories into continuous vectors. This can be more effective than one-hot encoding for models to capture the nuances of categorical data.\n","\n","### 5. **Lag Features**\n","\n","Create features that are lagged versions of the existing features. For instance, the electricity consumption from the previous day (or the same hour the previous day) can be a powerful feature for predicting future consumption.\n","\n","### 6. **Rolling Window Statistics**\n","\n","Generate statistical features based on rolling windows, such as the mean, median, variance, or sum of the past N hours/days. These features can capture trends and seasonality in the data.\n","\n","### 7. **Differencing**\n","\n","For non-stationary time series data, differencing can help stabilize the mean of the time series by removing changes in the level of a time series, and thus eliminate (or reduce) trend and seasonality.\n","\n","### Incorporating into a Deep Learning Model\n","\n","Once you've vectorized your features using the strategies above, you can feed them into various types of deep learning models suitable for time series forecasting, such as:\n","\n","- **Recurrent Neural Networks (RNNs)**: Good for capturing temporal dependencies.\n","- **Long Short-Term Memory (LSTM) networks**: A special kind of RNNs, effective in learning long-term dependencies.\n","- **Gated Recurrent Units (GRUs)**: Similar to LSTMs, but simpler and often faster to train.\n","- **Convolutional Neural Networks (CNNs)**: Not just for image data; 1D CNNs can be effective for sequence data.\n","- **Transformer models**: Though originally designed for natural language processing, transformers can be adapted for time series forecasting by treating the time series data as a sequence.\n","\n","Remember, the effectiveness of each vectorization technique and model architecture can vary based on the specifics of your dataset and the nature of the forecasting problem. It's often beneficial to experiment with different approaches and combinations thereof."],"metadata":{"id":"KlKR8vBzcdQq"}},{"cell_type":"markdown","source":["## **tips: When do we need to normailze target variable?**\n","The need to normalize a target variable in time series (or any other type of data) largely depends on its characteristics and the modeling approach you're using. Here are types of target variables that often require normalization:\n","\n","1. **Continuous Variables with Large Range**: If your target variable is a continuous variable that spans a large range of values, normalization can help to ensure that the optimization algorithm works efficiently. This is especially true for deep learning models, where having targets on a similar scale can significantly impact the convergence rate and stability of the learning process.\n","\n","2. **Skewed Variables**: For target variables that are highly skewed, normalization (or even log transformation, which is a form of normalization) can help make the distribution more symmetric, improving model performance by making it easier for the model to learn the underlying patterns.\n","\n","3. **Variables with Different Units and Scales**: In the context of multivariate time series forecasting, where you might be predicting multiple targets, normalization ensures that all variables contribute equally to the error term. Without normalization, a variable with a large scale can dominate the gradient updates, potentially leading to suboptimal performance.\n","\n","4. **High Magnitude Variables**: Variables with values that have a high magnitude can lead to numerical instability in deep learning models due to the way floating-point arithmetic is handled in computers. Normalizing these variables to a lower range can help prevent issues like overflow, underflow, or vanishing/exploding gradients.\n","\n","### When You Might Not Need to Normalize:\n","- **Binary or Categorical Targets**: For classification tasks where the target variable is binary or categorical (after being one-hot encoded or otherwise transformed), normalization of the target variable itself is not typically necessary. The focus would instead be on the features.\n","\n","- **Targets with Narrow Range**: If the target variable inherently falls within a narrow range and you're using a model that's less sensitive to the scale of the input (like decision trees or certain ensemble methods), normalization might not be necessary.\n","\n","- **Count Data with Low Variance**: If you're dealing with count data that doesn't vary widely, normalization might not offer significant benefits. However, for highly skewed count data, transformations like log scaling can still be beneficial.\n","\n","It’s important to consider the nature of your target variable and the requirements of your modeling approach when deciding on normalization. Also, the decision to normalize should be guided by experimentation and validation on your specific dataset, as the benefits can vary depending on the context and the peculiarities of the data at hand."],"metadata":{"id":"18IFOIub7HXY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6YAqM0fL1k9"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## Step 1: Data Preparation\n","First, you need to prepare your dataset. This includes loading your data, normalizing it, and creating input sequences and their corresponding labels."],"metadata":{"id":"X9C4J6cFeHZJ"}},{"cell_type":"markdown","source":["### Generate Sample Data\n","This data will consists of 10 features, with each row representing an hourly record."],"metadata":{"id":"5KU11PHA7RmS"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","def generate_sample_data(num_records=1000):\n","    # Generate random data for 10 features\n","    data = np.random.rand(num_records, 10)\n","\n","    # Assume the last feature is related to electricity consumption\n","    # and use it to create a target variable\n","    # The actual consumption is some combination of the features plus noise\n","    consumption = data[:, -1] * 0.5 + np.random.normal(0, 0.02, size=num_records)\n","\n","    return pd.DataFrame(data, columns=[f'feature{i}' for i in range(1, 11)]), consumption\n","\n","features, consumption = generate_sample_data()"],"metadata":{"id":"7vxcj4An7ROT","executionInfo":{"status":"ok","timestamp":1711971802430,"user_tz":-540,"elapsed":244,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"Rs7_lt7L7cDw","executionInfo":{"status":"ok","timestamp":1711971805485,"user_tz":-540,"elapsed":299,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"79b77bbd-0c28-44f4-b488-086d49e64052"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n","0    0.982371  0.159263  0.183412  0.470824  0.870993  0.562760  0.508709   \n","1    0.251807  0.411744  0.414182  0.458533  0.688538  0.867947  0.057313   \n","2    0.816489  0.104313  0.114748  0.048298  0.200422  0.176816  0.347866   \n","3    0.326579  0.606607  0.012005  0.365808  0.002051  0.165458  0.447850   \n","4    0.189004  0.115845  0.333425  0.551429  0.988902  0.418716  0.425996   \n","..        ...       ...       ...       ...       ...       ...       ...   \n","995  0.640417  0.585508  0.406552  0.329525  0.179808  0.060143  0.634498   \n","996  0.896951  0.982807  0.961668  0.592230  0.224200  0.106355  0.993925   \n","997  0.748100  0.890800  0.477564  0.294029  0.778404  0.503768  0.086030   \n","998  0.009057  0.087722  0.821955  0.481308  0.335301  0.249293  0.506858   \n","999  0.858440  0.754427  0.018366  0.166498  0.474035  0.658680  0.715126   \n","\n","     feature8  feature9  feature10  \n","0    0.086757  0.961986   0.347116  \n","1    0.490769  0.413978   0.010880  \n","2    0.659270  0.054294   0.860276  \n","3    0.535087  0.152929   0.745216  \n","4    0.059333  0.765820   0.181783  \n","..        ...       ...        ...  \n","995  0.779200  0.857763   0.724308  \n","996  0.308390  0.033971   0.391046  \n","997  0.887750  0.655807   0.036812  \n","998  0.083040  0.617180   0.380527  \n","999  0.816243  0.049219   0.018483  \n","\n","[1000 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-0a6475f2-8a9d-4a10-b959-5c83815ad1d3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature1</th>\n","      <th>feature2</th>\n","      <th>feature3</th>\n","      <th>feature4</th>\n","      <th>feature5</th>\n","      <th>feature6</th>\n","      <th>feature7</th>\n","      <th>feature8</th>\n","      <th>feature9</th>\n","      <th>feature10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.982371</td>\n","      <td>0.159263</td>\n","      <td>0.183412</td>\n","      <td>0.470824</td>\n","      <td>0.870993</td>\n","      <td>0.562760</td>\n","      <td>0.508709</td>\n","      <td>0.086757</td>\n","      <td>0.961986</td>\n","      <td>0.347116</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.251807</td>\n","      <td>0.411744</td>\n","      <td>0.414182</td>\n","      <td>0.458533</td>\n","      <td>0.688538</td>\n","      <td>0.867947</td>\n","      <td>0.057313</td>\n","      <td>0.490769</td>\n","      <td>0.413978</td>\n","      <td>0.010880</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.816489</td>\n","      <td>0.104313</td>\n","      <td>0.114748</td>\n","      <td>0.048298</td>\n","      <td>0.200422</td>\n","      <td>0.176816</td>\n","      <td>0.347866</td>\n","      <td>0.659270</td>\n","      <td>0.054294</td>\n","      <td>0.860276</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.326579</td>\n","      <td>0.606607</td>\n","      <td>0.012005</td>\n","      <td>0.365808</td>\n","      <td>0.002051</td>\n","      <td>0.165458</td>\n","      <td>0.447850</td>\n","      <td>0.535087</td>\n","      <td>0.152929</td>\n","      <td>0.745216</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.189004</td>\n","      <td>0.115845</td>\n","      <td>0.333425</td>\n","      <td>0.551429</td>\n","      <td>0.988902</td>\n","      <td>0.418716</td>\n","      <td>0.425996</td>\n","      <td>0.059333</td>\n","      <td>0.765820</td>\n","      <td>0.181783</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>0.640417</td>\n","      <td>0.585508</td>\n","      <td>0.406552</td>\n","      <td>0.329525</td>\n","      <td>0.179808</td>\n","      <td>0.060143</td>\n","      <td>0.634498</td>\n","      <td>0.779200</td>\n","      <td>0.857763</td>\n","      <td>0.724308</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>0.896951</td>\n","      <td>0.982807</td>\n","      <td>0.961668</td>\n","      <td>0.592230</td>\n","      <td>0.224200</td>\n","      <td>0.106355</td>\n","      <td>0.993925</td>\n","      <td>0.308390</td>\n","      <td>0.033971</td>\n","      <td>0.391046</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>0.748100</td>\n","      <td>0.890800</td>\n","      <td>0.477564</td>\n","      <td>0.294029</td>\n","      <td>0.778404</td>\n","      <td>0.503768</td>\n","      <td>0.086030</td>\n","      <td>0.887750</td>\n","      <td>0.655807</td>\n","      <td>0.036812</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>0.009057</td>\n","      <td>0.087722</td>\n","      <td>0.821955</td>\n","      <td>0.481308</td>\n","      <td>0.335301</td>\n","      <td>0.249293</td>\n","      <td>0.506858</td>\n","      <td>0.083040</td>\n","      <td>0.617180</td>\n","      <td>0.380527</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>0.858440</td>\n","      <td>0.754427</td>\n","      <td>0.018366</td>\n","      <td>0.166498</td>\n","      <td>0.474035</td>\n","      <td>0.658680</td>\n","      <td>0.715126</td>\n","      <td>0.816243</td>\n","      <td>0.049219</td>\n","      <td>0.018483</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 10 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a6475f2-8a9d-4a10-b959-5c83815ad1d3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0a6475f2-8a9d-4a10-b959-5c83815ad1d3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0a6475f2-8a9d-4a10-b959-5c83815ad1d3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6eea2cdb-3e27-4e6c-83da-daed09b9a11c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6eea2cdb-3e27-4e6c-83da-daed09b9a11c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6eea2cdb-3e27-4e6c-83da-daed09b9a11c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_cf785f8f-10bd-4634-a062-9ed9789b4f4e\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('features')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_cf785f8f-10bd-4634-a062-9ed9789b4f4e button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('features');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"features","summary":"{\n  \"name\": \"features\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"feature1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27971266138328593,\n        \"min\": 0.0011080262304651312,\n        \"max\": 0.9992344805031821,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.30927649484481934,\n          0.664583109204249,\n          0.23181391793280826\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2877406836496741,\n        \"min\": 0.00038515074738565414,\n        \"max\": 0.9997864387066695,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.8615067268177601,\n          0.7924864972522105,\n          0.3344714152962307\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2877654809222012,\n        \"min\": 0.0006565214592899604,\n        \"max\": 0.9997106298144672,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.337995144354338,\n          0.8410182031523357,\n          0.30579126809486123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28919445472060734,\n        \"min\": 0.000951459119968634,\n        \"max\": 0.9997522309574156,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.10672379577302038,\n          0.021952105762874874,\n          0.7195069013375063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2938469976846693,\n        \"min\": 0.00015119030072885398,\n        \"max\": 0.9979843233194875,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.2940983913430745,\n          0.40462533022134506,\n          0.9681811477217577\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28152381623861356,\n        \"min\": 0.00013001127079648178,\n        \"max\": 0.9998672005028749,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.31509732610908725,\n          0.9784382419347061,\n          0.2800784434796467\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28876690750766854,\n        \"min\": 0.000261252942947543,\n        \"max\": 0.9987296497208092,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.7184853723581804,\n          0.06256309884714695,\n          0.5883005195480489\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature8\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28111218800271587,\n        \"min\": 0.0002705754389797921,\n        \"max\": 0.9998750991412935,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.1695419587243815,\n          0.7855540942895888,\n          0.9885585037329886\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature9\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28738400018878796,\n        \"min\": 0.0005392141828836072,\n        \"max\": 0.9998426533190269,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.3168632336748509,\n          0.1261129509075627,\n          0.15757944347857888\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28452089952431625,\n        \"min\": 0.0020714291854075917,\n        \"max\": 0.9977736006191767,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.07658642450446862,\n          0.8246391606919152,\n          0.6590823151184065\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["consumption[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VkfXDRJ7cBJ","executionInfo":{"status":"ok","timestamp":1711971807636,"user_tz":-540,"elapsed":287,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"25a25638-c792-4324-d91c-410cae91ab09"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.20434603, 0.01926492, 0.41995442, 0.38108821, 0.07795549,\n","       0.30348063, 0.25940669, 0.01015272, 0.38136406, 0.16268581,\n","       0.04311213, 0.19002552, 0.20952929, 0.28378454, 0.09681489,\n","       0.28113735, 0.0799279 , 0.30924109, 0.34146173, 0.19844026])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["### Data Preprocessing\n","For LSTM models, we need to format our data into sequences. We'll also split the data into training and testing sets."],"metadata":{"id":"juzMJkm-8d4c"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import math"],"metadata":{"id":"g8qeD1NP8dla","executionInfo":{"status":"ok","timestamp":1711971810027,"user_tz":-540,"elapsed":1,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# suqenceデータの作成\n","# twを5にすると、3次元のデータ構造で、x方向に10個(特徴量数), y方向に5個(時系列数), z方向に995個（len(data_normalized) - 5)の\n","# データが作られる。これは、LSTM用に、各yに対して5時点分のsequenceデータを用意している作業\n","\n","def create_sequences(features, targets, time_steps=1):\n","    Xs, ys = [], []\n","    for i in range(len(features) - time_steps):\n","        Xs.append(features[i:(i + time_steps)])\n","        ys.append(targets[i + time_steps])\n","    return np.array(Xs), np.array(ys)\n","\n","# Normalize data\n","scaler = MinMaxScaler()\n","features_scaled = scaler.fit_transform(features)\n","\n","# Create sequences\n","time_steps = 5\n","X, y = create_sequences(features_scaled, consumption, time_steps)"],"metadata":{"id":"5acq2Jjc8diS","executionInfo":{"status":"ok","timestamp":1711971811335,"user_tz":-540,"elapsed":3,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1iQDx3l68dfq","executionInfo":{"status":"ok","timestamp":1711971813741,"user_tz":-540,"elapsed":8,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"94dac385-7dd4-4d9c-b6a9-daec4961f97e"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(995, 5, 10)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtZkLfeL8ddU","executionInfo":{"status":"ok","timestamp":1711971814830,"user_tz":-540,"elapsed":2,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"ad5bb4d4-466b-45a8-8e21-764658240683"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(995,)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Convert to Pytorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n","\n","# Create TensorDatasets and DataLoaders\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"RlHF56RY8da6","executionInfo":{"status":"ok","timestamp":1711971816419,"user_tz":-540,"elapsed":8,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Step2: Transformer Model Setup\n","Unlike traditional Transformers used in NLP, for time series forecasting, we focus more on **the encoder part**. We'll simplify the implementation to make it easier to understand.\n","<br><br>\n","Fist, difine a **Positional Encoding layer** to add information about the position of each time step in the input sequence, which helps the model distingish the order of data points."],"metadata":{"id":"merXDpOG_M4g"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0).transpose(0, 1))\n","\n","    def forward(self, x):\n","        return x + self.pe[:x.size(0), :]"],"metadata":{"id":"YR3mKK1KzJtR","executionInfo":{"status":"ok","timestamp":1711971819635,"user_tz":-540,"elapsed":7,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["Next, we define the Transformer model. We'll simplify the architecture to inculde a single Transformer Encoder layer."],"metadata":{"id":"x-86PVenzNNL"}},{"cell_type":"code","source":["class TransformerModel(nn.Module):\n","    def __init__(self, input_size=10, num_layers=1, nhead=2, d_model=128, dim_feedforward=512, dropout=0.1):\n","        super().__init__()\n","        self.pos_encoder = PositionalEncoding(d_model, max_len=5000)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n","                                                        dim_feedforward=dim_feedforward, dropout=dropout)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        self.encoder = nn.Linear(input_size, d_model)\n","        self.d_model = d_model\n","        self.decoder = nn.Linear(d_model, 1)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","\n","    def forward(self, src):\n","        src = self.encoder(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src)\n","        output = self.decoder(output)\n","        return output.squeeze(-1)\n","\n","\n","# class TransformerModel(nn.Module):\n","#     def __init__(self, input_size=10, num_layers=1, nhead=2, d_model=512, dim_feedforward=2048, dropout=0.1):\n","#         super(TransformerModel, self).__init__()\n","#         self.model_type = 'Transformer'\n","#         self.pos_encoder = PositionalEncoding(d_model, max_len=5000)\n","#         self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n","#                                                         dim_feedforward=dim_feedforward, dropout=dropout)\n","#         self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","#         self.encoder = nn.Linear(input_size, d_model)\n","#         self.d_model = d_model\n","#         self.decoder = nn.Linear(d_model, 1)\n","#         self.init_weights()\n","\n","#     def generate_attention_mask(self, sz, device):\n","#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","#         return mask.to(device)\n","\n","#     def init_weights(self):\n","#         initrange = 0.1\n","#         self.encoder.weight.data.uniform_(-initrange, initrange)\n","#         self.decoder.weight.data.uniform_(-initrange, initrange)\n","#         self.decoder.bias.data.zero_()\n","\n","#     def forward(self, src):\n","#         src = self.encoder(src) * math.sqrt(self.d_model)\n","#         src = self.pos_encoder(src)\n","\n","#         # Generate attention mask dynamically based on the batch size and sequence length\n","#         batch_size, seq_len, _ = src.size()\n","#         src_mask = self.generate_attention_mask(seq_len, src.device)\n","\n","#         output = self.transformer_encoder(src, src_mask)\n","#         output = self.decoder(output)\n","#         return output"],"metadata":{"id":"MKBPPuH8zNjz","executionInfo":{"status":"ok","timestamp":1711971821937,"user_tz":-540,"elapsed":338,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["## Step3: Initializing the Model\n","Initialize the Transformer model with appropriate parameters. Given tha we're working with time series data and not text, you might need to adjust the parameters like `d_model` and `nhead` depending on your dataset's characteristics."],"metadata":{"id":"OFq7mzAfzXM0"}},{"cell_type":"code","source":["# Adjusti the parameters according to your dataset and model complexity\n","input_size = 10 # Number of features\n","d_model = 512 # Embedding dimentions\n","nhead = 2 # Number of heads in the multi-head attention models\n","num_layers = 1 # Number of Transformer blocks\n","dim_feedforward = 2048 # Dimension of the feedforward networkd model in nn.TransformerEncoder\n","dropout = 0.1 # Dropout rate\n","\n","model = TransformerModel(input_size=input_size,\n","                         num_layers=num_layers,\n","                         nhead=nhead,\n","                         d_model=d_model,\n","                         dim_feedforward=dim_feedforward,\n","                         dropout=dropout)\n","\n","# Assuming you have aGPU, you would want to move your model to GPU.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81PqIYrFzZzd","executionInfo":{"status":"ok","timestamp":1711971826092,"user_tz":-540,"elapsed":290,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"429108b3-295a-4adb-f45d-8836ec9fe03d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"markdown","source":["## Step4: Training the Model\n","Define your training loop, including loss function and optimizer. Training a Transformer model follows the same PyTorch training loop pattern as other models.\n","<br><br>\n","Remember to create a source mask for the Transformer, as it uses self-attention mechanisms that need to know where padding or future tokens are:"],"metadata":{"id":"MkzCnyBMCmeq"}},{"cell_type":"code","source":["epochs = 5\n","\n","for epoch in range(epochs):\n","    model.train()  # Ensure the model is in training mode\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.unsqueeze(-1))\n","        loss.backward()\n","        optimizer.step()\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-JbsRyD7b-R","executionInfo":{"status":"ok","timestamp":1711971849491,"user_tz":-540,"elapsed":20473,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"cf24ed64-9da6-4bf2-a186-1272ebac8277"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([28, 1])) that is different to the input size (torch.Size([28, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.1722\n","Epoch 2, Loss: 0.1195\n","Epoch 3, Loss: 0.1595\n","Epoch 4, Loss: 0.0796\n","Epoch 5, Loss: 0.0730\n"]}]},{"cell_type":"markdown","source":["## Step5: Evaluating the Model\n","After training, you should evaluate the model's performance on the test set. This basic example doesn't include evaluation steps, but you would typically predict on the test set and compare it against the true values using a suitable metric (e.g., MSE for regression tasks)."],"metadata":{"id":"nechAds4F_Ya"}},{"cell_type":"code","source":["# Ensure the model is in evaluation mode\n","model.eval()\n","\n","test_loss = 0.0\n","predictions = []\n","targets_list = []\n","\n","# No gradient updates needed for testing\n","with torch.no_grad():\n","    for inputs, targets in test_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, targets.unsqueeze(-1))\n","        test_loss += loss.item() * inputs.size(0)\n","\n","        # Store predictions and targets to evaluate further metrics (if necessary)\n","        predictions.extend(outputs.view(-1).cpu().numpy())\n","        targets_list.extend(targets.view(-1).cpu().numpy())\n","\n","# Calculate average loss over all test data\n","test_loss /= len(test_loader.dataset)\n","\n","print(f\"Test Loss: {test_loss:.4f}\")\n","\n","# Optionally, calculate additional metrics like MAE, RMSE, etc., using predictions and targets_list\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7wVTtHbeD0b","executionInfo":{"status":"ok","timestamp":1711971857675,"user_tz":-540,"elapsed":272,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"610aedab-d97f-480d-f834-f82b7ea26883"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.0213\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([7, 1])) that is different to the input size (torch.Size([7, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]}]},{"cell_type":"code","source":["### erroro code\n","# with torch.no_grad():\n","#     predictions = []\n","#     for inputs, _ in test_loader:\n","#         predictions.append(model(inputs).numpy())\n","\n","# # flatten the list of predictions\n","# predictions = np.concatenate(predictions, axis=0)\n","\n","# loss = criterion(torch.tensor(predictions), torch.tensor(y_test))\n","# print(f\"Test Loss: {loss.item():.4f}\")"],"metadata":{"id":"ElwmMcL-kR4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### add validation step in Training Loop"],"metadata":{"id":"nRu2ZBS8eYny"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import math\n","\n","# Assuming the generate_sample_data and other functions remain unchanged\n","\n","features, consumption = generate_sample_data()\n","scaler = MinMaxScaler()\n","features_scaled = scaler.fit_transform(features)\n","time_steps = 5\n","X, y = create_sequences(features_scaled, consumption, time_steps)\n","\n","# Adjusted Split: First split into train+val and test, then split train+val into train and val\n","X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2\n","\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n","y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Assuming TransformerModel class remains unchanged\n","\n","model = TransformerModel(input_size=10, num_layers=1, nhead=2, d_model=128, dim_feedforward=512, dropout=0.1)\n","model.to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","epochs = 5\n","\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.unsqueeze(-1))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * inputs.size(0)\n","    train_loss /= len(train_loader.dataset)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets.unsqueeze(-1))\n","            val_loss += loss.item() * inputs.size(0)\n","    val_loss /= len(val_loader.dataset)\n","\n","    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOz3g0pjGVMx","executionInfo":{"status":"ok","timestamp":1711971899683,"user_tz":-540,"elapsed":1226,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"cac822d6-66b8-4c74-888b-30c330e233eb"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([21, 1])) that is different to the input size (torch.Size([21, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([7, 1])) that is different to the input size (torch.Size([7, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Training Loss: 0.4233, Validation Loss: 0.2043\n","Epoch 2, Training Loss: 0.1078, Validation Loss: 0.0365\n","Epoch 3, Training Loss: 0.0417, Validation Loss: 0.0239\n","Epoch 4, Training Loss: 0.0347, Validation Loss: 0.0280\n","Epoch 5, Training Loss: 0.0309, Validation Loss: 0.0204\n"]}]},{"cell_type":"markdown","source":["**Explanation:**\n","* **Dataset Splitting**: Adjusted to create a validation set from the original training data, ensuring you have separate training, validation, and test datasets.\n","* **Validation Loop**: After each training epoch, the model is evaluated on the validation set. The **model.eval()** call disables dropout and batch normalization during this evaluation phase, and torch.no_grad() ensures that gradients are not computed, reducing memory usage and speeding up computation.\n","* **Reporting**: The average training and validation losses are reported after each epoch, allowing you to monitor the model's performance and overfitting.\n","<br><br>\n","Remember, this adjustment uses part of your original training data for validation. If you have a separate validation dataset, you can skip the additional splitting and directly use your data."],"metadata":{"id":"lLeBbSrpenPF"}},{"cell_type":"code","source":["### test loop\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","test_loss = 0.0\n","predictions = []\n","targets_list = []\n","\n","# No gradient updates needed for testing\n","with torch.no_grad():\n","    for inputs, targets in test_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, targets.unsqueeze(-1))\n","        test_loss += loss.item() * inputs.size(0)\n","\n","        # Store predictions and targets to evaluate further metrics (if necessary)\n","        predictions.extend(outputs.view(-1).cpu().numpy())\n","        targets_list.extend(targets.view(-1).cpu().numpy())\n","\n","# Calculate average loss over all test data\n","test_loss /= len(test_loader.dataset)\n","\n","print(f\"Test Loss: {test_loss:.4f}\")\n","\n","# Optionally, calculate additional metrics like MAE, RMSE, etc., using predictions and targets_list\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vICCn_T9GTjq","executionInfo":{"status":"ok","timestamp":1711972007300,"user_tz":-540,"elapsed":272,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"bef2cdb7-f3d7-4c0a-e14c-f19e4f631c9e"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.0251\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wPNO-OLIGfES","executionInfo":{"status":"ok","timestamp":1711972011809,"user_tz":-540,"elapsed":266,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["# Transformer by gpt4"],"metadata":{"id":"fbCeR7sk1puk"}},{"cell_type":"markdown","source":["1. **Generate Sample Data**: Create synthetic electricity consumption data with 10 features as specified.\n","2. **Preprocessing**: Prepare the data for training, including normalization.\n","3. **PyTorch Dataset and DataLoader**: Implement custom dataset and dataloader for batching.\n","4. **Model Definition**: Define a Transformer model suitable for time series forecasting.\n","5. **Training and Validation**: Set up the training loop and validate the model on test data."],"metadata":{"id":"jDR9la5w1wD0"}},{"cell_type":"markdown","source":["## Step 1 & 2: Generate Sample Data and Preprocessing"],"metadata":{"id":"g9ov99oC1_s8"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# import warnings\n","# warnings.filterwarnings(\"ignore\")\n","\n","np.random.seed(42)  # For reproducibility\n","\n","# Generate a DataFrame with datetime information\n","num_hours = 365 * 24  # A year's worth of hourly data\n","date_rng = pd.date_range(start='1/1/2020', end='31/12/2020', freq='H')\n","df = pd.DataFrame(date_rng, columns=['date'])\n","df['weekday'] = df['date'].dt.weekday\n","df['hour'] = df['date'].dt.hour\n","df['season'] = df['date'].dt.month % 12 // 3 + 1\n","\n","# Generate synthetic features and target variable\n","for i in range(7):  # Additional 7 features\n","    df[f'feature_{i}'] = np.random.rand(len(df))\n","df['electricity_consumption'] = np.random.rand(len(df)) * 100  # Target variable"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUHtBOv-2iQ_","executionInfo":{"status":"ok","timestamp":1711968517146,"user_tz":-540,"elapsed":258,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"96124bdc-cccb-4d83-817b-1eca36577290"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-f0ae24ceee4b>:11: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n","  date_rng = pd.date_range(start='1/1/2020', end='31/12/2020', freq='H')\n"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"YUAFiDXwQs1l","executionInfo":{"status":"ok","timestamp":1711968519663,"user_tz":-540,"elapsed":7,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"45a0612e-9101-4de7-a936-2b187e4a2cc5"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 date  weekday  hour  season  feature_0  feature_1  feature_2  \\\n","0 2020-01-01 00:00:00        2     0       1   0.374540   0.671368   0.409980   \n","1 2020-01-01 01:00:00        2     1       1   0.950714   0.523158   0.838483   \n","2 2020-01-01 02:00:00        2     2       1   0.731994   0.898639   0.185176   \n","3 2020-01-01 03:00:00        2     3       1   0.598658   0.164393   0.554842   \n","4 2020-01-01 04:00:00        2     4       1   0.156019   0.804109   0.722233   \n","\n","   feature_3  feature_4  feature_5  feature_6  electricity_consumption  \n","0   0.421576   0.137686   0.120749   0.616654                 1.923384  \n","1   0.280547   0.260339   0.520433   0.003229                47.550482  \n","2   0.895044   0.489540   0.095159   0.792586                26.352564  \n","3   0.332239   0.061339   0.256357   0.243121                53.995885  \n","4   0.578596   0.095686   0.451709   0.299217                17.865769  "],"text/html":["\n","  <div id=\"df-bb7ef833-f981-4a1b-aee8-fe533e2d6d76\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>weekday</th>\n","      <th>hour</th>\n","      <th>season</th>\n","      <th>feature_0</th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>feature_3</th>\n","      <th>feature_4</th>\n","      <th>feature_5</th>\n","      <th>feature_6</th>\n","      <th>electricity_consumption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-01-01 00:00:00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.374540</td>\n","      <td>0.671368</td>\n","      <td>0.409980</td>\n","      <td>0.421576</td>\n","      <td>0.137686</td>\n","      <td>0.120749</td>\n","      <td>0.616654</td>\n","      <td>1.923384</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-01-01 01:00:00</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.950714</td>\n","      <td>0.523158</td>\n","      <td>0.838483</td>\n","      <td>0.280547</td>\n","      <td>0.260339</td>\n","      <td>0.520433</td>\n","      <td>0.003229</td>\n","      <td>47.550482</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-01-01 02:00:00</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.731994</td>\n","      <td>0.898639</td>\n","      <td>0.185176</td>\n","      <td>0.895044</td>\n","      <td>0.489540</td>\n","      <td>0.095159</td>\n","      <td>0.792586</td>\n","      <td>26.352564</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-01-01 03:00:00</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.598658</td>\n","      <td>0.164393</td>\n","      <td>0.554842</td>\n","      <td>0.332239</td>\n","      <td>0.061339</td>\n","      <td>0.256357</td>\n","      <td>0.243121</td>\n","      <td>53.995885</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-01-01 04:00:00</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0.156019</td>\n","      <td>0.804109</td>\n","      <td>0.722233</td>\n","      <td>0.578596</td>\n","      <td>0.095686</td>\n","      <td>0.451709</td>\n","      <td>0.299217</td>\n","      <td>17.865769</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb7ef833-f981-4a1b-aee8-fe533e2d6d76')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bb7ef833-f981-4a1b-aee8-fe533e2d6d76 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bb7ef833-f981-4a1b-aee8-fe533e2d6d76');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f128cabd-0380-4bd5-bca2-d24f1d8956fc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f128cabd-0380-4bd5-bca2-d24f1d8956fc')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f128cabd-0380-4bd5-bca2-d24f1d8956fc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8761,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-01 00:00:00\",\n        \"max\": \"2020-12-31 00:00:00\",\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          \"2020-04-13 12:00:00\",\n          \"2020-04-15 11:00:00\",\n          \"2020-03-11 19:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hour\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 23,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          8,\n          16,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2884343986745382,\n        \"min\": 1.1634755366141114e-05,\n        \"max\": 0.9997176732861306,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.4626756050653935,\n          0.8284692994491171,\n          0.549529535895017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28865327253328293,\n        \"min\": 5.282693229680113e-05,\n        \"max\": 0.9999248268331765,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.7733040401143754,\n          0.5538309158678235,\n          0.5050578002704087\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2876422502713278,\n        \"min\": 4.8123894311746795e-05,\n        \"max\": 0.9999009770092316,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.9612079012228597,\n          0.42544911655181694,\n          0.991956370803152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28704594194006017,\n        \"min\": 5.536675737993768e-06,\n        \"max\": 0.9997893718125953,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.33263624690907834,\n          0.8409420244918254,\n          0.7918277209316302\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2901410481097046,\n        \"min\": 1.6736257934746313e-05,\n        \"max\": 0.9999721473679823,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.06391397575305591,\n          0.776655442237305,\n          0.31420407746420054\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2887925611047968,\n        \"min\": 9.359385827234501e-05,\n        \"max\": 0.9999569543687702,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.07736641410153167,\n          0.9175483844887636,\n          0.28080288854758184\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2868698677160411,\n        \"min\": 8.432211304310044e-06,\n        \"max\": 0.9999396960457185,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          0.9612848944348956,\n          0.918599777596716,\n          0.0027404927727919803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"electricity_consumption\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.86914245684661,\n        \"min\": 0.027012235672785323,\n        \"max\": 99.96188950868437,\n        \"num_unique_values\": 8761,\n        \"samples\": [\n          36.180315610267165,\n          81.93904449770228,\n          43.06096058596256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Step 3: PyTorch Dataset and DataLoader"],"metadata":{"id":"-ABAFyAw23lK"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.utils.data import Dataset\n","import torch\n","import torch.nn as nn\n","\n","# Normalize features\n","scaler = MinMaxScaler()\n","df.iloc[:,1:-1] = scaler.fit_transform(df.iloc[:,1:-1])\n","\n","# Function to create sequences\n","def create_sequences(input_data, target_data, input_steps, forecast_steps):\n","    X, y = [], []\n","    for i in range(len(input_data) - input_steps - forecast_steps):\n","        X.append(input_data.iloc[i:(i+input_steps)].values)\n","        y.append(target_data.iloc[i+input_steps:i+input_steps+forecast_steps].values)\n","    return np.array(X), np.array(y)\n","\n","encoder_length = 168  # 7 days of hourly records\n","forecast_length = 24  # Predicting the next 24 hours\n","\n","# Creating sequences\n","X, y = create_sequences(df.iloc[:,1:-1], df[['electricity_consumption']], encoder_length, forecast_length)\n","\n","# Splitting dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"oWPby7AAGbgq","executionInfo":{"status":"ok","timestamp":1711968613728,"user_tz":-540,"elapsed":2238,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class ElectricityDataset(Dataset):\n","    def __init__(self, features, targets):\n","        self.features = features\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)"],"metadata":{"id":"mWm0qBwH2li7","executionInfo":{"status":"ok","timestamp":1711968641753,"user_tz":-540,"elapsed":307,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aOqkmwH22q_L","executionInfo":{"status":"ok","timestamp":1711961493568,"user_tz":-540,"elapsed":283,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Step 4: Transformer Model Definition\n","Given the complexity, we'll focus on a simplified Transformer structure, emphasizing positional encoding for handling sequential data."],"metadata":{"id":"5_fhXWee28HG"}},{"cell_type":"code","source":["class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, model_dim, num_heads, num_encoder_layers, output_dim):\n","        super(TransformerModel, self).__init__()\n","        self.input_projection = nn.Linear(input_dim, model_dim)\n","        self.positional_encoder = PositionalEncoding(model_dim)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n","        # Ensure the output dimension matches target dimension [batch_size, forecast_length, 1]\n","        self.fc_out = nn.Linear(model_dim, 1)  # Output dim is 1 per time step\n","\n","    def forward(self, src):\n","        src = self.input_projection(src)  # Project input features to model_dim\n","        src = self.positional_encoder(src)\n","        encoded_src = self.transformer_encoder(src)\n","        output = self.fc_out(encoded_src)  # Apply linear transformation\n","        # Select the last forecast_length steps from output\n","        output = output[-forecast_length:, :, :]\n","        return output.permute(1, 0, 2)  # Adjust output to match [batch_size, forecast_length, features]\n","\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, d_model)\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return x\n"],"metadata":{"id":"MYaypcYZ20yC","executionInfo":{"status":"ok","timestamp":1711968677440,"user_tz":-540,"elapsed":7,"user":{"displayName":"yo it","userId":"02303648966403166717"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Training"],"metadata":{"id":"q4nLyiNV3CAe"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from sklearn.model_selection import train_test_split\n","\n","# Split data into training and validation sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","train_dataset = ElectricityDataset(X_train, y_train)\n","val_dataset = ElectricityDataset(X_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Model instantiation\n","input_dim = 10  # Number of features\n","model_dim = 512\n","num_heads = 8\n","num_encoder_layers = 3\n","num_decoder_layers = 3\n","output_dim = 24  # Predicting next 24 hours of consumption\n","\n","model = TransformerModel(input_dim, model_dim, num_heads, num_encoder_layers, output_dim)\n","\n","# Loss and optimizer\n","criterion = nn.MSELoss()\n","optimizer = Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","model.train()  # Set model to training mode\n","for batch_idx, (data, target) in enumerate(train_loader):\n","    optimizer.zero_grad()\n","    data = data.permute(1, 0, 2)  # [seq_len, batch, features]\n","    output = model(data)\n","    # Here, ensure output and target shapes are aligned\n","    output = output[:, -forecast_length:, :]  # Focus on the last 'forecast_length' outputs\n","    loss = criterion(output, target)\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Train Epoch: 1 [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCV6v8Ae3GCS","executionInfo":{"status":"ok","timestamp":1711970190238,"user_tz":-540,"elapsed":1484528,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"6611a541-136e-44c1-af43-455ac3ed3c01"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/6855 (0%)]\tLoss: 3373.349365\n","Train Epoch: 1 [168/6855 (0%)]\tLoss: 2403.863037\n","Train Epoch: 1 [336/6855 (1%)]\tLoss: 2219.698242\n","Train Epoch: 1 [504/6855 (1%)]\tLoss: 2319.867432\n","Train Epoch: 1 [672/6855 (2%)]\tLoss: 2098.401123\n","Train Epoch: 1 [840/6855 (2%)]\tLoss: 2151.233154\n","Train Epoch: 1 [1008/6855 (3%)]\tLoss: 2190.414551\n","Train Epoch: 1 [1176/6855 (3%)]\tLoss: 2091.758057\n","Train Epoch: 1 [1344/6855 (4%)]\tLoss: 2071.520264\n","Train Epoch: 1 [1512/6855 (4%)]\tLoss: 2153.845947\n","Train Epoch: 1 [1680/6855 (5%)]\tLoss: 1919.999023\n","Train Epoch: 1 [1848/6855 (5%)]\tLoss: 1978.755005\n","Train Epoch: 1 [2016/6855 (6%)]\tLoss: 1963.810547\n","Train Epoch: 1 [2184/6855 (6%)]\tLoss: 1861.000000\n","Train Epoch: 1 [2352/6855 (7%)]\tLoss: 1816.627319\n","Train Epoch: 1 [2520/6855 (7%)]\tLoss: 1864.074707\n","Train Epoch: 1 [2688/6855 (7%)]\tLoss: 1797.737305\n","Train Epoch: 1 [2856/6855 (8%)]\tLoss: 1932.668335\n","Train Epoch: 1 [3024/6855 (8%)]\tLoss: 1710.181030\n","Train Epoch: 1 [3192/6855 (9%)]\tLoss: 1678.543579\n","Train Epoch: 1 [3360/6855 (9%)]\tLoss: 1631.539429\n","Train Epoch: 1 [3528/6855 (10%)]\tLoss: 1621.053345\n","Train Epoch: 1 [3696/6855 (10%)]\tLoss: 1657.741089\n","Train Epoch: 1 [3864/6855 (11%)]\tLoss: 1518.202637\n","Train Epoch: 1 [4032/6855 (11%)]\tLoss: 1628.132324\n","Train Epoch: 1 [4200/6855 (12%)]\tLoss: 1714.759766\n","Train Epoch: 1 [4368/6855 (12%)]\tLoss: 1606.871460\n","Train Epoch: 1 [4536/6855 (13%)]\tLoss: 1466.933960\n","Train Epoch: 1 [4704/6855 (13%)]\tLoss: 1474.219238\n","Train Epoch: 1 [4872/6855 (13%)]\tLoss: 1338.637207\n","Train Epoch: 1 [5040/6855 (14%)]\tLoss: 1434.381470\n","Train Epoch: 1 [5208/6855 (14%)]\tLoss: 1387.995605\n","Train Epoch: 1 [5376/6855 (15%)]\tLoss: 1341.895142\n","Train Epoch: 1 [5544/6855 (15%)]\tLoss: 1470.061523\n","Train Epoch: 1 [5712/6855 (16%)]\tLoss: 1297.835449\n","Train Epoch: 1 [5880/6855 (16%)]\tLoss: 1281.620728\n","Train Epoch: 1 [6048/6855 (17%)]\tLoss: 1218.513062\n","Train Epoch: 1 [6216/6855 (17%)]\tLoss: 1290.247559\n","Train Epoch: 1 [6384/6855 (18%)]\tLoss: 1319.328491\n","Train Epoch: 1 [6552/6855 (18%)]\tLoss: 1210.826538\n","Train Epoch: 1 [6720/6855 (19%)]\tLoss: 1213.296021\n","Train Epoch: 1 [6888/6855 (19%)]\tLoss: 1170.972656\n","Train Epoch: 1 [7056/6855 (20%)]\tLoss: 1177.381958\n","Train Epoch: 1 [7224/6855 (20%)]\tLoss: 1203.852905\n","Train Epoch: 1 [7392/6855 (20%)]\tLoss: 1156.572388\n","Train Epoch: 1 [7560/6855 (21%)]\tLoss: 1142.144409\n","Train Epoch: 1 [7728/6855 (21%)]\tLoss: 1068.379272\n","Train Epoch: 1 [7896/6855 (22%)]\tLoss: 1019.835754\n","Train Epoch: 1 [8064/6855 (22%)]\tLoss: 1091.933472\n","Train Epoch: 1 [8232/6855 (23%)]\tLoss: 1071.610474\n","Train Epoch: 1 [8400/6855 (23%)]\tLoss: 1044.459595\n","Train Epoch: 1 [8568/6855 (24%)]\tLoss: 1135.837158\n","Train Epoch: 1 [8736/6855 (24%)]\tLoss: 1000.940247\n","Train Epoch: 1 [8904/6855 (25%)]\tLoss: 980.016418\n","Train Epoch: 1 [9072/6855 (25%)]\tLoss: 1053.266968\n","Train Epoch: 1 [9240/6855 (26%)]\tLoss: 1015.591858\n","Train Epoch: 1 [9408/6855 (26%)]\tLoss: 1010.612122\n","Train Epoch: 1 [9576/6855 (27%)]\tLoss: 1028.570923\n","Train Epoch: 1 [9744/6855 (27%)]\tLoss: 928.777649\n","Train Epoch: 1 [9912/6855 (27%)]\tLoss: 959.343994\n","Train Epoch: 1 [10080/6855 (28%)]\tLoss: 929.989807\n","Train Epoch: 1 [10248/6855 (28%)]\tLoss: 901.675964\n","Train Epoch: 1 [10416/6855 (29%)]\tLoss: 904.350281\n","Train Epoch: 1 [10584/6855 (29%)]\tLoss: 933.399597\n","Train Epoch: 1 [10752/6855 (30%)]\tLoss: 943.761719\n","Train Epoch: 1 [10920/6855 (30%)]\tLoss: 896.690979\n","Train Epoch: 1 [11088/6855 (31%)]\tLoss: 887.499573\n","Train Epoch: 1 [11256/6855 (31%)]\tLoss: 904.174744\n","Train Epoch: 1 [11424/6855 (32%)]\tLoss: 853.961609\n","Train Epoch: 1 [11592/6855 (32%)]\tLoss: 887.605042\n","Train Epoch: 1 [11760/6855 (33%)]\tLoss: 891.313171\n","Train Epoch: 1 [11928/6855 (33%)]\tLoss: 883.214539\n","Train Epoch: 1 [12096/6855 (33%)]\tLoss: 823.643738\n","Train Epoch: 1 [12264/6855 (34%)]\tLoss: 859.739319\n","Train Epoch: 1 [12432/6855 (34%)]\tLoss: 837.472168\n","Train Epoch: 1 [12600/6855 (35%)]\tLoss: 840.912903\n","Train Epoch: 1 [12768/6855 (35%)]\tLoss: 895.689148\n","Train Epoch: 1 [12936/6855 (36%)]\tLoss: 829.335693\n","Train Epoch: 1 [13104/6855 (36%)]\tLoss: 802.493225\n","Train Epoch: 1 [13272/6855 (37%)]\tLoss: 863.557617\n","Train Epoch: 1 [13440/6855 (37%)]\tLoss: 775.671631\n","Train Epoch: 1 [13608/6855 (38%)]\tLoss: 886.797852\n","Train Epoch: 1 [13776/6855 (38%)]\tLoss: 780.898438\n","Train Epoch: 1 [13944/6855 (39%)]\tLoss: 789.562012\n","Train Epoch: 1 [14112/6855 (39%)]\tLoss: 813.060059\n","Train Epoch: 1 [14280/6855 (40%)]\tLoss: 879.132996\n","Train Epoch: 1 [14448/6855 (40%)]\tLoss: 788.315674\n","Train Epoch: 1 [14616/6855 (40%)]\tLoss: 871.341370\n","Train Epoch: 1 [14784/6855 (41%)]\tLoss: 865.194885\n","Train Epoch: 1 [14952/6855 (41%)]\tLoss: 854.454895\n","Train Epoch: 1 [15120/6855 (42%)]\tLoss: 808.658203\n","Train Epoch: 1 [15288/6855 (42%)]\tLoss: 823.627258\n","Train Epoch: 1 [15456/6855 (43%)]\tLoss: 788.682190\n","Train Epoch: 1 [15624/6855 (43%)]\tLoss: 849.478271\n","Train Epoch: 1 [15792/6855 (44%)]\tLoss: 804.023254\n","Train Epoch: 1 [15960/6855 (44%)]\tLoss: 842.982422\n","Train Epoch: 1 [16128/6855 (45%)]\tLoss: 850.964600\n","Train Epoch: 1 [16296/6855 (45%)]\tLoss: 854.977051\n","Train Epoch: 1 [16464/6855 (46%)]\tLoss: 848.009583\n","Train Epoch: 1 [16632/6855 (46%)]\tLoss: 835.285217\n","Train Epoch: 1 [16800/6855 (47%)]\tLoss: 850.701599\n","Train Epoch: 1 [16968/6855 (47%)]\tLoss: 795.821960\n","Train Epoch: 1 [17136/6855 (47%)]\tLoss: 811.475098\n","Train Epoch: 1 [17304/6855 (48%)]\tLoss: 802.200256\n","Train Epoch: 1 [17472/6855 (48%)]\tLoss: 844.819824\n","Train Epoch: 1 [17640/6855 (49%)]\tLoss: 805.076416\n","Train Epoch: 1 [17808/6855 (49%)]\tLoss: 832.395996\n","Train Epoch: 1 [17976/6855 (50%)]\tLoss: 806.265381\n","Train Epoch: 1 [18144/6855 (50%)]\tLoss: 865.818359\n","Train Epoch: 1 [18312/6855 (51%)]\tLoss: 835.483459\n","Train Epoch: 1 [18480/6855 (51%)]\tLoss: 838.194519\n","Train Epoch: 1 [18648/6855 (52%)]\tLoss: 829.277832\n","Train Epoch: 1 [18816/6855 (52%)]\tLoss: 819.053406\n","Train Epoch: 1 [18984/6855 (53%)]\tLoss: 837.774963\n","Train Epoch: 1 [19152/6855 (53%)]\tLoss: 778.775391\n","Train Epoch: 1 [19320/6855 (53%)]\tLoss: 835.265320\n","Train Epoch: 1 [19488/6855 (54%)]\tLoss: 818.354309\n","Train Epoch: 1 [19656/6855 (54%)]\tLoss: 862.616638\n","Train Epoch: 1 [19824/6855 (55%)]\tLoss: 845.542542\n","Train Epoch: 1 [19992/6855 (55%)]\tLoss: 815.969421\n","Train Epoch: 1 [20160/6855 (56%)]\tLoss: 854.753906\n","Train Epoch: 1 [20328/6855 (56%)]\tLoss: 777.541199\n","Train Epoch: 1 [20496/6855 (57%)]\tLoss: 818.450012\n","Train Epoch: 1 [20664/6855 (57%)]\tLoss: 807.013916\n","Train Epoch: 1 [20832/6855 (58%)]\tLoss: 843.449890\n","Train Epoch: 1 [21000/6855 (58%)]\tLoss: 829.296204\n","Train Epoch: 1 [21168/6855 (59%)]\tLoss: 850.197205\n","Train Epoch: 1 [21336/6855 (59%)]\tLoss: 806.060547\n","Train Epoch: 1 [21504/6855 (60%)]\tLoss: 798.409180\n","Train Epoch: 1 [21672/6855 (60%)]\tLoss: 836.450195\n","Train Epoch: 1 [21840/6855 (60%)]\tLoss: 829.395508\n","Train Epoch: 1 [22008/6855 (61%)]\tLoss: 840.191711\n","Train Epoch: 1 [22176/6855 (61%)]\tLoss: 823.451843\n","Train Epoch: 1 [22344/6855 (62%)]\tLoss: 826.725891\n","Train Epoch: 1 [22512/6855 (62%)]\tLoss: 820.744385\n","Train Epoch: 1 [22680/6855 (63%)]\tLoss: 830.193604\n","Train Epoch: 1 [22848/6855 (63%)]\tLoss: 834.119446\n","Train Epoch: 1 [23016/6855 (64%)]\tLoss: 865.997375\n","Train Epoch: 1 [23184/6855 (64%)]\tLoss: 833.347351\n","Train Epoch: 1 [23352/6855 (65%)]\tLoss: 842.999817\n","Train Epoch: 1 [23520/6855 (65%)]\tLoss: 842.387268\n","Train Epoch: 1 [23688/6855 (66%)]\tLoss: 757.658264\n","Train Epoch: 1 [23856/6855 (66%)]\tLoss: 871.007812\n","Train Epoch: 1 [24024/6855 (67%)]\tLoss: 864.075867\n","Train Epoch: 1 [24192/6855 (67%)]\tLoss: 885.604675\n","Train Epoch: 1 [24360/6855 (67%)]\tLoss: 855.654968\n","Train Epoch: 1 [24528/6855 (68%)]\tLoss: 871.877441\n","Train Epoch: 1 [24696/6855 (68%)]\tLoss: 859.834900\n","Train Epoch: 1 [24864/6855 (69%)]\tLoss: 817.856445\n","Train Epoch: 1 [25032/6855 (69%)]\tLoss: 881.134277\n","Train Epoch: 1 [25200/6855 (70%)]\tLoss: 848.015808\n","Train Epoch: 1 [25368/6855 (70%)]\tLoss: 834.181580\n","Train Epoch: 1 [25536/6855 (71%)]\tLoss: 851.622742\n","Train Epoch: 1 [25704/6855 (71%)]\tLoss: 867.627930\n","Train Epoch: 1 [25872/6855 (72%)]\tLoss: 828.520264\n","Train Epoch: 1 [26040/6855 (72%)]\tLoss: 829.930908\n","Train Epoch: 1 [26208/6855 (73%)]\tLoss: 849.284180\n","Train Epoch: 1 [26376/6855 (73%)]\tLoss: 879.323914\n","Train Epoch: 1 [26544/6855 (73%)]\tLoss: 828.904480\n","Train Epoch: 1 [26712/6855 (74%)]\tLoss: 815.914551\n","Train Epoch: 1 [26880/6855 (74%)]\tLoss: 871.425293\n","Train Epoch: 1 [27048/6855 (75%)]\tLoss: 856.428223\n","Train Epoch: 1 [27216/6855 (75%)]\tLoss: 810.129700\n","Train Epoch: 1 [27384/6855 (76%)]\tLoss: 847.251953\n","Train Epoch: 1 [27552/6855 (76%)]\tLoss: 816.340637\n","Train Epoch: 1 [27720/6855 (77%)]\tLoss: 850.890320\n","Train Epoch: 1 [27888/6855 (77%)]\tLoss: 871.675842\n","Train Epoch: 1 [28056/6855 (78%)]\tLoss: 851.009460\n","Train Epoch: 1 [28224/6855 (78%)]\tLoss: 823.140320\n","Train Epoch: 1 [28392/6855 (79%)]\tLoss: 829.291748\n","Train Epoch: 1 [28560/6855 (79%)]\tLoss: 828.343323\n","Train Epoch: 1 [28728/6855 (80%)]\tLoss: 798.235107\n","Train Epoch: 1 [28896/6855 (80%)]\tLoss: 829.857605\n","Train Epoch: 1 [29064/6855 (80%)]\tLoss: 804.345703\n","Train Epoch: 1 [29232/6855 (81%)]\tLoss: 777.590515\n","Train Epoch: 1 [29400/6855 (81%)]\tLoss: 868.153320\n","Train Epoch: 1 [29568/6855 (82%)]\tLoss: 827.690674\n","Train Epoch: 1 [29736/6855 (82%)]\tLoss: 786.591492\n","Train Epoch: 1 [29904/6855 (83%)]\tLoss: 814.038635\n","Train Epoch: 1 [30072/6855 (83%)]\tLoss: 841.652588\n","Train Epoch: 1 [30240/6855 (84%)]\tLoss: 837.888184\n","Train Epoch: 1 [30408/6855 (84%)]\tLoss: 829.565125\n","Train Epoch: 1 [30576/6855 (85%)]\tLoss: 855.088440\n","Train Epoch: 1 [30744/6855 (85%)]\tLoss: 861.824280\n","Train Epoch: 1 [30912/6855 (86%)]\tLoss: 822.293640\n","Train Epoch: 1 [31080/6855 (86%)]\tLoss: 804.640625\n","Train Epoch: 1 [31248/6855 (87%)]\tLoss: 828.666016\n","Train Epoch: 1 [31416/6855 (87%)]\tLoss: 814.844299\n","Train Epoch: 1 [31584/6855 (87%)]\tLoss: 895.621582\n","Train Epoch: 1 [31752/6855 (88%)]\tLoss: 848.527649\n","Train Epoch: 1 [31920/6855 (88%)]\tLoss: 808.585144\n","Train Epoch: 1 [32088/6855 (89%)]\tLoss: 837.908203\n","Train Epoch: 1 [32256/6855 (89%)]\tLoss: 859.561523\n","Train Epoch: 1 [32424/6855 (90%)]\tLoss: 868.942383\n","Train Epoch: 1 [32592/6855 (90%)]\tLoss: 829.850159\n","Train Epoch: 1 [32760/6855 (91%)]\tLoss: 852.159119\n","Train Epoch: 1 [32928/6855 (91%)]\tLoss: 842.399231\n","Train Epoch: 1 [33096/6855 (92%)]\tLoss: 826.332581\n","Train Epoch: 1 [33264/6855 (92%)]\tLoss: 808.451843\n","Train Epoch: 1 [33432/6855 (93%)]\tLoss: 786.827881\n","Train Epoch: 1 [33600/6855 (93%)]\tLoss: 874.883301\n","Train Epoch: 1 [33768/6855 (93%)]\tLoss: 818.639832\n","Train Epoch: 1 [33936/6855 (94%)]\tLoss: 864.711487\n","Train Epoch: 1 [34104/6855 (94%)]\tLoss: 830.905823\n","Train Epoch: 1 [34272/6855 (95%)]\tLoss: 903.662659\n","Train Epoch: 1 [34440/6855 (95%)]\tLoss: 816.476257\n","Train Epoch: 1 [34608/6855 (96%)]\tLoss: 828.408386\n","Train Epoch: 1 [34776/6855 (96%)]\tLoss: 856.755371\n","Train Epoch: 1 [34944/6855 (97%)]\tLoss: 856.772156\n","Train Epoch: 1 [35112/6855 (97%)]\tLoss: 832.122314\n","Train Epoch: 1 [35280/6855 (98%)]\tLoss: 877.964050\n","Train Epoch: 1 [35448/6855 (98%)]\tLoss: 880.214294\n","Train Epoch: 1 [35616/6855 (99%)]\tLoss: 859.350403\n","Train Epoch: 1 [35784/6855 (99%)]\tLoss: 759.938904\n","Train Epoch: 1 [35952/6855 (100%)]\tLoss: 923.636719\n"]}]},{"cell_type":"markdown","source":["### about error at training loop\n"],"metadata":{"id":"cFCFLalVSc4w"}},{"cell_type":"code","source":["# error code\n","def forward(self, src):\n","    src = self.input_projection(src)  # Project input features to model_dim\n","    src = self.positional_encoder(src)\n","    encoded_src = self.transformer_encoder(src)\n","    output = self.fc_out(encoded_src)  # Apply linear transformation\n","    # Ensure output is reshaped or sliced correctly if necessary\n","    return output.permute(1, 0, 2)  # Adjust output to match [batch_size, seq_len, features]\n","\n","\n","# Training loop\n","model.train()  # Set model to training mode\n","for batch_idx, (data, target) in enumerate(train_loader):\n","    optimizer.zero_grad()\n","    data = data.permute(1, 0, 2)  # Transformer expects [seq_len, batch, features]\n","    output = model(data)\n","    loss = criterion(output, target)  # Directly compare output and target\n","    loss.backward()\n","    optimizer.step()\n","    print(f\"Train Epoch: 1 [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")"],"metadata":{"id":"179207mfSfSS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The error and warning you're encountering are due to a shape mismatch between the model's output and the target tensors during the loss computation. Specifically, your model's output has the shape **[batch_size, 168, 1]**, while your target tensor has the shape **[batch_size, 24, 1]**. This discrepancy leads to the RuntimeError because the dimensions do not align for a valid Mean Squared Error (MSE) computation.\n","<br><br>\n","The cause of this issue is that the Transformer model is designed to output a sequence with the same length as the input sequence (168 steps in your case), but you are trying to compare it against a target sequence of only 24 steps."],"metadata":{"id":"eQBY3UFIShMx"}},{"cell_type":"markdown","source":["**Solution**\n","<br><br>\n","To solve this issue, you need to modify your model or the way you handle its output so that it matches the target's shape. One straightforward approach is to adjust the Transformer model's output processing to select or aggregate its output to match the target size of 24 steps.\n","<br><br>\n","Given the structure of your model, where the output dimensionality is **[batch_size, seq_len, features]**, and you want to predict the next 24 hours (forecast_length), you should modify the forward method of your model to correctly shape the output. Here's how you might adjust your model:\n","<br><br>\n","Adjusting the Model's Output\n","<br><br>\n","One way to adjust the model's output is to focus on the last **forecast_length** outputs for comparison with the target. However, since your model outputs one value per timestep across the encoder's entire sequence length (168 steps), you need to select a subset of these steps that aligns with your forecasting objective.\n","<br><br>\n","A simple approach is to adjust the model output within the **forward** method to return only the last **forecast_length** timesteps:"],"metadata":{"id":"Zu3izbrySjB2"}},{"cell_type":"code","source":["# 修正後\n","for batch_idx, (data, target) in enumerate(train_loader):\n","    optimizer.zero_grad()\n","    data = data.permute(1, 0, 2)  # [seq_len, batch, features]\n","    output = model(data)\n","    # Here, ensure output and target shapes are aligned\n","    output = output[:, -forecast_length:, :]  # Focus on the last 'forecast_length' outputs\n","    loss = criterion(output, target)\n","    loss.backward()\n","    optimizer.step()\n"],"metadata":{"id":"8PTRa7ShSky5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This adjustment ensures that during the loss computation, only the last 24 outputs (corresponding to the **forecast_length**) from the model are compared against the 24-hour targets, resolving the shape mismatch issue."],"metadata":{"id":"oIhHRlMKSm_7"}},{"cell_type":"code","source":[],"metadata":{"id":"PMY6N7V3SpkR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step6: Validation loop"],"metadata":{"id":"wkrVlfIlSp8t"}},{"cell_type":"code","source":["# Validation loop\n","model.eval()  # Set model to evaluation mode\n","val_loss = 0\n","with torch.no_grad():\n","    for data, target in val_loader:\n","        data = data.permute(1, 0, 2)  # Adjusting data dimensions for the model\n","        output = model(data)\n","        # Ensure output shape matches target shape [batch_size, forecast_length, 1]\n","        # No need for view or reshape if model output is correctly sized\n","        val_loss += criterion(output, target).item()  # Sum up batch loss\n","\n","val_loss /= len(val_loader.dataset)\n","print(f'\\nValidation set: Average loss: {val_loss:.4f}\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1M2zwxDLSub_","executionInfo":{"status":"ok","timestamp":1711970432791,"user_tz":-540,"elapsed":110748,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"31eba805-2b7a-4df5-84df-805a3cd71133"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Validation set: Average loss: 26.4183\n","\n"]}]},{"cell_type":"markdown","source":["### error code at validation loop"],"metadata":{"id":"oAv6hBGaSxYA"}},{"cell_type":"markdown","source":["This modification assumes that your model's output already has the correct shape **[batch_size, forecast_length, 1]**, as intended after our previous adjustments to the model. This way, you directly compare **output** and **target** without additional reshaping, ensuring that the shapes align for the loss calculation.\n","<br><br>\n","If your model's output does not inherently have the correct shape, you may need to revisit and ensure that the model's forward method or the post-processing of its output (just before the loss calculation) correctly aligns the output shape with the target tensor shape. Since we've ensured the model's output should be **[batch_size, forecast_length, 1]**, this direct comparison in the validation loop should now work without issues."],"metadata":{"id":"WSVAIewlS4EU"}},{"cell_type":"code","source":["# Validation loop\n","model.eval()  # Set model to evaluation mode\n","val_loss = 0\n","with torch.no_grad():\n","    for data, target in val_loader:\n","        data = data.permute(1, 0, 2)  # Adjusting data dimensions for the model\n","        output = model(data)\n","        val_loss += criterion(output.view(-1, output_dim), target).item()  # Sum up batch loss\n","\n","val_loss /= len(val_loader.dataset)\n","print(f'\\nValidation set: Average loss: {val_loss:.4f}\\n')"],"metadata":{"id":"_EQQ-cl-SzXe"},"execution_count":null,"outputs":[]}]}