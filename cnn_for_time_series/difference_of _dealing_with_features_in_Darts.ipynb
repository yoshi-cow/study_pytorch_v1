{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0QZt6dyY7IHAts5lSNFx4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# difference of dealing with featurs in Darts\n","\n","Darts class deal with features by another paarameter like **`past_covariates`**. Therefore, when using Darts library, we need separates additional features from **X** dataset.\n","\n","In this notebook, I show how to use additional features in Darts and comapare darts and pytorch."],"metadata":{"id":"ggYoFX03etKR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1SccfLWelQA"},"outputs":[],"source":["!pip install darts"]},{"cell_type":"markdown","source":["## Darts `past_covariates`"],"metadata":{"id":"pO88oTr1fTIr"}},{"cell_type":"code","source":["from darts.models import TCNModel\n","from darts.datasets import AirPassengersDataset\n","from darts.utils.timeseries_generation import datetime_attribute_timeseries\n","from darts import TimeSeries\n","import matplotlib.pyplot as plt\n","\n","# Load the target time series (e.g., monthly air passengers data)\n","series = AirPassengersDataset().load()\n","\n","# Generate a past covariate (e.g., month of the year as a cyclic feature)\n","covariates = datetime_attribute_timeseries(\n","    series.time_index, attribute=\"month\", cyclic=True, one_hot=False\n",")\n","\n","# Create the TCN model\n","model = TCNModel(\n","    input_chunk_length=24,\n","    output_chunk_length=12,\n","    kernel_size=3,\n","    num_filters=16,\n","    num_layers=3,\n","    dropout=0.2,\n","    weight_norm=True,\n","    random_state=42\n",")\n","\n","# Split the data into training and validation sets\n","train_series = series[:-36]\n","val_series = series[-36:]\n","\n","# Ensure covariates cover the required range for training and prediction\n","train_covariates = covariates[:len(train_series)]\n","val_covariates = covariates[-(len(val_series) + model.output_chunk_length):]\n","\n","# Fit the model with the past covariates\n","model.fit(train_series, past_covariates=train_covariates)\n","\n","# Make predictions\n","pred_series = model.predict(n=12, series=train_series, past_covariates=covariates)\n","\n","# Plot the results\n","series.plot(label='True Series')\n","pred_series.plot(label='Predictions', linestyle='dashed')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"BdE0DPtufS6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vfs_MilXfYyb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pytorch codes to deal with additional features"],"metadata":{"id":"jJ1XvLV3fYP3"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","# Load the target time series (e.g., monthly air passengers data)\n","url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n","data = pd.read_csv(url, usecols=[1])\n","series = data.values.astype(float).reshape(-1)\n","\n","# Generate a past covariate (e.g., month of the year as a cyclic feature)\n","dates = pd.date_range(start='1949-01', periods=len(series), freq='M')\n","covariates = pd.get_dummies(dates.month, drop_first=True).values\n","\n","# Normalize the series and covariates\n","scaler = StandardScaler()\n","series_scaled = scaler.fit_transform(series.reshape(-1, 1)).reshape(-1)\n","covariates_scaled = pd.get_dummies(dates.month, drop_first=True).values\n","\n","# Create sequences of input data for training\n","def create_sequences(data, covariates, input_length, output_length):\n","    X, y = [], []\n","    for i in range(len(data) - input_length - output_length):\n","        combined_features = np.hstack([data[i:i+input_length].reshape(-1, 1), covariates[i:i+input_length]])\n","        X.append(combined_features)\n","        y.append(data[i+input_length:i+input_length+output_length])\n","    return np.array(X), np.array(y)\n","\n","input_chunk_length = 24\n","output_chunk_length = 12\n","X, y = create_sequences(series_scaled, covariates_scaled, input_chunk_length, output_chunk_length)\n","\n","# Debug prints to check shapes\n","print(f\"X shape: {X.shape}\")\n","print(f\"y shape: {y.shape}\")\n","\n","# Split the data into training and validation sets\n","train_size = len(X) - 36\n","X_train, y_train = X[:train_size], y[:train_size]\n","X_val, y_val = X[train_size:], y[train_size:]\n","\n","# Debug prints to check shapes after split\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"X_val shape: {X_val.shape}\")\n","print(f\"y_val shape: {y_val.shape}\")\n","\n","# Create DataLoader for training\n","train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n","                              torch.tensor(y_train, dtype=torch.float32))\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n","                            torch.tensor(y_val, dtype=torch.float32))\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Check the shapes after DataLoader creation\n","for batch_x, batch_y in train_loader:\n","    print(f\"batch_x shape: {batch_x.shape}\")\n","    print(f\"batch_y shape: {batch_y.shape}\")\n","    break  # Only check the first batch\n","\n","# Define the TCN model\n","class ResidualBlock(nn.Module):\n","    def __init__(self, num_filters, kernel_size, dilation_base, dropout, weight_norm, nr_blocks_below, num_layers, input_size, target_size):\n","        super(ResidualBlock, self).__init__()\n","        self.dilation_base = dilation_base\n","        self.kernel_size = kernel_size\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","        self.num_layers = num_layers\n","        self.nr_blocks_below = nr_blocks_below\n","\n","        input_dim = input_size if nr_blocks_below == 0 else num_filters\n","        output_dim = target_size if nr_blocks_below == num_layers - 1 else num_filters\n","        self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size, dilation=(dilation_base**nr_blocks_below))\n","        self.conv2 = nn.Conv1d(num_filters, output_dim, kernel_size, dilation=(dilation_base**nr_blocks_below))\n","        if weight_norm:\n","            self.conv1 = nn.utils.parametrizations.weight_norm(self.conv1)\n","            self.conv2 = nn.utils.parametrizations.weight_norm(self.conv2)\n","        if input_dim != output_dim:\n","            self.conv3 = nn.Conv1d(input_dim, output_dim, 1)\n","\n","    def forward(self, x):\n","        residual = x\n","        left_padding = (self.dilation_base**self.nr_blocks_below) * (self.kernel_size - 1)\n","        x = F.pad(x, (left_padding, 0))\n","        x = self.dropout1(F.relu(self.conv1(x)))\n","        x = F.pad(x, (left_padding, 0))\n","        x = self.conv2(x)\n","        if self.nr_blocks_below < self.num_layers - 1:\n","            x = F.relu(x)\n","        x = self.dropout2(x)\n","        if self.conv1.in_channels != self.conv2.out_channels:\n","            residual = self.conv3(residual)\n","        x = x + residual\n","        return x\n","\n","class TCNModel(nn.Module):\n","    def __init__(self, input_size, target_size, nr_params, kernel_size, num_filters, num_layers, dilation_base, target_length, dropout, weight_norm):\n","        super(TCNModel, self).__init__()\n","        self.input_size = input_size\n","        self.num_layers = num_layers\n","        self.res_blocks_list = []\n","        for i in range(num_layers):\n","            res_block = ResidualBlock(num_filters, kernel_size, dilation_base, dropout, weight_norm, i, num_layers, input_size, target_size * nr_params)\n","            self.res_blocks_list.append(res_block)\n","        self.res_blocks = nn.ModuleList(self.res_blocks_list)\n","        self.target_size = target_size\n","        self.nr_params = nr_params\n","        self.target_length = target_length\n","\n","    def forward(self, x):\n","        x = x.transpose(1, 2)\n","        for res_block in self.res_blocks:\n","            x = res_block(x)\n","        x = x.transpose(1, 2)\n","        return x\n","\n","# Initialize the model\n","input_size = X_train.shape[2]\n","target_size = 1\n","nr_params = 1\n","model = TCNModel(input_size, target_size, nr_params, kernel_size=3, num_filters=16, num_layers=3, dilation_base=2, target_length=output_chunk_length, dropout=0.2, weight_norm=True)\n","\n","# Define the loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch_x, batch_y in train_loader:\n","        optimizer.zero_grad()\n","        output = model(batch_x)\n","        output = output[:, -output_chunk_length:, :]  # Select the last output_chunk_length points\n","        loss = criterion(output.squeeze(), batch_y)\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for batch_x, batch_y in val_loader:\n","            output = model(batch_x)\n","            output = output[:, -output_chunk_length:, :]  # Select the last output_chunk_length points\n","            loss = criterion(output.squeeze(), batch_y)\n","            val_loss += loss.item()\n","    val_loss /= len(val_loader)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')\n","\n","# Make predictions\n","model.eval()\n","with torch.no_grad():\n","    X_test = torch.tensor(X_val, dtype=torch.float32)\n","    pred_series = model(X_test)\n","    pred_series = pred_series[:, -output_chunk_length:, :].squeeze().numpy()  # Select the last output_chunk_length points\n","\n","# Rescale predictions\n","pred_series_rescaled = scaler.inverse_transform(pred_series.reshape(-1, 1)).reshape(pred_series.shape)\n","\n","# Extract the last prediction from each sequence\n","last_predictions = pred_series_rescaled[:, -1]\n","\n","# Plot the last predictions\n","plt.figure(figsize=(10, 6))\n","plt.plot(series, label='True Series')\n","plt.plot(range(len(series) - len(last_predictions), len(series)), last_predictions, label='Last Predictions', linestyle='dashed')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"v3FFUIWDfS4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k7s3ydKGfS1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ochKss4IfSxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yKEKV4E2fSox"},"execution_count":null,"outputs":[]}]}