{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlI+OFLeqccqTGcpdofT0o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Parameters of `nn.Conv1d` of PyTorch\n","\n","## 1. in_channels:\n","* **Role**: Specifies the number of input channels. For example, if you are processing a sequence with a single feature per time step, **`in_channels`** would be 1. If you have multiple features per time step, **`in_channels`** would match the number of features.\n","* **Example**: For a single-channel time series, **`in_channels=1`**. For a multi-channels time series (e.g., audio with left and right channels), **`in_channels=2`**.\n","\n","## 2. out_channels:\n","* **Role**: Specifies the number of output channels (filters). This parameter determine how many different filters will be applied to the input data, each producing its own feature map.\n","* **Example**: If **`out_channels=64`**, there will be 64 different filters, and thus the output will have 64 channels.\n","\n","## 3. Kernel_size:\n","* **Role**: Defines the size (length) of the convolutional kernel (filter). This parameter determines the number of time steps the filter spans. A larger kernel size can capture patterns over a longer range of time steps.\n","* **Example**: If **`kernel_size=3`**, the filter will look at 3 consecutive time steps in the input sequence.\n","\n","## 4. stride (default=1):\n","* **Role**: Specifies the stride of the convolution. This is the number of time steps the filter moves at each step. A larger stride reduces the size of the output.\n","* **Example**: **`stride=2`** means the filter moves 2 time steps at a time.\n","\n","## 5. padding (default=0):\n","* **Role**: Determines the amount of zero-padding added to both sides of the input sequence. Padding helps control the output size and preserve the original length.\n","\n","## 6. dilation (default=1):\n","* **Role**: Specifies the spacing between elements in the filter. Dilation allows the filter to cover a larger receptive field without increasing the kernel size.\n","* **Example**: **`dilation=2`** uses every second element in the input sequence, effectively spreading out the filter.\n","\n","## 7. groups (defalut=1):\n","* **Role**: Controls the connections between inputs and outputs. If **`groups=1`**, each filter is applied to all input channels. If **`groups=in_channels`**, each filer is applied to one input channel (depthwise convolution).\n","\n","## 8. bias (default=True):\n","* **Role**: If set to **`True`**, a learnable bias parameter is added to the output. This bias is added to each output feature map.\n","* **Example**: **`bias=False`** means no bias term is added to the output."],"metadata":{"id":"VkB-BtrkC2FL"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Tv8zrEUCtjD","executionInfo":{"status":"ok","timestamp":1720925399548,"user_tz":-540,"elapsed":6099,"user":{"displayName":"yo it","userId":"02303648966403166717"}},"outputId":"eb64ab4a-1938-4cfe-93c6-59f7492b557d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([10, 1, 100])\n","Output shape: torch.Size([10, 64, 100])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","# Define a Conv1d layer\n","conv1d_layer = nn.Conv1d(\n","    in_channels=1,    # Single input channel\n","    out_channels=64,  # 64 output channels (filters)\n","    kernel_size=3,    # Filter size of 3 time steps\n","    stride=1,         # Stride of 1\n","    padding=1,        # Padding of 1 time step on each side\n","    dilation=1,       # Dilation of 1 (standard convolution)\n","    groups=1,         # Standard convolution (no grouping)\n","    bias=True         # Include bias term\n",")\n","\n","# Example input: batch size of 10, 1 input channel, sequence length of 100\n","input_data = torch.randn(10, 1, 100)\n","\n","# Apply the Conv1d layer\n","output_data = conv1d_layer(input_data)\n","\n","print(f\"Input shape: {input_data.shape}\")\n","print(f\"Output shape: {output_data.shape}\")\n"]},{"cell_type":"markdown","source":["## Summary\n","\n","- **`in_channels`**: Number of input channels.\n","- **`out_channels`**: Number of output channels (filters).\n","- **`kernel_size`**: Size of the convolutional kernel.\n","- **`stride`**: Step size of the convolution.\n","- **`padding`**: Amount of zero-padding added to the input.\n","- **`dilation`**: Spacing between elements in the filter.\n","- **`groups`**: Controls connections between inputs and outputs.\n","- **`bias`**: Whether to include a bias term in the output.\n","\n","These parameters allow you to control the behavior and configuration of the convolutional layer, enabling you to tailor it to your specific task and dataset."],"metadata":{"id":"cKqjwYGSJb4C"}},{"cell_type":"code","source":[],"metadata":{"id":"7FpIU5m5JZCG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `filters` and `kernels` in nn.Conv1d\n","\n","## Filters\n","* **Definition**: Filters, also known as feature detectors, are the learnable parameters in the convolutional layers. Each filter is applied to the input data to extract specific features.\n","* **Role**: The number of filters (**`out_channels`** in **`nn.Conv1d`**) determines how many different feature maps (or output channels) the layer will produce. Each filter is responsible for detecting different patterns or features in the input data.\n","\n","## Kernels\n","* **Definition**: A kernel (or convolutional kernel) is the actual matrix (or vector in the 1D case) that slides over the input data to perform the convolution operation. The size of this matrix is defined by the **`kernel_size`** parameter.\n","* **Role**: The kernel size determines  the receptive field of the filter, which is the window of input data that the filter looks at each time it is applied. The kernel performs element-wise multiplications and summations to produce the outpu value at each position.\n","\n","## How They Work Together\n","1. **Kernel Size (`kernel_size`)**: Determines the size of the sliding window. For example, if **`kernel_size=3`** in **`nn.Conv1d`**, the kernel will look at 3 consecutive time steps in the input sequence.\n","2. **Number of Filters (`out_channels`)**: Specifies how many such kernels (each potentially learnining to detect a different features) are used. For example, if **`out_channels=64`**, there will be 64 different filters, each with its own kernel of size **`kernel_size`**."],"metadata":{"id":"0RM5KCtuJrgA"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Define a Conv1d layer\n","conv1d_layer = nn.Conv1d(\n","    in_channels=1,    # Single input channel\n","    out_channels=64,  # 64 output channels (filters)\n","    kernel_size=3,    # Each filter has a kernel size of 3\n","    stride=1,         # Stride of 1\n","    padding=1,        # Padding of 1 time step on each side\n","    dilation=1,       # Dilation of 1 (standard convolution)\n","    groups=1,         # Standard convolution (no grouping)\n","    bias=True         # Include bias term\n",")\n","\n","# Example input: batch size of 10, 1 input channel, sequence length of 100\n","input_data = torch.randn(10, 1, 100)\n","\n","# Apply the Conv1d layer\n","output_data = conv1d_layer(input_data)\n","\n","print(f\"Input shape: {input_data.shape}\")\n","print(f\"Output shape: {output_data.shape}\")\n"],"metadata":{"id":"WJegmoCWM3yF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Breakdown of Parameters:\n","* **in_channelss=1**: The input has 1 channel (e.g., a single feature per time step).\n","* **out_channels=64**: The layer will use 64 different filters, each producing one output channels.\n","* **kernel_size=3**: Each filter will have a kernel size of 3, meaning it will look at 3 consecutive time steps at a time.\n","\n","## Summary:\n","- **Kernel Size:** Defines the window size of the convolutional operation.\n","- **Number of Filters:** Defines the number of such convolutional operations (each with its own kernel) to apply to the input, resulting in multiple feature maps.\n","\n","By adjusting these parameters, you can control how the convolutional layer processes the input data, influencing the types of features it can learn and the richness of the output representation."],"metadata":{"id":"aneVfmzmM5E5"}},{"cell_type":"code","source":[],"metadata":{"id":"LvgmZSJHNmne"},"execution_count":null,"outputs":[]}]}